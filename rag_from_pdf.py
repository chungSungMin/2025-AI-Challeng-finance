import os
import torch
import json
import re
import random # ì£¼ì œ ë‹¤ì–‘ì„±ì„ ìœ„í•´ ì¶”ê°€
import time   # ìƒì„± ì‹œê°„ ì¸¡ì •ì„ ìœ„í•´ ì¶”ê°€
import transformers
from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from pydantic import BaseModel, Field, ValidationError
from sentence_transformers import CrossEncoder
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# Pydantic ëª¨ë¸ì€ ë³€ê²½ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
class MultipleChoiceQuestion(BaseModel):
    question: str = Field(description="ìƒì„±ëœ ê°ê´€ì‹ ì§ˆë¬¸")
    options: dict[str, str] = Field(description="í‚¤ê°€ '1', '2', '3', '4'ì¸ 4ê°œì˜ ì„ íƒì§€ ë”•ì…”ë„ˆë¦¬")
    answer: str = Field(description="ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ì„ íƒì§€ì˜ í‚¤ ('1'~'4')")

# setup_model, load_real_llm í•¨ìˆ˜ëŠ” ë³€ê²½ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
def setup_model():
    """HuggingFaceì—ì„œ 4ë¹„íŠ¸ ì–‘ìí™”ëœ LLMê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("ğŸš€ LLM ëª¨ë¸ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    model_id = "K-intelligence/Midm-2.0-Base-Instruct"
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto",
            quantization_config=quantization_config
        )
        print("âœ… LLM ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

def load_real_llm(model, tokenizer):
    """ë¡œë“œëœ ëª¨ë¸ì„ LangChainê³¼ ì—°ë™ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤."""
    if model is None or tokenizer is None:
        return None
    pipe = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,
        repetition_penalty=1.2,
        temperature=0.8,
        do_sample=True,
    )
    llm = HuggingFacePipeline(pipeline=pipe)
    return llm

# âœ¨ [í•µì‹¬ ìˆ˜ì • 1] rerank_documents í•¨ìˆ˜ê°€ ë¯¸ë¦¬ ìƒì„±ëœ cross_encoder ëª¨ë¸ì„ ì¸ìë¡œ ë°›ë„ë¡ ë³€ê²½
def rerank_documents(cross_encoder: CrossEncoder, question: str, documents: list, top_n=3):
    """CrossEncoderë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ì„±ì´ ë†’ì€ ë¬¸ì„œë¥¼ ì¬ì •ë ¬í•©ë‹ˆë‹¤."""
    pairs = [[question, doc.page_content] for doc in documents]
    scores = cross_encoder.predict(pairs, show_progress_bar=False)
    
    scored_docs = sorted(zip(scores, documents), key=lambda x: x[0], reverse=True)
    
    return [doc for score, doc in scored_docs[:top_n]]


def main():
    """ê°œì„ ëœ RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ 300ê°œì˜ í€´ì¦ˆë¥¼ ìƒì„±í•˜ê³  JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    
    NUM_QUIZZES_TO_GENERATE = 300
    OUTPUT_FILENAME = "ì‹ ìš©ì •ë³´ì˜ì´ìš©ë°ë³´í˜¸_300.json"

    print("--- [ì¤€ë¹„ ë‹¨ê³„: RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”] ---")
    
    pdf_folder_path = "./data"
    if not os.path.exists(pdf_folder_path) or not any(f.endswith('.pdf') for f in os.listdir(pdf_folder_path)):
        print(f"âŒ ì—ëŸ¬: '{pdf_folder_path}' í´ë”ê°€ ì—†ê±°ë‚˜ í´ë” ì•ˆì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    documents = []
    for file in os.listdir(pdf_folder_path):
        if file.endswith('.pdf'):
            pdf_path = os.path.join(pdf_folder_path, file)
            print(f"ğŸ“„ '{file}' íŒŒì¼ ë¡œë“œ ì¤‘...")
            loader = PyPDFLoader(pdf_path)
            documents.extend(loader.load())
            
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(documents)
    
    if not splits:
        print("âŒ ë¬¸ì„œë¥¼ ë¶„í• í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. PDF íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    vectordb = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectordb.as_retriever(search_kwargs={"k": 10})
    
    print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ.\n" + "=" * 50)

    model, tokenizer = setup_model()
    llm = load_real_llm(model, tokenizer)
    if not llm:
        print("LLM ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤."); return

    # âœ¨ [í•µì‹¬ ìˆ˜ì • 2] CrossEncoder ëª¨ë¸ì„ ë£¨í”„ ì‹œì‘ ì „ì— ë”± í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
    print("ğŸš€ Reranker ëª¨ë¸ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    try:
        reranker = CrossEncoder('dragonkue/bge-reranker-v2-m3-ko', device='cuda' if torch.cuda.is_available() else 'cpu')
        print("âœ… Reranker ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"âŒ Reranker ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    generated_quizzes = []
    
    start_time = time.time()
    for i in range(NUM_QUIZZES_TO_GENERATE):
        print(f"\n{'='*20} í€´ì¦ˆ {i+1}/{NUM_QUIZZES_TO_GENERATE} ìƒì„± ì‹œì‘ {'='*20}")
        
        llm_output = ""
        try:
            seed_chunk = random.choice(splits)
            seed_query = seed_chunk.page_content[:200]
            
            retrieved_docs = retriever.invoke(seed_query)
            
            if len(retrieved_docs) < 3:
                context_docs = [seed_chunk]
            else:
                # âœ¨ [í•µì‹¬ ìˆ˜ì • 3] ë¯¸ë¦¬ ë¡œë“œëœ reranker ëª¨ë¸ì„ ì¸ìë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
                context_docs = rerank_documents(reranker, seed_query, retrieved_docs, top_n=3)

            context_string = "\n---\n".join([doc.page_content for doc in context_docs])

            prompt = ChatPromptTemplate.from_template(
                """
                [ì§€ì‹œ]
                ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ êµìœ¡ ì½˜í…ì¸  ì œì‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [ë¬¸ì„œ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ, ë‚´ìš©ì˜ í•µì‹¬ ê°œë…ì„ ë¬»ëŠ” ê°ê´€ì‹ ë¬¸ì œ 1ê°œë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

                [ì‘ì—… ì ˆì°¨]
                1. **ìƒê°ì˜ ê³¼ì •**: ë¨¼ì € [ë¬¸ì„œ ë‚´ìš©]ì„ ë¶„ì„í•˜ì—¬ ë¬¸ì œ ìƒì„±ì— ëŒ€í•œ ê³„íšì„ ë‹¨ê³„ë³„ë¡œ ì„œìˆ í•©ë‹ˆë‹¤.
                2. **ìµœì¢… JSON ì¶œë ¥**: 'ìƒê°ì˜ ê³¼ì •'ì´ ëë‚˜ë©´, ê·¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ê²°ê³¼ë¬¼ì„ ì•„ë˜ [JSON ì¶œë ¥ í˜•ì‹]ì— ë§ì¶° JSONìœ¼ë¡œë§Œ ì¶œë ¥í•©ë‹ˆë‹¤. JSON ê°ì²´ëŠ” ë°˜ë“œì‹œ ```json ... ``` ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ê°ì‹¸ì•¼ í•©ë‹ˆë‹¤.

                [ë¬¸ì„œ ë‚´ìš©]
                {context}

                [JSON ì¶œë ¥ í˜•ì‹]
                - ìµœìƒìœ„ ê°ì²´ëŠ” "question", "options", "answer" ì„¸ ê°œì˜ í‚¤ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.
                - "question"ì˜ ê°’ì€ ë¬¸ìì—´ì…ë‹ˆë‹¤.
                - "options"ì˜ ê°’ì€ ë°˜ë“œì‹œ í‚¤ê°€ "1", "2", "3", "4"ì¸ 4ê°œì˜ ì„ íƒì§€ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬(ê°ì²´)ì—¬ì•¼ í•©ë‹ˆë‹¤.
                - "answer"ì˜ ê°’ì€ ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ì„ íƒì§€ì˜ í‚¤(ì˜ˆ: "1") ë¬¸ìì—´ì…ë‹ˆë‹¤.

                ---
                
                [ìƒê°ì˜ ê³¼ì •]
                1. í•µì‹¬ ê°œë… ì„ ì •: 
                2. ì§ˆë¬¸ êµ¬ì„±: 
                3. ì„ íƒì§€(options) êµ¬ì„±: 
                   - ì •ë‹µ (í‚¤ "1"): 
                   - ì˜¤ë‹µ (í‚¤ "2"): 
                   - ì˜¤ë‹µ (í‚¤ "3"): 
                   - ì˜¤ë‹µ (í‚¤ "4"): 
                4. ìµœì¢… JSON ì¡°í•© ë° ì •ë‹µ í‚¤ í™•ì¸: 

                [ìµœì¢… JSON ì¶œë ¥]
                ```json
                """
            )
            
            chain = prompt | llm | StrOutputParser()
            llm_output = chain.invoke({"context": context_string})
            
            json_str = None
            
            json_match = re.search(r"```json\n(.*?)\n```", llm_output, re.DOTALL)
            if json_match:
                content_inside_block = json_match.group(1).strip()
                first_brace_index = content_inside_block.find('{')
                last_brace_index = content_inside_block.rfind('}')
                if first_brace_index != -1 and last_brace_index != -1:
                    json_str = content_inside_block[first_brace_index : last_brace_index + 1]

            if not json_str:
                first_brace_index = llm_output.find('{')
                last_brace_index = llm_output.rfind('}')
                if first_brace_index != -1 and last_brace_index != -1:
                    json_str = llm_output[first_brace_index : last_brace_index + 1]

            if not json_str:
                raise ValueError("LLM ì¶œë ¥ì—ì„œ ìœ íš¨í•œ JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            quiz_data = json.loads(json_str)
            validated_quiz = MultipleChoiceQuestion(**quiz_data)
            quiz_json = validated_quiz.model_dump()

            generated_quizzes.append(quiz_json)
            print(f"âœ… í€´ì¦ˆ {i+1} ìƒì„± ì™„ë£Œ. í˜„ì¬ê¹Œì§€ ì´ {len(generated_quizzes)}ê°œ ìƒì„±.")

        except (ValidationError, json.JSONDecodeError, ValueError, KeyError) as e:
            print(f"âŒ í€´ì¦ˆ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
            print("\n--- [LLM ì›ë³¸ ì¶œë ¥ (ì˜¤ë¥˜ ì›ì¸)] ---")
            print(llm_output)
            print("---------------------------------")
            continue
        finally:
            # âœ¨ [í•µì‹¬ ìˆ˜ì • 4] ë£¨í”„ê°€ ëë‚  ë•Œë§ˆë‹¤ ìºì‹œë¥¼ ë¹„ì›Œ ë©”ëª¨ë¦¬ ì¡°ê°í™”ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    end_time = time.time()
    total_time = end_time - start_time
    
    print(f"\n{'='*20} ëª¨ë“  ì‘ì—… ì™„ë£Œ {'='*20}")
    if generated_quizzes:
        print(f"ì´ ìš”ì²­: {NUM_QUIZZES_TO_GENERATE}ê°œ")
        print(f"ì„±ê³µ: {len(generated_quizzes)}ê°œ")
        print(f"ì‹¤íŒ¨: {NUM_QUIZZES_TO_GENERATE - len(generated_quizzes)}ê°œ")
        print(f"ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ ({total_time/60:.2f}ë¶„)")
        
        print(f"\nìƒì„±ëœ í€´ì¦ˆë¥¼ '{OUTPUT_FILENAME}' íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
            json.dump(generated_quizzes, f, ensure_ascii=False, indent=4)
        print("âœ… íŒŒì¼ ì €ì¥ ì™„ë£Œ.")
    else:
        print("\nâŒ ìƒì„±ëœ í€´ì¦ˆê°€ ì—†ì–´ íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
