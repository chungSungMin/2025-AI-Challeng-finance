import os
import torch
import transformers
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
# langchain_community.embeddings -> langchain_huggingfaceë¡œ ë³€ê²½
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import HuggingFacePipeline # ì‹¤ì œ LLM ì—°ë™ì„ ìœ„í•´ ì¶”ê°€
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# ----------------------------------------------------------------
# âœ¨ 1. ì‹¤ì œ LLMì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (ìµœì´ˆ ì½”ë“œ í™œìš©)
# ----------------------------------------------------------------
def setup_model():
    """
    HuggingFaceì—ì„œ 4ë¹„íŠ¸ ì–‘ìí™”ëœ ì‹¤ì œ LLMê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    print("ğŸš€ ì‹¤ì œ LLM ëª¨ë¸ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    model_id = "SEOKDONG/llama3.1_korean_v1.1_sft_by_aidx"

    try:
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto",
            quantization_config=quantization_config,
        )
        print("âœ… ì‹¤ì œ LLM ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

# ----------------------------------------------------------------
# âœ¨ 2. ë¡œë“œëœ ëª¨ë¸ì„ LangChainê³¼ ì—°ë™í•˜ëŠ” ë¶€ë¶„
# ----------------------------------------------------------------
def load_real_llm():
    model, tokenizer = setup_model()
    if model is None or tokenizer is None:
        return None

    # Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipe = transformers.pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024 # ë‹µë³€ ìƒì„± ìµœëŒ€ ê¸¸ì´
    )

    # LangChainì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ íŒŒì´í”„ë¼ì¸ì„ ë˜í•‘
    llm = HuggingFacePipeline(pipeline=pipe)
    return llm

# --- RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘ ---

# --- ë‹¨ê³„ 1: PDF ë¡œë“œ ---
print("--- [ë‹¨ê³„ 1: PDF ë¡œë“œ] ---")
pdf_folder_path = "./data"
documents = []
if not os.path.exists(pdf_folder_path):
    print(f"ê²½ê³ : '{pdf_folder_path}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. PDFë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
else:
    for file in os.listdir(pdf_folder_path):
        if file.endswith('.pdf'):
            pdf_path = os.path.join(pdf_folder_path, file)
            loader = PyPDFLoader(pdf_path)
            documents.extend(loader.load())
print(f"âœ… ì´ {len(documents)}ê°œì˜ í˜ì´ì§€ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
# âœ¨ [í™•ì¸ ì½”ë“œ] ë¡œë”©ëœ ì²« í˜ì´ì§€ ë‚´ìš© í™•ì¸
if documents:
    print(f"ğŸ“„ ì²« í˜ì´ì§€ ì¶œì²˜: {documents[0].metadata.get('source', 'N/A')}")
    print(f"ğŸ“„ ì²« í˜ì´ì§€ ë‚´ìš© ì¼ë¶€: {documents[0].page_content[:200]}...\n")
print("-" * 50)


# --- ë‹¨ê³„ 2: ì²­í¬ ë¶„í•  ---
print("--- [ë‹¨ê³„ 2: ì²­í¬ ë¶„í• ] ---")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
splits = text_splitter.split_documents(documents)
print(f"âœ… ë¬¸ì„œë¥¼ ì´ {len(splits)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
# âœ¨ [í™•ì¸ ì½”ë“œ] ë¶„í• ëœ ì²« ì²­í¬ ë‚´ìš© í™•ì¸
if splits:
    print(f"ğŸ“„ ì²« ë²ˆì§¸ ì²­í¬ ë‚´ìš©: {splits[0].page_content[:300]}...\n")
print("-" * 50)


# --- ë‹¨ê³„ 3: ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥ ---
print("--- [ë‹¨ê³„ 3: ì„ë² ë”© ë° ë²¡í„° DB ì €ì¥] ---")
model_name = "jhgan/ko-sbert-nli"
model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
encode_kwargs = {'normalize_embeddings': True}
embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)
vectordb = Chroma.from_documents(documents=splits, embedding=embeddings)
print("âœ… ë²¡í„° DB ì €ì¥ ì™„ë£Œ.\n")
print("-" * 50)


# --- ë‹¨ê³„ 4 & 5: RAG ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰ ---
print("--- [ë‹¨ê³„ 4/5: RAG ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰] ---")
retriever = vectordb.as_retriever()

# âœ¨ [í™•ì¸ ì½”ë“œ] ë¦¬íŠ¸ë¦¬ë²„ ë‹¨ë… í…ŒìŠ¤íŠ¸
print("ğŸ•µï¸ ë¦¬íŠ¸ë¦¬ë²„ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì˜ ì°¾ì•„ì˜¤ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤...")
test_question = "ë¶í•œ í•´í‚¹ ê·¸ë£¹ì˜ ì£¼ìš” ê³µê²© ë°©ì‹ì€ ë¬´ì—‡ì¸ê°€ìš”?"
try:
    relevant_docs = retriever.invoke(test_question)
    print(f"\n[í…ŒìŠ¤íŠ¸ ì§ˆë¬¸]: '{test_question}'")
    for i, doc in enumerate(relevant_docs):
        print(f"\n--- ê´€ë ¨ ë¬¸ì„œ #{i+1} ---")
        print(f"ì¶œì²˜: {doc.metadata.get('source', 'N/A')}")
        print(f"ë‚´ìš©: {doc.page_content[:200]}...")
except Exception as e:
    print(f"ë¦¬íŠ¸ë¦¬ë²„ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
print("\n" + "-" * 50)


print("âš™ï¸ ì‹¤ì œ LLMì„ ë¡œë“œí•˜ì—¬ RAG ì²´ì¸ì„ ìµœì¢… êµ¬ì„±í•©ë‹ˆë‹¤...")
# ì‹¤ì œ LLM ë¡œë“œ
llm = load_real_llm()

if llm:
    template = """
    ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ ì‚¬ì´ë²„ ë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ [Context] ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ [Question]ì— ëŒ€í•´ ëª…í™•í•˜ê³  ì „ë¬¸ê°€ì ìœ¼ë¡œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
    ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ [Context]ì— ë‹µë³€ì— í•„ìš”í•œ ì •ë³´ê°€ ì—†ë‹¤ë©´, "ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.

    [Context]
    {context}

    [Question]
    {question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    print("\n--- [ìµœì¢… ì‹¤í–‰] ---")
    question_to_ask = "ë¶í•œ í•´í‚¹ ê·¸ë£¹ ë¼ìë£¨ìŠ¤ì˜ ì£¼ìš” í™œë™ ë°©ì‹ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜."
    print(f"ì§ˆë¬¸: {question_to_ask}")
    
    # RAG ì²´ì¸ ì‹¤í–‰
    response = rag_chain.invoke(question_to_ask)

    print("\n--- [ìµœì¢… ê²°ê³¼] ---")
    print(f"ë‹µë³€: {response}")
    print("-" * 50)
else:
    print("LLM ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ RAG ì²´ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")