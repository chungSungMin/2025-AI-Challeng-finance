import torch
import pandas as pd
import re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from peft import PeftModel
import os

# --- 1. ì„¤ì • (Configuration) ---

# ê¸°ë³¸ ëª¨ë¸ ID (í•™ìŠµì— ì‚¬ìš©í•œ ëª¨ë¸)
BASE_MODEL_ID = "K-intelligence/Midm-2.0-Base-Instruct"

# â­ï¸ ì‚¬ìš©ì ì„¤ì •: í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ê°€ ì €ì¥ëœ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.
LORA_ADAPTER_PATH = "/workspace/2025-AI-Challeng-finance/midm-lora-adapter-combined-laws/checkpoint-22" 

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ì œì¶œ íŒŒì¼ ê²½ë¡œ
TEST_CSV_PATH = '/workspace/open/test.csv'
SUBMISSION_CSV_PATH = './submission_new_traindataset.csv' 

# --- 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---

def is_multiple_choice(question_text: str) -> bool:
    lines = question_text.strip().split("\n")
    option_count = sum(bool(re.match(r"^\s*[1-9][0-9]?\s", line)) for line in lines)
    return option_count >= 2

def extract_question_and_choices(full_text: str) -> tuple[str, list[str]]:
    lines = full_text.strip().split("\n")
    q_lines = []
    options = []
    for line in lines:
        if re.match(r"^\s*[1-9][0-9]?\s", line):
            options.append(line.strip())
        else:
            q_lines.append(line.strip())
    question = " ".join(q_lines)
    return question, options


def make_prompt(text: str) -> str:
    """
    ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ LoRA í•™ìŠµì— ì‚¬ìš©ëœ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        # ê°ê´€ì‹ í”„ë¡¬í”„íŠ¸
        prompt = f"""### ì§€ì‹œ:
                    ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë‹µë³€ì„ ì„ íƒì§€ì—ì„œ ê³ ë¥´ì„¸ìš”.

                    ### ì˜ˆì‹œ1
                    ì§ˆë¬¸ : ê°œì¸ì •ë³´ë³´í˜¸ë²•ì— ë”°ë¼ ë³´í˜¸ìœ„ì›íšŒê°€ ê³¼ì§•ê¸ˆì„ ë¶€ê³¼í•  ë•Œ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ì‚¬í•­ ì¤‘ í•˜ë‚˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
                    ì„ íƒì§€ : 
                    1. ìœ„ë°˜í–‰ìœ„ì˜ ë‚´ìš© ë° ì •ë„
                    2. ê°œì¸ì •ë³´ ì²˜ë¦¬ìì˜ ì£¼ì†Œì™€ ì—°ë½ì²˜
                    3. ê°œì¸ì •ë³´ì˜ ìˆ˜ì§‘ ë° ì´ìš© ëª©ì 
                    4. ê°œì¸ì •ë³´ ì²˜ë¦¬ìì˜ ì§ì› ìˆ˜ì™€ ì—°ë´‰ ìˆ˜ì¤€
                    ë‹µ : 1

                    ### ì˜ˆì‹œ2
                    ì§ˆë¬¸ : ê°œì¸ì •ë³´ ë³´í˜¸ ê¸°ë³¸ê³„íšì€ ì–¸ì œê¹Œì§€ ìˆ˜ë¦½í•´ì•¼ í•˜ëŠ”ê°€?
                    ì„ íƒì§€ : 
                    1. ê°œì¸ì •ë³´ ë³´í˜¸ ê¸°ë³¸ê³„íšì€ ë§¤ë…„ 12ì›” 31ì¼ê¹Œì§€ ìˆ˜ë¦½í•´ì•¼ í•œë‹¤
                    2. ê·¸ 3ë…„ì´ ì‹œì‘ë˜ëŠ” í•´ì˜ ì „ë…„ë„ 6ì›” 30ì¼ê¹Œì§€ ìˆ˜ë¦½í•´ì•¼í•œë‹¤.
                    3. ê°œì¸ì •ë³´ ë³´í˜¸ ê¸°ë³¸ê³„íšì€ ë§¤ë…„ 9ì›” 30ì¼ê¹Œì§€ ìˆ˜ë¦½í•´ì•¼ í•œë‹¤.
                    4. ê°œì¸ì •ë³´ ë³´í˜¸ ê¸°ë³¸ê³„íšì€ ê·¸ 3ë…„ì´ ì‹œì‘ë˜ëŠ” í•´ì˜ ì „ë…„ë„ 12ì›” 31ì¼ê¹Œì§€ ìˆ˜ë¦½í•´ì•¼ í•œë‹¤.
                    5. ê°œì¸ì •ë³´ ë³´í˜¸ ê¸°ë³¸ê³„íšì€ ë§¤ë…„ 10ì›” 30ì¼ê¹Œì§€ ìˆ˜ë¦½í•´ì•¼ í•œë‹¤.
                    ë‹µë³€ : 2

                    ### ì‹¤ì œ ì…ë ¥
                    ì§ˆë¬¸:
                    {question}

                    ì„ íƒì§€:
                    {chr(10).join(options)}

                    ë‹µë³€:
                """
    else:
        # ì£¼ê´€ì‹ í”„ë¡¬í”„íŠ¸
        prompt = f"""### ì§€ì‹œ:
                    ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì•Œê³ ìˆëŠ” ëŒ€ë¡œ ì„œìˆ í•˜ì„¸ìš”.
                    ë‹¨, ëª¨ë“  ë¬¸ì¥ì€ í•œêµ­ì–´ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•˜ê³ , ì™„ë²½í•œ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•´ì•¼í•©ë‹ˆë‹¤. 
                    

                    ### ì˜ˆì‹œ1 : 
                    ì§ˆë¬¸ : ê°œì¸ì •ë³´ì˜ êµ­ì™¸ ì´ì „ì´ ì¤‘ì§€ë  ìˆ˜ ìˆëŠ” ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€?
                    ë‹µë³€ : ê°œì¸ì •ë³´ì˜ êµ­ì™¸ ì´ì „ì´ ì¤‘ì§€ë  ìˆ˜ ìˆëŠ” ì¡°ê±´ì€ ì œ28ì¡°ì˜8ì œ1í•­, ì œ4í•­ ë˜ëŠ” ì œ5í•­ì„ ìœ„ë°˜í•˜ê±°ë‚˜ ê°œì¸ì •ë³´ë¥¼ ì´ì „ë°›ëŠ” ìë‚˜ êµ­ê°€ê°€ ê°œì¸ì •ë³´ ë³´í˜¸ ìˆ˜ì¤€ì— ë¯¸ì¹˜ì§€ ëª»í•˜ì—¬ ì •ë³´ì£¼ì²´ì—ê²Œ í”¼í•´ê°€ ë°œìƒí•  ìš°ë ¤ê°€ ìˆëŠ” ê²½ìš°ì´ë‹¤.


                    ### ì˜ˆì‹œ2 : 
                    ì§ˆë¬¸ : ë¶„ìŸì¡°ì •ìœ„ì›íšŒê°€ ë¶„ìŸì¡°ì • ì‹ ì²­ì„ ë°›ì€ í›„ ì‹¬ì‚¬í•˜ì—¬ ì¡°ì •ì•ˆì„ ì‘ì„±í•´ì•¼ í•˜ëŠ” ê¸°ê°„ì€ ì–¼ë§ˆì¸ê°€?
                    ë‹µë³€ : ë¶„ìŸì¡°ì •ìœ„ì›íšŒëŠ” ë¶„ìŸì¡°ì • ì‹ ì²­ì„ ë°›ì€ ë‚ ë¶€í„° 60ì¼ ì´ë‚´ì— ì‹¬ì‚¬í•˜ì—¬ ì¡°ì •ì•ˆì„ ì‘ì„±í•´ì•¼ í•œë‹¤.
                    

                    ### ì‹¤ì œ ì…ë ¥ : 
                    ì§ˆë¬¸:
                    {text}

                    ### ë‹µë³€:
                """

    return prompt

# --- 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---

print("â³ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")

#=============ì–‘ìí™”=====================#

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quantization_config,
    device_map="auto"
)

# trust_remote_code : hugging faceì—ì„œ ì œê³µí•˜ëŠ” ì¶”ê°€ì ì¸ í† í¬ë‚˜ì´ì € ì½”ë“œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ 
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"â³ '{LORA_ADAPTER_PATH}'ì—ì„œ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë”©í•˜ì—¬ ëª¨ë¸ì— ì ìš©í•©ë‹ˆë‹¤...")
try:
    model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: LoRA ì–´ëŒ‘í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {LORA_ADAPTER_PATH}")
    print(e)
    exit()

print("â³ LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©í•©ë‹ˆë‹¤...")
model = model.merge_and_unload()

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)
print("âœ… ëª¨ë¸ ë¡œë”© ë° ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- 5. ì¶”ë¡  ë° í›„ì²˜ë¦¬ ---

def post_process_answer(generated_text: str, original_question: str) -> str:
    """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ìµœì¢… ë‹µë³€ì„ ì¶”ì¶œí•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤."""
    answer = generated_text.strip()
    
    if not answer:
        return "ë¯¸ì‘ë‹µ"

    # ë§Œì•½ì„ ëŒ€ë¹„í•´ ë‹µë³€ì— í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
    if "##" or   "###" or "---" in answer:
        answer = answer.split("###")[0].strip()
        
    if is_multiple_choice(original_question):
        match = re.match(r"\D*([1-9][0-9]?)", answer)
        return match.group(1) if match else "1" # ìˆ«ì ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ 1ë²ˆìœ¼ë¡œ ì¶”ì¸¡
    
    return answer if answer else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."


def is_code_detected(text: str) -> bool:
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ì½”ë“œê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    # ì‚¬ìš©ìê°€ ì œê³µí•œ íŠ¹ì • ì½”ë“œ íŒ¨í„´ ë° ì¼ë°˜ì ì¸ í‚¤ì›Œë“œ ì¶”ê°€
    code_keywords = ['def ', 'import ', 'class ', 'r\'', 'sys.stdout', 'ans_qna']
    text_lower = text.lower()
    return any(keyword in text_lower for keyword in code_keywords)


# --- 6. ë©”ì¸ ì‹¤í–‰ (ì¬ì‹œë„ ë¡œì§ ì ìš©) ---
if __name__ == "__main__":
    try:
        test_df = pd.read_csv(TEST_CSV_PATH)
        print(f"âœ… '{TEST_CSV_PATH}'ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{TEST_CSV_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()

    preds = []
    
    # â­ï¸ 2. ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì„¤ì •
    MAX_RETRIES = 3 

    for index, q in tqdm(enumerate(test_df['Question']), total=len(test_df), desc="ğŸš€ ì¶”ë¡  ì§„í–‰ ì¤‘"):
        prompt = make_prompt(q)
        
        is_valid_answer = False
        retries = 0
        generated_text = ""

        # â­ï¸ 3. ìœ íš¨í•œ ë‹µë³€ì„ ì–»ê±°ë‚˜ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ
        while not is_valid_answer and retries < MAX_RETRIES:
            if retries > 0:
                print(f"\nğŸ”„ TEST_{index} ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì¬ì‹œë„ ì¤‘... ({retries}/{MAX_RETRIES})")

            output = pipe(
                prompt, 
                max_new_tokens=512,
                # â­ï¸ ì¬ì‹œë„í•  ë•Œë§ˆë‹¤ temperatureë¥¼ ì•½ê°„ ë†’ì—¬ ë‹¤ë¥¸ ë‹µë³€ì„ ìœ ë„
                temperature=0.1 + (retries * 0.15),
                top_p=0.9,
                do_sample=True,
                return_full_text=False,
                eos_token_id=tokenizer.eos_token_id
            )
            
            generated_text = output[0]['generated_text']

            # ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ì½”ë“œ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸
            if is_code_detected(generated_text):
                retries += 1
                # ë§ˆì§€ë§‰ ì¬ì‹œë„ì—ì„œë„ ì‹¤íŒ¨í•˜ë©´, ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥ í›„ ë£¨í”„ ì¢…ë£Œ
                if retries == MAX_RETRIES:
                    print(f"âŒ TEST_{index} ì§ˆë¬¸ì— ëŒ€í•´ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë§ˆì§€ë§‰ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    is_valid_answer = True # ë£¨í”„ë¥¼ ë¹ ì ¸ë‚˜ê°€ê¸° ìœ„í•´ Trueë¡œ ì„¤ì •
            else:
                is_valid_answer = True # ì½”ë“œ íŒ¨í„´ì´ ì—†ìœ¼ë¯€ë¡œ ìœ íš¨í•œ ë‹µë³€ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë£¨í”„ ì¢…ë£Œ

        # ìµœì¢…ì ìœ¼ë¡œ ì–»ì€ ë‹µë³€ì„ í›„ì²˜ë¦¬
        pred_answer = post_process_answer(generated_text, original_question=q)
        preds.append(pred_answer)

    print("\nğŸ“„ ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        sample_submission = pd.read_csv('/workspace/open/sample_submission.csv')
        sample_submission['Answer'] = preds
        sample_submission.to_csv(SUBMISSION_CSV_PATH, index=False, encoding='utf-8-sig')
        print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: '{SUBMISSION_CSV_PATH}'")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '/workspace/open/sample_submission.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")


