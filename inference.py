import torch
import pandas as pd
import re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from peft import PeftModel
import os

# --- 1. ì„¤ì • (Configuration) ---

# ê¸°ë³¸ ëª¨ë¸ ID (í•™ìŠµì— ì‚¬ìš©í•œ ëª¨ë¸)
BASE_MODEL_ID = "K-intelligence/Midm-2.0-Base-Instruct"

# â­ï¸ ì‚¬ìš©ì ì„¤ì •: í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ê°€ ì €ì¥ëœ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.
LORA_ADAPTER_PATH = "./midm-lora-adapter-unified-trainer/checkpoint-201" 

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ì œì¶œ íŒŒì¼ ê²½ë¡œ
TEST_CSV_PATH = '/workspace/open/test.csv'
SUBMISSION_CSV_PATH = './submission_fewshot_v2.csv' # ì €ì¥ë  íŒŒì¼ ì´ë¦„ ë³€ê²½

# --- 2. Few-shot ì˜ˆì‹œ ë° í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (â˜…â˜…â˜…â˜…â˜… ë‹µë³€ í˜•ì‹ ì œì–´ë¥¼ ìœ„í•´ ì§€ì‹œë¬¸ ê°•í™”) ---

# ëª¨ë¸ì—ê²Œ ë¬¸ì œ ìœ í˜•ë³„ ëª¨ë²” ë‹µì•ˆ í˜•ì‹ì„ ëª…í™•íˆ ì•Œë ¤ì£¼ì–´ ë‹µë³€ì˜ ì •í™•ë„ì™€ ì¼ê´€ì„±ì„ ë†’ì…ë‹ˆë‹¤.
# ì§€ì‹œë¬¸ì„ "ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì‹œì˜¤", "ë‹µë³€ ë‚´ìš©ë§Œ ì„œìˆ í•˜ì‹œì˜¤" ì™€ ê°™ì´ ë” ê°•ë ¥í•˜ê²Œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
FEW_SHOT_EXAMPLES = """
### ì§€ì‹œ:
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ì„ íƒì§€ì˜ ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì‹œì˜¤.

### ì§ˆë¬¸:
ê°œì¸ì •ë³´ ë³´í˜¸ë²•ìƒ, ì •ë³´ì£¼ì²´ì˜ ë™ì˜ ì—†ì´ ê°œì¸ì •ë³´ë¥¼ ì œ3ìì—ê²Œ ì œê³µí•  ìˆ˜ ìˆëŠ” ê²½ìš°ê°€ ì•„ë‹Œ ê²ƒì€?
1. ì •ë³´ì£¼ì²´ë¡œë¶€í„° ë³„ë„ì˜ ë™ì˜ë¥¼ ë°›ì€ ê²½ìš°
2. ë²•ë¥ ì— íŠ¹ë³„í•œ ê·œì •ì´ ìˆê±°ë‚˜ ë²•ë ¹ìƒ ì˜ë¬´ë¥¼ ì¤€ìˆ˜í•˜ê¸° ìœ„í•˜ì—¬ ë¶ˆê°€í”¼í•œ ê²½ìš°
3. ì •ë³´ì£¼ì²´ ë˜ëŠ” ê·¸ ë²•ì •ëŒ€ë¦¬ì¸ì´ ì˜ì‚¬í‘œì‹œë¥¼ í•  ìˆ˜ ì—†ëŠ” ìƒíƒœì— ìˆê±°ë‚˜ ì£¼ì†Œë¶ˆëª… ë“±ìœ¼ë¡œ ì‚¬ì „ ë™ì˜ë¥¼ ë°›ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°ë¡œì„œ ëª…ë°±íˆ ì •ë³´ì£¼ì²´ ë˜ëŠ” ì œ3ìì˜ ê¸‰ë°•í•œ ìƒëª…, ì‹ ì²´, ì¬ì‚°ì˜ ì´ìµì„ ìœ„í•˜ì—¬ í•„ìš”í•˜ë‹¤ê³  ì¸ì •ë˜ëŠ” ê²½ìš°
4. í†µê³„ì‘ì„± ë° í•™ìˆ ì—°êµ¬ ë“±ì˜ ëª©ì ì„ ìœ„í•˜ì—¬ í•„ìš”í•œ ê²½ìš°ë¡œì„œ íŠ¹ì • ê°œì¸ì„ ì•Œì•„ë³¼ ìˆ˜ ì—†ëŠ” í˜•íƒœë¡œ ê°œì¸ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²½ìš°

### ë‹µë³€:
1

---

### ì§€ì‹œ:
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ë‚´ìš©ë§Œ ì„œìˆ í•˜ì‹œì˜¤.

### ì§ˆë¬¸:
ê°œì¸ì •ë³´ ë³´í˜¸ë²•ìƒ 'ê°€ëª…ì²˜ë¦¬'ë€ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•˜ì‹œì˜¤.

### ë‹µë³€:
ê°œì¸ì •ë³´ì˜ ì¼ë¶€ë¥¼ ì‚­ì œí•˜ê±°ë‚˜ ì¼ë¶€ ë˜ëŠ” ì „ë¶€ë¥¼ ëŒ€ì²´í•˜ëŠ” ë“±ì˜ ë°©ë²•ìœ¼ë¡œ ì¶”ê°€ ì •ë³´ ì—†ì´ëŠ” íŠ¹ì • ê°œì¸ì„ ì•Œì•„ë³¼ ìˆ˜ ì—†ë„ë¡ ì²˜ë¦¬í•˜ëŠ” ê²ƒ.
"""

# --- 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---

def is_multiple_choice(question_text: str) -> bool:
    lines = question_text.strip().split("\n")
    option_count = sum(bool(re.match(r"^\s*[1-9][0-9]?\s", line)) for line in lines)
    return option_count >= 2

def extract_question_and_choices(full_text: str) -> tuple[str, list[str]]:
    lines = full_text.strip().split("\n")
    q_lines = []
    options = []
    for line in lines:
        if re.match(r"^\s*[1-9][0-9]?\s", line):
            options.append(line.strip())
        else:
            q_lines.append(line.strip())
    question = " ".join(q_lines)
    return question, options

# --- 4. í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸° (ì§€ì‹œë¬¸ ê°•í™” ì ìš©) ---

def make_prompt(text: str) -> str:
    """
    ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ Few-shot ì˜ˆì‹œê°€ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ê°•í™”ëœ ì§€ì‹œë¬¸ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ë‹µë³€ í˜•ì‹ì„ ëª…í™•íˆ ì¸ì§€í•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.
    """
    # 1. ë¨¼ì € Few-shot ì˜ˆì‹œë“¤ì„ í”„ë¡¬í”„íŠ¸ ì•ë¶€ë¶„ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    base_prompt = FEW_SHOT_EXAMPLES.strip() + "\n\n---\n\n"
    
    # 2. ì‹¤ì œ ì§ˆë¬¸ì— ëŒ€í•´ ê°•í™”ëœ ì§€ì‹œë¬¸ì„ ì ìš©í•©ë‹ˆë‹¤.
    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        # ê°ê´€ì‹: ì •ë‹µ 'ë²ˆí˜¸ë§Œ' ì¶œë ¥í•˜ë„ë¡ ëª…ì‹œ
        final_prompt = f"""### ì§€ì‹œ:
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ì„ íƒì§€ì˜ ë²ˆí˜¸ë§Œ ì¶œë ¥í•˜ì‹œì˜¤.

### ì§ˆë¬¸:
{question}

### ì„ íƒì§€:
{chr(10).join(options)}

### ë‹µë³€:"""
    else:
        # ì£¼ê´€ì‹: 'ë‹µë³€ ë‚´ìš©ë§Œ' ì„œìˆ í•˜ë„ë¡ ëª…ì‹œ
        final_prompt = f"""### ì§€ì‹œ:
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ë‚´ìš©ë§Œ ì„œìˆ í•˜ì‹œì˜¤.

### ì§ˆë¬¸:
{text}

### ë‹µë³€:"""

    return base_prompt + final_prompt

# --- 5. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---

print("â³ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quantization_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"â³ '{LORA_ADAPTER_PATH}'ì—ì„œ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë”©í•˜ì—¬ ëª¨ë¸ì— ì ìš©í•©ë‹ˆë‹¤...")
try:
    model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: LoRA ì–´ëŒ‘í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {LORA_ADAPTER_PATH}")
    print(e)
    exit()

print("â³ LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©í•©ë‹ˆë‹¤...")
model = model.merge_and_unload()

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)
print("âœ… ëª¨ë¸ ë¡œë”© ë° ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- 6. ì¶”ë¡  ë° í›„ì²˜ë¦¬ (ë³€ê²½ ì—†ìŒ) ---

def post_process_answer(generated_text: str, original_question: str) -> str:
    """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤."""
    answer = generated_text.strip()

    if not answer:
        return "ë¯¸ì‘ë‹µ"

    if is_multiple_choice(original_question):
        match = re.match(r"\D*([1-9][0-9]?)", answer)
        return match.group(1) if match else "0"
    else:
        # ë§Œì•½ì„ ëŒ€ë¹„í•´ ë‹µë³€ì— í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
        if "###" in answer:
            answer = answer.split("###")[0].strip()
        return answer

# --- 7. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    try:
        test_df = pd.read_csv(TEST_CSV_PATH)
        print(f"âœ… '{TEST_CSV_PATH}'ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{TEST_CSV_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()

    preds = []
    for q in tqdm(test_df['Question'], desc="ğŸš€ ì¶”ë¡  ì§„í–‰ ì¤‘"):
        prompt = make_prompt(q)
        
        output = pipe(
            prompt, 
            max_new_tokens=256, # ì£¼ê´€ì‹ ë‹µë³€ì„ ìœ„í•´ ì¡°ê¸ˆ ë” ì—¬ìœ ìˆê²Œ ì„¤ì •
            temperature=0.01,   # ì¼ê´€ëœ ë‹µë³€ ìƒì„±ì„ ìœ„í•´ ë§¤ìš° ë‚®ê²Œ ì„¤ì •
            top_p=0.9,
            do_sample=True,
            return_full_text=False
        )
        
        generated_text = output[0]['generated_text']
        pred_answer = post_process_answer(generated_text, original_question=q)
        preds.append(pred_answer)

    print("ğŸ“„ ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        sample_submission = pd.read_csv('/workspace/open/sample_submission.csv')
        sample_submission['Answer'] = preds
        sample_submission.to_csv(SUBMISSION_CSV_PATH, index=False, encoding='utf-8-sig')
        print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: '{SUBMISSION_CSV_PATH}'")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '/workspace/open/sample_submission.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
