import os
import torch
import json
import transformers
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# setup_model() í•¨ìˆ˜ëŠ” ë³€ê²½ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
def setup_model():
    """HuggingFaceì—ì„œ 4ë¹„íŠ¸ ì–‘ìí™”ëœ ì‹¤ì œ LLMê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print("ğŸš€ ì‹¤ì œ LLM ëª¨ë¸ ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    model_id = "SEOKDONG/llama3.1_korean_v1.1_sft_by_aidx"
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            device_map="auto",
            quantization_config=quantization_config,
        )
        print("âœ… ì‹¤ì œ LLM ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ.")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

# load_real_llm() í•¨ìˆ˜ë„ ë³€ê²½ ì—†ì´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
def load_real_llm():
    """ë¡œë“œëœ ëª¨ë¸ì„ LangChainê³¼ ì—°ë™ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤."""
    model, tokenizer = setup_model()
    if model is None or tokenizer is None:
        return None
    pipe = transformers.pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=1024, repetition_penalty=1.2)
    llm = HuggingFacePipeline(pipeline=pipe)
    return llm

# âœ¨ ì‚¬ìš©ìê°€ ì œê³µí•œ ë¬¸ì œ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
def get_quiz_generation_prompt():
    return """
    ë‹¹ì‹ ì€ ì •ë³´ë³´ì•ˆê¸°ì‚¬ êµ­ê°€ê³µì¸ì‹œí—˜ ì¶œì œìœ„ì›ì…ë‹ˆë‹¤. ì˜¤ì§ ì£¼ì–´ì§„ [Context] ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬, ìˆ˜í—˜ìƒì˜ í—ˆë¥¼ ì°Œë¥´ëŠ” ë³€ë³„ë ¥ ë†’ì€ ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

    [ê¸°ë³¸ ê·œì¹™]
    1. [Context]ì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ì§ˆë¬¸ê³¼ ì„ ì§€, í•´ì„¤ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    2. 4ê°œì˜ ì„ ì§€ë¥¼ ìƒì„±í•˜ë©°, ê·¸ì¤‘ ì •ë‹µì€ ë°˜ë“œì‹œ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤.
    3. ì¶”ì¸¡ì— ê¸°ë°˜í•˜ê±°ë‚˜ [Context]ì— ì—†ëŠ” ë¶ˆí™•ì‹¤í•œ ì •ë³´ë¡œ ì„ ì§€ë‚˜ í•´ì„¤ì„ ë§Œë“¤ì–´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

    [ê¸ˆì§€ ì‚¬í•­]
    - 'ë‹¤ìŒ ì¤‘ ì„±ê²©ì´ ë‹¤ë¥¸ í•˜ë‚˜ëŠ”?' ê³¼ ê°™ì´ ê¸°ì¤€ì´ ëª¨í˜¸í•œ ì§ˆë¬¸ì€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
    - ë‹¨ìˆœíˆ [Context]ì˜ ë¬¸ì¥ì„ ë³µì‚¬-ë¶™ì—¬ë„£ê¸° í•œ ê²ƒì²˜ëŸ¼ ë³´ì´ëŠ” ì„ ì§€ëŠ” ë§Œë“¤ì§€ ë§ˆì„¸ìš”.

    [í•´ì„¤ ì‘ì„± ê·œì¹™]
    - 'explanation' í•­ëª©ì—ëŠ” ì •ë‹µì˜ ê·¼ê±°ë¥¼ [Context] ë‚´ìš©ì— ê¸°ë°˜í•˜ì—¬ ëª…í™•íˆ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
    - ì •ë‹µ í•´ì„¤ë¿ë§Œ ì•„ë‹ˆë¼, ë‚˜ë¨¸ì§€ ì˜¤ë‹µ ì„ ì§€ë“¤ì´ ì™œ í‹€ë ¸ëŠ”ì§€ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ë„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

    [ì„¸ë¶„í™”ëœ JSON í˜•ì‹]
    - ë°˜ë“œì‹œ ì•„ë˜ [JSON í˜•ì‹]ì„ ì—„ê²©í•˜ê²Œ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ì½”ë“œ ë¸”ë¡(` ```json ... ``` `)ìœ¼ë¡œ ê°ì‹¸ì„œ ì¶œë ¥í•´ì£¼ì„¸ìš”.
    {{
        "topic": "{topic}",
        "question": "ìƒì„±ëœ ì§ˆë¬¸ ë‚´ìš©",
        "options": {{
            "1": "ì²« ë²ˆì§¸ ì„ ì§€",
            "2": "ë‘ ë²ˆì§¸ ì„ ì§€",
            "3": "ì„¸ ë²ˆì§¸ ì„ ì§€",
            "4.": "ë„¤ ë²ˆì§¸ ì„ ì§€"
        }},
        "answer": "ì •ë‹µ ì„ ì§€ì˜ ë²ˆí˜¸ (ì˜ˆ: '1')",
        "explanation": {{
            "correct_reason": "ì •ë‹µì´ ë§ëŠ” ì´ìœ ì— ëŒ€í•œ ìƒì„¸í•œ í•´ì„¤",
            "incorrect_reasons": {{
                "1": "1ë²ˆ ì„ ì§€ê°€ ì˜¤ë‹µì¸ ì´ìœ ",
                "2": "2ë²ˆ ì„ ì§€ê°€ ì˜¤ë‹µì¸ ì´ìœ ",
                "3": "3ë²ˆ ì„ ì§€ê°€ ì˜¤ë‹µì¸ ì´ìœ "
            }}
        }}
    }}
    ---
    [ì‹¤ì œ ìƒì„± ìš”ì²­]
    
    [Topic]: {topic}

    [Context]:
    {context}
    """

def main():
    """RAG íŒŒì´í”„ë¼ì¸ì„ ì„¤ì •í•˜ê³  ì§€ì •ëœ ì£¼ì œì— ëŒ€í•´ ë¬¸ì œë¥¼ ìƒì„±í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""

    # PDF ë¡œë“œ, ì²­í¬ ë¶„í• , DB ì €ì¥ ê³¼ì •ì€ ë™ì¼í•©ë‹ˆë‹¤.
    print("--- [ë‹¨ê³„ 1: PDF ë¡œë“œ, ë¶„í• , DB ì €ì¥] ---")
    pdf_folder_path = "./data"
    documents = []
    if not os.path.exists(pdf_folder_path):
        print(f"ê²½ê³ : '{pdf_folder_path}' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    for file in os.listdir(pdf_folder_path):
        if file.endswith('.pdf'):
            pdf_path = os.path.join(pdf_folder_path, file)
            loader = PyPDFLoader(pdf_path)
            documents.extend(loader.load())

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(documents)
    
    model_name = "jhgan/ko-sbert-nli"
    model_kwargs = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
    encode_kwargs = {'normalize_embeddings': True}
    embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs)
    vectordb = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectordb.as_retriever()
    print("âœ… RAG ì¤€ë¹„ ì™„ë£Œ.\n")
    print("-" * 50)

    # --- ë‹¨ê³„ 2: ë¬¸ì œ ìƒì„± ì‹¤í–‰ ---
    print("--- [ë‹¨ê³„ 2: ë¬¸ì œ ìƒì„± ì‹¤í–‰] ---")
    
    # âœ¨âœ¨âœ¨ ì—¬ê¸°ì— ë¬¸ì œë¥¼ ìƒì„±í•˜ê³  ì‹¶ì€ 'ì£¼ì œ'ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. âœ¨âœ¨âœ¨
    topic_to_generate = "ë¶í•œì˜ ì‚¬ì´ë²„ ê³µê²© ì „ìˆ "
    print(f"ì£¼ì œ: '{topic_to_generate}'ì— ëŒ€í•œ ë¬¸ì œ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # 1. ì£¼ì œ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ(Context) ê²€ìƒ‰
    retrieved_docs = retriever.invoke(topic_to_generate)
    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹©ë‹ˆë‹¤.
    context_string = "\n---\n".join([doc.page_content for doc in retrieved_docs])
    
    print(f"\n[ê²€ìƒ‰ëœ Context ì¼ë¶€]:\n{context_string[:500]}...\n")

    # ì‹¤ì œ LLM ë¡œë“œ
    llm = load_real_llm()
    if not llm:
        print("LLM ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # 2. ê²€ìƒ‰ëœ Contextë¥¼ í”„ë¡¬í”„íŠ¸ì— ì£¼ì…í•˜ì—¬ ë¬¸ì œ ìƒì„± ìš”ì²­
    prompt_template = get_quiz_generation_prompt()
    prompt = ChatPromptTemplate.from_template(prompt_template)
    
    # ì²´ì¸ êµ¬ì„±: ì´ì œ ì²´ì¸ì€ retrieverë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    quiz_generation_chain = prompt | llm | StrOutputParser()
    
    # ì²´ì¸ ì‹¤í–‰
    response = quiz_generation_chain.invoke({
        "topic": topic_to_generate,
        "context": context_string
    })

    print("\n--- [ìµœì¢… ìƒì„± ê²°ê³¼] ---")
    print("LLMì´ ìƒì„±í•œ ì›ë³¸ ì¶œë ¥:")
    print(response)
    
    # âœ¨ [ì¶”ê°€] ìƒì„±ëœ JSON ë¬¸ìì—´ì„ íŒŒì‹±í•˜ì—¬ ê¹”ë”í•˜ê²Œ ì¶œë ¥
    try:
        # ëª¨ë¸ ì¶œë ¥ì—ì„œ JSON ì½”ë“œ ë¸”ë¡ë§Œ ì¶”ì¶œ
        json_str = response.split("```json")[1].split("```")[0].strip()
        quiz_data = json.loads(json_str)
        print("\nâœ… JSON íŒŒì‹± ì„±ê³µ! ê¹”ë”í•˜ê²Œ ì •ë¦¬ëœ ê²°ê³¼:")
        print(json.dumps(quiz_data, indent=2, ensure_ascii=False))
    except (IndexError, json.JSONDecodeError) as e:
        print(f"\nâŒ ì˜¤ë¥˜: ëª¨ë¸ì´ ìƒì„±í•œ ê²°ê³¼ì—ì„œ ìœ íš¨í•œ JSONì„ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (ì˜¤ë¥˜: {e})")

    print("-" * 50)


if __name__ == "__main__":
    main()