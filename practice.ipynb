{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "341fd34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.17.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from peft) (25.0)\n",
      "Requirement already satisfied: psutil in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from peft) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from peft) (2.7.1)\n",
      "Requirement already satisfied: transformers in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from peft) (4.53.0.dev0)\n",
      "Requirement already satisfied: tqdm in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from peft) (4.67.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: safetensors in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from peft) (0.33.0)\n",
      "Requirement already satisfied: filelock in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2023.9.2)\n",
      "Requirement already satisfied: requests in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.3)\n",
      "Requirement already satisfied: setuptools in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.7.14)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from transformers->peft) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/jeongseungmin/anaconda3/envs/fastapi/lib/python3.12/site-packages (from transformers->peft) (0.21.1)\n",
      "Downloading peft-0.17.0-py3-none-any.whl (503 kB)\n",
      "Downloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: accelerate, peft\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [peft]━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [peft]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.10.0 peft-0.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch \n",
    "import json \n",
    "from datasets import load_dataset # 허깅 페이스나 대용량 데이터를 쉽게 불러온다.\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, # 언어 모델을 자동으로 불러온다.\n",
    "    AutoTokenizer, # 모델에 맞는 토크나이저를 자동으로 불러온다\n",
    "    BitsAndBytesConfig, # 양자화 설정을 위한 라이브러리\n",
    "    TrainingArguments, # 모델 훈련에 필요한 모든 설정을 담는 라이블리\n",
    "    Trainer, # 실제 훈련 과정을 관리 하는 메일 라이브러리 \n",
    "    DataCollatorForLanguageModeling # 데이터를 배치 단위로 묶어준다.\n",
    ")\n",
    "\n",
    "# peft ( parameter-Efficient Fine Tuning ) LoRA와 관련된 설정\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2db3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"\"\n",
    "DATASET_PATH = \"\"\n",
    "OUTPUT_DIR = \"\"\n",
    "\n",
    "# MODEL에 맞는 tokenizer를 불러오고, 만일 해당 tokenizer에 padding 설정이 되어있지 않다면 , 이를 EOS 토큰으로 추가해준다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_model_code=True)\n",
    "if tokenizer.pad_token is None : \n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca26552",
   "metadata": {},
   "source": [
    "# 아래 방식은 attention mask를 모든 token에 부여하여, 질문, 선택지, 답변에 대해서 모두 모델이 예측하도록 훈련하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752cab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples) : \n",
    "    # prompt 템플릿 설정\n",
    "    prompt_template = \"\"\"\n",
    "        다음 질문에 대한 옳은 답변을 선택지에서 고르세요.\n",
    "\n",
    "        ### 질문 : \n",
    "        {question}\n",
    "\n",
    "        ### 선택지 : \n",
    "        1. {option_1}\n",
    "        2. {option_2}\n",
    "        3. {option_3}\n",
    "        4. {option_4}\n",
    "\n",
    "        ### 답변 : \n",
    "    \"\"\"\n",
    "\n",
    "    # examples 라는 데이터셋에서 option과 question, answer를 추출한다.\n",
    "    options = examples['options']\n",
    "    formatted_options = {\n",
    "        'option_1' : options.get('1',''), 'option_2' : options.get('2',''),\n",
    "        'option_3' : options.get('3', ''), 'option_4' : options.get('4', '')\n",
    "    }\n",
    "\n",
    "    # **formatted_options 여기서 '**'는 dictionary unpacking을 의미한다.\n",
    "    prompt = prompt_template.format(question=examples['question'], **formatted_options)\n",
    "    full_text = prompt + str(examples['answer'])\n",
    "\n",
    "    tokenized_output = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        # padding=\"longest\" 를 사용하면 배치 내 가장 긴 토큰으로 맞춰지게된다.\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    \"\"\"tokenized_output\n",
    "    해당 변순의 경우 크게 2가지로 구성되어있습니다.\n",
    "        input_ids : []  # 입력 prompt의 token들의 리스트\n",
    "        attention_maks : [] # 실제 입력과 패딩을 구별하기 위한 리스트\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    tokenized_output[\"label\"] = tokenized_output[\"input_ids\"].copy()\n",
    "    return tokenized_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577dfdc9",
   "metadata": {},
   "source": [
    "# answer 만을 기준으로 모델을 학습 시키는 코드 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 (수정 및 개선 버전)\n",
    "def preprocess_function_with_loss_mask(examples):\n",
    "    # 프롬프트 템플릿 정의\n",
    "    prompt_template = \"\"\"\n",
    "        다음 질문에 대한 옳은 답변을 선택지에서 고르세요.\n",
    "\n",
    "        ### 질문 : \n",
    "        {question}\n",
    "\n",
    "        ### 선택지 : \n",
    "        1. {option_1}\n",
    "        2. {option_2}\n",
    "        3. {option_3}\n",
    "        4. {option_4}\n",
    "\n",
    "        ### 답변 : \n",
    "    \"\"\"\n",
    "    \n",
    "    # 옵션 포맷팅\n",
    "    options = examples['options']\n",
    "    formatted_options = {\n",
    "        'option_1' : options.get('1', ''), 'option_2' : options.get('2', ''),\n",
    "        'option_3' : options.get('3', ''), 'option_4' : options.get('4', '')\n",
    "    }\n",
    "    \n",
    "    # 프롬프트와 답변 생성\n",
    "    prompt = prompt_template.format(question=examples['question'], **formatted_options)\n",
    "    answer = str(options['answer'])\n",
    "\n",
    "    # 1. 프롬프트 부분의 길이를 정확히 계산 (BOS 토큰 등 포함)\n",
    "    #    - 나중에 loss mask를 적용하기 위한 기준점\n",
    "    prompt_len = len(tokenizer(prompt, add_special_tokens=True)[\"input_ids\"])\n",
    "\n",
    "    # 2. 프롬프트와 답변을 합쳐서 토큰화\n",
    "    #    - 여기서는 아직 패딩이나 잘라내기를 하지 않습니다.\n",
    "    full_text = prompt + answer\n",
    "    #    - `add_special_tokens=True`는 보통 모델 입력 시작(BOS)과 끝(EOS)을 위해 필요합니다.\n",
    "    #      모델에 따라 `False`로 설정해야 할 수도 있습니다.\n",
    "    tokenized_full = tokenizer(full_text, add_special_tokens=True)\n",
    "    \n",
    "    input_ids = tokenized_full[\"input_ids\"]\n",
    "    \n",
    "    # 3. 레이블 생성 및 loss mask 적용\n",
    "    labels = input_ids.copy()\n",
    "    labels[:prompt_len] = [-100] * prompt_len # 프롬프트 부분은 loss 계산에서 제외\n",
    "\n",
    "    # 4. 토큰화된 결과와 원본 길이를 함께 반환\n",
    "    #    - 나중에 filter에서 길이를 확인하기 위함\n",
    "    tokenized_output = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": tokenized_full[\"attention_mask\"],\n",
    "        \"original_len\": len(input_ids) # 길이를 체크하기 위한 임시 컬럼 추가\n",
    "    }\n",
    "    \n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947a2479",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\" : \n",
    "    dataset = load_dataset('json', data_files=DATASET_PATH, split='train')\n",
    "\n",
    "    dataset = dataset.map(preprocess_function_with_loss_mask, remove_columns=list(dataset.features))\n",
    "\n",
    "    ############ token 길이가 1024가 넘는 경우 제거 #############\n",
    "    def filter_long_data(example):\n",
    "        return example['original_len'] <= 1024\n",
    "    filtered_dataset = dataset.filter(filter_long_data)\n",
    "    final_dataset = filtered_dataset.remove_columns(['original_len'])\n",
    "    ############# ############# ############# ############# ############# #############\n",
    "\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, # 4비트 양자화\n",
    "        bnb_4bit_use_double_quant=True, \n",
    "        bnb_4bit_quant_type=\"nf4\", # 가중치 분포 최적화\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 # 계산시 4비트를 16비트로 변환 -> 학습의 안정성\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model) # 양자화 모델의 안정성을 높혀주는 함수\n",
    "\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        task_type=\"CASUAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=5,\n",
    "        logging_steps=10,\n",
    "        fp16=True,\n",
    "        save_strategy=\"epoch\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    print('LoRA 파인튜닝 시작...')\n",
    "    trainer.train()\n",
    "\n",
    "    print(f\"학습된 LoRA 어뎁터를 {OUTPUT_DIR}에 저장합니다. \")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "\n",
    "    print('Fine Tunning 종료..')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastapi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
