nohup: ignoring input
데이터셋 전처리를 시작합니다...
'K-intelligence/Midm-2.0-Base-Instruct'의 베이스 모델을 로딩합니다...
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:02<00:11,  2.79s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:06<00:09,  3.15s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:09<00:06,  3.24s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:12<00:03,  3.29s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:14<00:00,  2.73s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:14<00:00,  2.93s/it]
trainable params: 20,447,232 || all params: 11,566,125,056 || trainable%: 0.1768
/workspace/2025-AI-Challeng-finance/train.py:110: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
LoRA 파인튜닝을 시작합니다...
  0%|          | 0/9 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/venv/main/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
