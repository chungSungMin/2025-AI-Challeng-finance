nohup: ignoring input
Original dataset size: 339
Final dataset size: 339
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:04<00:17,  4.48s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:10<00:15,  5.19s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:15<00:10,  5.27s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:21<00:05,  5.40s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:23<00:00,  4.46s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:23<00:00,  4.78s/it]
trainable params: 20,447,232 || all params: 11,566,125,056 || trainable%: 0.1768
/workspace/2025-AI-Challeng-finance/train.py:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
LoRA 파인튜닝 시작...
  0%|          | 0/110 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
  1%|          | 1/110 [00:15<27:20, 15.05s/it]  2%|▏         | 2/110 [00:29<26:31, 14.74s/it]  3%|▎         | 3/110 [00:44<26:05, 14.63s/it]  4%|▎         | 4/110 [00:58<25:46, 14.59s/it]  5%|▍         | 5/110 [01:13<25:29, 14.57s/it]  5%|▌         | 6/110 [01:27<25:13, 14.56s/it]  6%|▋         | 7/110 [01:42<24:57, 14.54s/it]  7%|▋         | 8/110 [01:56<24:42, 14.53s/it]  8%|▊         | 9/110 [02:11<24:28, 14.54s/it]  9%|▉         | 10/110 [02:25<24:14, 14.55s/it]                                                {'loss': 1.3996, 'grad_norm': 0.43153905868530273, 'learning_rate': 0.00018363636363636366, 'epoch': 0.47}
  9%|▉         | 10/110 [02:25<24:14, 14.55s/it] 10%|█         | 11/110 [02:40<23:59, 14.54s/it] 11%|█         | 12/110 [02:54<23:44, 14.53s/it] 12%|█▏        | 13/110 [03:09<23:30, 14.54s/it] 13%|█▎        | 14/110 [03:23<23:16, 14.55s/it] 14%|█▎        | 15/110 [03:38<23:01, 14.54s/it] 15%|█▍        | 16/110 [03:53<22:46, 14.54s/it] 15%|█▌        | 17/110 [04:07<22:31, 14.53s/it] 16%|█▋        | 18/110 [04:22<22:16, 14.52s/it] 17%|█▋        | 19/110 [04:36<22:01, 14.52s/it] 18%|█▊        | 20/110 [04:51<21:46, 14.52s/it]                                                {'loss': 1.0115, 'grad_norm': 0.3504432141780853, 'learning_rate': 0.00016545454545454545, 'epoch': 0.94}
 18%|█▊        | 20/110 [04:51<21:46, 14.52s/it] 19%|█▉        | 21/110 [05:05<21:32, 14.52s/it] 20%|██        | 22/110 [05:08<16:10, 11.03s/it] 21%|██        | 23/110 [05:24<18:01, 12.43s/it] 22%|██▏       | 24/110 [05:38<18:42, 13.06s/it] 23%|██▎       | 25/110 [05:53<19:07, 13.49s/it] 24%|██▎       | 26/110 [06:07<19:18, 13.80s/it] 25%|██▍       | 27/110 [06:22<19:23, 14.02s/it] 25%|██▌       | 28/110 [06:36<19:21, 14.17s/it] 26%|██▋       | 29/110 [06:51<19:16, 14.28s/it] 27%|██▋       | 30/110 [07:05<19:08, 14.35s/it]                                                {'loss': 0.902, 'grad_norm': 0.3314976394176483, 'learning_rate': 0.00014727272727272728, 'epoch': 1.38}
 27%|██▋       | 30/110 [07:05<19:08, 14.35s/it] 28%|██▊       | 31/110 [07:20<18:57, 14.40s/it] 29%|██▉       | 32/110 [07:34<18:45, 14.43s/it] 30%|███       | 33/110 [07:49<18:33, 14.46s/it] 31%|███       | 34/110 [08:03<18:20, 14.48s/it] 32%|███▏      | 35/110 [08:18<18:07, 14.50s/it] 33%|███▎      | 36/110 [08:32<17:53, 14.50s/it] 34%|███▎      | 37/110 [08:47<17:38, 14.51s/it] 35%|███▍      | 38/110 [09:02<17:25, 14.51s/it] 35%|███▌      | 39/110 [09:16<17:10, 14.51s/it] 36%|███▋      | 40/110 [09:31<16:55, 14.51s/it]                                                {'loss': 0.841, 'grad_norm': 0.3354986011981964, 'learning_rate': 0.0001290909090909091, 'epoch': 1.85}
 36%|███▋      | 40/110 [09:31<16:55, 14.51s/it] 37%|███▋      | 41/110 [09:45<16:41, 14.51s/it] 38%|███▊      | 42/110 [10:00<16:26, 14.51s/it] 39%|███▉      | 43/110 [10:14<16:11, 14.51s/it] 40%|████      | 44/110 [10:17<12:07, 11.02s/it] 41%|████      | 45/110 [10:32<13:23, 12.36s/it] 42%|████▏     | 46/110 [10:47<13:52, 13.01s/it] 43%|████▎     | 47/110 [11:01<14:08, 13.47s/it] 44%|████▎     | 48/110 [11:16<14:14, 13.78s/it] 45%|████▍     | 49/110 [11:31<14:14, 14.01s/it] 45%|████▌     | 50/110 [11:45<14:10, 14.17s/it]                                                {'loss': 0.8137, 'grad_norm': 0.29533085227012634, 'learning_rate': 0.00011090909090909092, 'epoch': 2.28}
 45%|████▌     | 50/110 [11:45<14:10, 14.17s/it] 46%|████▋     | 51/110 [12:00<14:02, 14.28s/it] 47%|████▋     | 52/110 [12:14<13:52, 14.35s/it] 48%|████▊     | 53/110 [12:29<13:41, 14.41s/it] 49%|████▉     | 54/110 [12:43<13:28, 14.44s/it] 50%|█████     | 55/110 [12:58<13:16, 14.48s/it] 51%|█████     | 56/110 [13:12<13:02, 14.49s/it] 52%|█████▏    | 57/110 [13:27<12:49, 14.51s/it] 53%|█████▎    | 58/110 [13:41<12:34, 14.52s/it] 54%|█████▎    | 59/110 [13:56<12:20, 14.52s/it] 55%|█████▍    | 60/110 [14:10<12:05, 14.52s/it]                                                {'loss': 0.7862, 'grad_norm': 0.3867288827896118, 'learning_rate': 9.272727272727273e-05, 'epoch': 2.75}
 55%|█████▍    | 60/110 [14:10<12:05, 14.52s/it] 55%|█████▌    | 61/110 [14:25<11:51, 14.53s/it] 56%|█████▋    | 62/110 [14:39<11:37, 14.53s/it] 57%|█████▋    | 63/110 [14:54<11:22, 14.53s/it] 58%|█████▊    | 64/110 [15:09<11:08, 14.53s/it] 59%|█████▉    | 65/110 [15:23<10:53, 14.52s/it] 60%|██████    | 66/110 [15:26<08:05, 11.03s/it] 61%|██████    | 67/110 [15:52<11:02, 15.42s/it] 62%|██████▏   | 68/110 [16:06<10:36, 15.15s/it] 63%|██████▎   | 69/110 [16:21<10:13, 14.96s/it] 64%|██████▎   | 70/110 [16:35<09:52, 14.82s/it]                                                {'loss': 0.761, 'grad_norm': 0.3368258476257324, 'learning_rate': 7.454545454545455e-05, 'epoch': 3.19}
 64%|██████▎   | 70/110 [16:35<09:52, 14.82s/it] 65%|██████▍   | 71/110 [16:50<09:34, 14.73s/it] 65%|██████▌   | 72/110 [17:04<09:17, 14.66s/it] 66%|██████▋   | 73/110 [17:19<09:01, 14.62s/it] 67%|██████▋   | 74/110 [17:33<08:45, 14.59s/it] 68%|██████▊   | 75/110 [17:48<08:29, 14.56s/it] 69%|██████▉   | 76/110 [18:02<08:14, 14.55s/it] 70%|███████   | 77/110 [18:17<07:59, 14.54s/it] 71%|███████   | 78/110 [18:31<07:45, 14.53s/it] 72%|███████▏  | 79/110 [18:46<07:30, 14.53s/it] 73%|███████▎  | 80/110 [19:00<07:15, 14.53s/it]                                                {'loss': 0.728, 'grad_norm': 0.3872966766357422, 'learning_rate': 5.636363636363636e-05, 'epoch': 3.66}
 73%|███████▎  | 80/110 [19:00<07:15, 14.53s/it] 74%|███████▎  | 81/110 [19:15<07:01, 14.53s/it] 75%|███████▍  | 82/110 [19:29<06:46, 14.52s/it] 75%|███████▌  | 83/110 [19:44<06:32, 14.54s/it] 76%|███████▋  | 84/110 [19:58<06:18, 14.54s/it] 77%|███████▋  | 85/110 [20:13<06:03, 14.53s/it] 78%|███████▊  | 86/110 [20:28<05:48, 14.53s/it] 79%|███████▉  | 87/110 [20:42<05:34, 14.52s/it] 80%|████████  | 88/110 [20:45<04:02, 11.03s/it] 81%|████████  | 89/110 [21:01<04:20, 12.41s/it] 82%|████████▏ | 90/110 [21:15<04:20, 13.04s/it]                                                {'loss': 0.7044, 'grad_norm': 0.41012176871299744, 'learning_rate': 3.818181818181819e-05, 'epoch': 4.09}
 82%|████████▏ | 90/110 [21:15<04:20, 13.04s/it] 83%|████████▎ | 91/110 [21:30<04:16, 13.48s/it] 84%|████████▎ | 92/110 [21:44<04:08, 13.79s/it] 85%|████████▍ | 93/110 [21:59<03:58, 14.01s/it] 85%|████████▌ | 94/110 [22:13<03:46, 14.16s/it] 86%|████████▋ | 95/110 [22:28<03:34, 14.27s/it] 87%|████████▋ | 96/110 [22:42<03:20, 14.34s/it] 88%|████████▊ | 97/110 [22:57<03:07, 14.39s/it] 89%|████████▉ | 98/110 [23:11<02:53, 14.43s/it] 90%|█████████ | 99/110 [23:26<02:39, 14.46s/it] 91%|█████████ | 100/110 [23:40<02:24, 14.48s/it]                                                 {'loss': 0.6838, 'grad_norm': 0.44231972098350525, 'learning_rate': 2e-05, 'epoch': 4.56}
 91%|█████████ | 100/110 [23:40<02:24, 14.48s/it] 92%|█████████▏| 101/110 [23:55<02:10, 14.49s/it] 93%|█████████▎| 102/110 [24:09<01:55, 14.50s/it] 94%|█████████▎| 103/110 [24:24<01:41, 14.51s/it] 95%|█████████▍| 104/110 [24:38<01:27, 14.51s/it] 95%|█████████▌| 105/110 [24:53<01:12, 14.52s/it] 96%|█████████▋| 106/110 [25:07<00:58, 14.51s/it] 97%|█████████▋| 107/110 [25:22<00:43, 14.51s/it] 98%|█████████▊| 108/110 [25:36<00:29, 14.52s/it] 99%|█████████▉| 109/110 [25:51<00:14, 14.52s/it]100%|██████████| 110/110 [25:54<00:00, 11.03s/it]                                                 {'loss': 0.7063, 'grad_norm': 0.8817723393440247, 'learning_rate': 1.818181818181818e-06, 'epoch': 5.0}
100%|██████████| 110/110 [25:54<00:00, 11.03s/it]                                                 {'train_runtime': 1555.3237, 'train_samples_per_second': 1.09, 'train_steps_per_second': 0.071, 'train_loss': 0.8488662633028897, 'epoch': 5.0}
100%|██████████| 110/110 [25:55<00:00, 11.03s/it]100%|██████████| 110/110 [25:55<00:00, 14.14s/it]
학습된 LoRA 어뎁터를 ./midm-lora-adapter-trainer에 저장합니다. 
Fine Tunning 종료..
