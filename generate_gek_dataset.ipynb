{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf06b956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 총 4개의 파일을 병합합니다: ['/workspace/CyberMetric/CyberMetric-10000-v1.json', '/workspace/CyberMetric/CyberMetric-2000-v1.json', '/workspace/CyberMetric/CyberMetric-500-v1.json', '/workspace/CyberMetric/CyberMetric-80-v1.json']\n",
      "\n",
      "✅ 병합이 완료되었습니다. 총 12760개의 질문이 '/workspace/merged_questions.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge_json_files(input_pattern: str, output_filename: str):\n",
    "    \"\"\"\n",
    "    주어진 패턴과 일치하는 모든 JSON 파일을 찾아 하나의 파일로 병합합니다.\n",
    "\n",
    "    :param input_pattern: 찾을 파일의 패턴 (예: 'data/*.json')\n",
    "    :param output_filename: 저장할 최종 파일의 이름\n",
    "    \"\"\"\n",
    "    # 1. 병합할 JSON 파일 목록 찾기\n",
    "    json_files = glob.glob(input_pattern)\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"❌ 오류: '{input_pattern}' 패턴과 일치하는 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    print(f\"📄 총 {len(json_files)}개의 파일을 병합합니다: {json_files}\")\n",
    "\n",
    "    # 2. 모든 'questions'를 담을 빈 리스트 생성\n",
    "    all_questions = []\n",
    "\n",
    "    # 3. 각 파일을 순회하며 'questions' 리스트를 읽고 합치기\n",
    "    for file_path in json_files:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                # 'questions' 키가 존재하고, 그 값이 리스트인 경우에만 추가\n",
    "                if 'questions' in data and isinstance(data['questions'], list):\n",
    "                    all_questions.extend(data['questions'])\n",
    "                else:\n",
    "                    print(f\"⚠️ 경고: '{file_path}' 파일에 'questions' 리스트가 없거나 형식이 잘못되어 건너뜁니다.\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"⚠️ 경고: '{file_path}' 파일이 올바른 JSON 형식이 아니므로 건너뜁니다.\")\n",
    "        except Exception as e:\n",
    "            print(f\"'{file_path}' 처리 중 오류 발생: {e}\")\n",
    "\n",
    "    # 4. 최종적으로 합쳐진 데이터 구조 만들기\n",
    "    final_data = {\"questions\": all_questions}\n",
    "\n",
    "    # 5. 새로운 JSON 파일에 저장\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        # indent=4: 보기 좋게 4칸 들여쓰기 적용\n",
    "        # ensure_ascii=False: 한글 등 비-ASCII 문자가 깨지지 않도록 설정\n",
    "        json.dump(final_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n✅ 병합이 완료되었습니다. 총 {len(all_questions)}개의 질문이 '{output_filename}' 파일에 저장되었습니다.\")\n",
    "\n",
    "# --- 메인 실행 부분 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # ⭐️ 여기에 병합할 JSON 파일들이 있는 폴더 경로와 파일 패턴을 지정하세요.\n",
    "    INPUT_PATTERN = '/workspace/CyberMetric/*.json' \n",
    "    \n",
    "    # ★★★ 수정된 부분: 저장 경로를 /workspace/로 명시 ★★★\n",
    "    OUTPUT_FILE = '/workspace/merged_questions.json'\n",
    "    \n",
    "    merge_json_files(INPUT_PATTERN, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b1310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12760\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "file_path = \"/workspace/merged_questions.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(len(dataset[\"questions\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b63f495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Which of the following authentication factors involves something you know?', 'answer': 'Username'}\n"
     ]
    }
   ],
   "source": [
    "document_len = len(dataset[\"questions\"])\n",
    "real_data = dataset[\"questions\"]\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for i in range(document_len):\n",
    "\n",
    "    question = real_data[i][\"question\"]\n",
    "    answer = real_data[i][\"answers\"].get(real_data[i][\"solution\"])\n",
    "\n",
    "    docs = {\n",
    "        \"question\" : question,\n",
    "        \"answer\" : answer\n",
    "    }\n",
    "\n",
    "    all_docs.append(docs)\n",
    "\n",
    "print(all_docs[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfbe4c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /venv/main/lib/python3.12/site-packages (4.55.0)\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.12/site-packages (2.8.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /venv/main/lib/python3.12/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.12/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /venv/main/lib/python3.12/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /venv/main/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /venv/main/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /venv/main/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /venv/main/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /venv/main/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /venv/main/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6050de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'facebook/nllb-200-distilled-600M' 번역 모델을 로딩합니다... (시간이 걸릴 수 있습니다)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38497b2dcd6544abbfe5f01352939673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51479fba385c40f58904aa94c54819df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec908a552cb4880a14131cbf60b676b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a27c1628efb49dcbb5facfd1b636c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edc8b65dd104dc5aea408daff0cbb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5631384e731048d9948989b837325e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6451cb4c70440199d9ef38eabfb47e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c138fa7daf75454f83525838921ea066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 번역 모델 로딩 완료.\n",
      "\n",
      "총 12760개의 항목에 대한 번역을 시작합니다...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f408e06e31248aca7581d45e9bddbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "번역 진행 중:   0%|          | 0/12760 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 번역이 완료되었습니다.\n",
      "\n",
      "--- [번역 결과 샘플] ---\n",
      "{\n",
      "  \"question\": \"다음 중 어느 것 은 정보 의 비밀 에 관한 것 입니까?\",\n",
      "  \"answer\": \"비공개성\"\n",
      "}\n",
      "\n",
      "💾 모든 번역 결과가 'translated_qa_ko.jsonl' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import pipeline\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "OUTPUT_FILENAME = \"/workspace/translated_qa_ko.jsonl\"\n",
    "\n",
    "print(f\"'{MODEL_NAME}' 번역 모델을 로딩합니다... (시간이 걸릴 수 있습니다)\")\n",
    "\n",
    "# GPU 사용 가능 여부 확인 후 장치 설정\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# 번역을 위한 pipeline 객체 생성\n",
    "translator = pipeline(\n",
    "    'translation',\n",
    "    model=MODEL_NAME,\n",
    "    tokenizer=MODEL_NAME,\n",
    "    src_lang='eng_Latn',  # 소스 언어: 영어 (Latin script)\n",
    "    tgt_lang='kor_Hang',  # 타겟 언어: 한국어 (Hangul script)\n",
    "    device=device         # GPU가 있으면 사용, 없으면 CPU(-1) 사용\n",
    ")\n",
    "\n",
    "print(\"✅ 번역 모델 로딩 완료.\")\n",
    "\n",
    "# --- 3. 데이터 번역 실행 ---\n",
    "\n",
    "# 번역된 결과를 저장할 새로운 리스트\n",
    "translated_docs = []\n",
    "\n",
    "print(f\"\\n총 {len(all_docs)}개의 항목에 대한 번역을 시작합니다...\")\n",
    "\n",
    "# tqdm을 사용하여 진행 상황을 시각적으로 표시\n",
    "for doc in tqdm(all_docs, desc=\"번역 진행 중\"):\n",
    "    # 질문과 답변 텍스트를 리스트로 묶어 한 번에 번역 (더 효율적)\n",
    "    texts_to_translate = [\n",
    "        doc.get(\"question\", \"\"),\n",
    "        doc.get(\"answer\", \"\")\n",
    "    ]\n",
    "    \n",
    "    # 번역 실행\n",
    "    translation_results = translator(texts_to_translate, max_length=512)\n",
    "    \n",
    "    # 번역 결과 추출\n",
    "    translated_question = translation_results[0]['translation_text']\n",
    "    translated_answer = translation_results[1]['translation_text']\n",
    "    \n",
    "    # 번역된 내용으로 새로운 딕셔너리 생성\n",
    "    translated_doc = {\n",
    "        \"question\": translated_question,\n",
    "        \"answer\": translated_answer\n",
    "    }\n",
    "    \n",
    "    # 최종 리스트에 추가\n",
    "    translated_docs.append(translated_doc)\n",
    "\n",
    "print(\"\\n✅ 번역이 완료되었습니다.\")\n",
    "\n",
    "# --- 4. 번역 결과 확인 및 파일 저장 ---\n",
    "\n",
    "print(\"\\n--- [번역 결과 샘플] ---\")\n",
    "print(json.dumps(translated_docs[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "# 번역된 전체 결과를 jsonl 파일로 저장\n",
    "with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "    for doc in translated_docs:\n",
    "        # 각 딕셔너리를 JSON 문자열로 변환하여 파일에 한 줄씩 씀\n",
    "        f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"\\n💾 모든 번역 결과가 '{OUTPUT_FILENAME}' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💾 모든 번역 결과가 '/workspace/translated_qa_ko.jsonl' 파일에 저장되었습니다.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# 번역된 전체 결과를 jsonl 파일로 저장\n",
    "OUTPUT_FILENAME = \"/workspace/translated_qa_ko.jsonl\"\n",
    "\n",
    "\n",
    "with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "    for doc in translated_docs:\n",
    "        # 각 딕셔너리를 JSON 문자열로 변환하여 파일에 한 줄씩 씀\n",
    "        f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"\\n💾 모든 번역 결과가 '{OUTPUT_FILENAME}' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a213a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
