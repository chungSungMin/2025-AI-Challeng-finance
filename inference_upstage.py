import torch
import pandas as pd
import re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
# peft is no longer needed as we are not loading a LoRA adapter
# from peft import PeftModel 
import os
from datasets import load_dataset
from langchain_community.document_loaders import PyPDFLoader

# RAG libraries
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# --- 1. Configuration ---

# ‚òÖ‚òÖ‚òÖ Changed Model ID to SOLAR ‚òÖ‚òÖ‚òÖ
BASE_MODEL_ID = "upstage/SOLAR-10.7B-Instruct-v1.0"

# --- LoRA adapter path is no longer needed ---
# LORA_ADAPTER_PATH = "/path/to/your/lora/adapter" 

# Test data and submission file paths
TEST_CSV_PATH = '/workspace/open/test.csv'
SUBMISSION_CSV_PATH = './submission_rag_solar_inference.csv' 

# List of PDF files for the RAG knowledge base
RAG_DATA_FILES = [
    "/workspace/·ÑÄ·Ö¢·Ñã·Öµ·Ü´·ÑÄ·Ö≥·Ü∑·Ñã·Ö≤·Üº·Ñé·Ö¢·ÑÄ·ÖØ·Ü´·Ñã·Ö¥ ·ÑÄ·Ö™·Ü´·ÑÖ·Öµ ·ÑÜ·Öµ·Üæ ·ÑÄ·Ö¢·Ñã·Öµ·Ü´·ÑÄ·Ö≥·Ü∑·Ñã·Ö≤·Üº·Ñé·Ö¢·ÑÜ·ÖÆ·Ñå·Ö°·Ñã·Ö¥ ·Ñá·Ö©·Ñí·Ö©·Ñã·Ö¶ ·ÑÄ·Ö™·Ü´·Ñí·Ö°·Ü´ ·Ñá·Ö•·Ü∏·ÑÖ·Ö≤·ÜØ(·Ñá·Ö•·Ü∏·ÑÖ·Ö≤·ÜØ)(·Ñå·Ö¶20369·Ñí·Ö©)(20241017).pdf",
    "/workspace/·ÑÄ·Ö¢·Ñã·Öµ·Ü´·Ñå·Ö•·Üº·Ñá·Ö© ·Ñá·Ö©·Ñí·Ö©·Ñá·Ö•·Ü∏(·Ñá·Ö•·Ü∏·ÑÖ·Ö≤·ÜØ)(·Ñå·Ö¶19234·Ñí·Ö©)(20250313) (1).pdf",
    # "/workspace/·ÑÄ·Ö•·Ñá·Ö•·ÑÇ·Ö•·Ü´·Ñâ·Ö≥.pdf",
    # "/workspace/·ÑÄ·Öß·Üº·Ñé·Ö°·ÜØ·ÑÄ·Ö©·Üº·ÑÜ·ÖÆ·Ñã·ÖØ·Ü´ ·ÑÉ·Ö≥·Üº·Ñã·Ö¥ ·ÑÄ·Ö¢·Ñã·Öµ·Ü´·Ñå·Ö•·Üº·Ñá·Ö© ·Ñé·Ö•·ÑÖ·Öµ·Ñã·Ö¶ ·ÑÄ·Ö™·Ü´·Ñí·Ö°·Ü´ ·ÑÄ·Ö≤·Ñå·Ö•·Üº(·ÑÉ·Ö¢·Ñê·Ö©·Üº·ÑÖ·Öß·Üº·ÑÖ·Öß·Üº)(·Ñå·Ö¶35039·Ñí·Ö©)(20241203).pdf",
    # "/workspace/·ÑÄ·Ö≥·Ü∑·Ñã·Ö≤·Üº·Ñá·Ö©·Ñã·Ö°·Ü´·Ñã·Öß·Ü´·ÑÄ·ÖÆ·Ñã·ÖØ·Ü´.pdf",
    # "/workspace/·ÑÄ·Ö≥·Ü∑·Ñã·Ö≤·Üº·Ñâ·Ö©·Ñá·Öµ·Ñå·Ö° ·Ñá·Ö©·Ñí·Ö©·Ñã·Ö¶ ·ÑÄ·Ö™·Ü´·Ñí·Ö°·Ü´ ·Ñá·Ö•·Ü∏·ÑÖ·Ö≤·ÜØ(·Ñá·Ö•·Ü∏·ÑÖ·Ö≤·ÜØ)(·Ñå·Ö¶20305·Ñí·Ö©)(20240814).pdf",
    # "/workspace/·ÑÄ·Ö≥·Ü∑·Ñã·Ö≤·Üº·Ñâ·Öµ·ÜØ·ÑÜ·Öß·Üº·ÑÄ·Ö•·ÑÖ·Ö¢ ·ÑÜ·Öµ·Üæ ·Ñá·Öµ·ÑÜ·Öµ·ÜØ·Ñá·Ö©·Ñå·Ö°·Üº·Ñã·Ö¶ ·ÑÄ·Ö™·Ü´·Ñí·Ö°·Ü´ ·Ñá·Ö•·Ü∏·ÑÖ·Ö≤·ÜØ ·Ñâ·Öµ·Ñí·Ö¢·Üº·ÑÄ·Ö≤·Ñé·Öµ·Ü®(·Ñé·Ö©·Üº·ÑÖ·Öµ·ÑÖ·Öß·Üº)(·Ñå·Ö¶01406·Ñí·Ö©)(20170726).pdf",
    # "/workspace/·ÑÖ·Ö¢·Ü´·Ñâ·Ö•·Ü∑·Ñã·Ö∞·Ñã·Ö•.pdf",
    # "/workspace/·ÑÜ·Ö°·Ñã·Öµ·ÑÉ·Ö¶·Ñã·Öµ·Ñê·Ö•.pdf",
    # "/workspace/·ÑÜ·Ö¶·Ñê·Ö°·Ñá·Ö•·Ñâ·Ö≥.pdf",
    "/workspace/·Ñá·Ö•·Ü∏·Ñã·ÖØ·Ü´ ·ÑÄ·Ö¢·Ñã·Öµ·Ü´·Ñå·Ö•·Üº·Ñá·Ö© ·Ñá·Ö©·Ñí·Ö©·Ñã·Ö¶ ·ÑÄ·Ö™·Ü´·Ñí·Ö°·Ü´ ·ÑÄ·Ö≤·Ñé·Öµ·Ü®(·ÑÉ·Ö¢·Ñá·Ö•·Ü∏·Ñã·ÖØ·Ü´·ÑÄ·Ö≤·Ñé·Öµ·Ü®)(·Ñå·Ö¶03109·Ñí·Ö©)(20240315).pdf",
    # "/workspace/·Ñã·Ö°·Ñã·ÖÆ·Ü∫·Ñâ·Ö©·Ñâ·Öµ·Üº.pdf",
    # "/workspace/·Ñå·Ö•·Üº·Ñá·Ö©_·Ñá·Ö©·Ñã·Ö°·Ü´.pdf",
    # "/workspace/·Ñè·Ö≥·ÜØ·ÑÖ·Ö°·Ñã·ÖÆ·ÑÉ·Ö≥·Ñè·Ö•·Ü∑·Ñë·Ö≤·Ñê·Öµ·Üº ·Ñá·Ö°·ÜØ·Ñå·Ö•·Ü´ ·ÑÜ·Öµ·Üæ ·Ñã·Öµ·Ñã·Ö≠·Üº·Ñå·Ö° ·Ñá·Ö©·Ñí·Ö©·Ñã·Ö¶ ·ÑÄ·Ö™·Ü´·Ñí·Ö°·Ü´ ·Ñá·Ö•·Ü∏·ÑÖ·Ö≤·ÜØ(·Ñá·Ö•·Ü∏·ÑÖ·Ö≤·ÜØ)(·Ñå·Ö¶20732·Ñí·Ö©)(20250131).pdf",
]
# Embedding model and vector DB path configuration
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"
FAISS_DB_PATH = "./faiss_index_laws_solar"


# --- 2. RAG Backend Build Function ---
def build_or_load_rag_backend():
    """PDF Î∞è JSONL ÌååÏùºÎì§Î°úÎ∂ÄÌÑ∞ FAISS Î≤°ÌÑ∞ DBÎ•º Íµ¨Ï∂ïÌïòÍ±∞ÎÇò Í∏∞Ï°¥ DBÎ•º Î°úÎìúÌï©ÎãàÎã§."""
    
    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    if os.path.exists(FAISS_DB_PATH):
        print(f"‚è≥ Í∏∞Ï°¥ Î≤°ÌÑ∞ DBÎ•º '{FAISS_DB_PATH}'ÏóêÏÑú Î°úÎìúÌï©ÎãàÎã§...")
        vector_db = FAISS.load_local(
            FAISS_DB_PATH, 
            embedding_model, 
            allow_dangerous_deserialization=True
        )
        print("‚úÖ Î≤°ÌÑ∞ DB Î°úÎìú ÏôÑÎ£å.")
    else:
        print(f"‚è≥ '{RAG_DATA_FILES}' ÌååÏùºÎì§Î°ú ÏÉàÎ°úÏö¥ Î≤°ÌÑ∞ DBÎ•º Íµ¨Ï∂ïÌï©ÎãàÎã§...")
        
        all_documents = []
        for file_path in tqdm(RAG_DATA_FILES, desc="üìö PDF ÌååÏùº Î°úÎî© Ï§ë"):
            try:
                loader = PyPDFLoader(file_path)
                documents_from_pdf = loader.load() 
                all_documents.extend(documents_from_pdf)
            except Exception as e:
                print(f"‚ö†Ô∏è Í≤ΩÍ≥†: '{file_path}' ÌååÏùºÏùÑ Ï≤òÎ¶¨ÌïòÎäî Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}")
        
        if not all_documents:
            print("‚ùå Ïò§Î•ò: Ï≤òÎ¶¨Ìï† Î¨∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§. RAG_DATA_FILES Í≤ΩÎ°úÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.")
            exit()
            
        # ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ
        # ‚òÖ‚òÖ‚òÖ Ïù¥ Î∂ÄÎ∂ÑÏù¥ ÌïµÏã¨ÏûÖÎãàÎã§! Î¨∏ÏÑúÎ•º ÏûëÏùÄ Îã®ÏúÑÎ°ú Î∂ÑÌï† ‚òÖ‚òÖ‚òÖ
        # ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=124)
        split_documents = text_splitter.split_documents(all_documents)
        print(f"‚úÖ Î¨∏ÏÑúÎ•º Ï¥ù {len(split_documents)}Í∞úÏùò ÏûëÏùÄ Ï°∞Í∞Å(chunk)ÏúºÎ°ú Î∂ÑÌï†ÌñàÏäµÎãàÎã§.")
        # ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ

        print(f"Ï¥ù {len(split_documents)}Í∞úÏùò ÌéòÏù¥ÏßÄ(Document)Î•º ÏûÑÎ≤†Îî©Ìï©ÎãàÎã§...")
        # Î∂ÑÌï†Îêú Î¨∏ÏÑúÎ•º Í∏∞Î∞òÏúºÎ°ú Î≤°ÌÑ∞ DBÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§.
        vector_db = FAISS.from_documents(split_documents, embedding_model)
        
        vector_db.save_local(FAISS_DB_PATH)
        print(f"‚úÖ ÏÉàÎ°úÏö¥ Î≤°ÌÑ∞ DB Íµ¨Ï∂ï Î∞è Ï†ÄÏû• ÏôÑÎ£å: '{FAISS_DB_PATH}'")

    # Í≤ÄÏÉâÍ∏∞Îäî Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
    return vector_db.as_retriever(search_kwargs={'k': 3})


# --- 3. Utility and Prompt Functions ---

def is_multiple_choice(question_text: str) -> bool:
    """Determines if a question is multiple-choice or open-ended."""
    lines = question_text.strip().split("\n")
    option_count = sum(bool(re.match(r"^\s*[1-9][0-9]?[\.\s]", line)) for line in lines)
    return option_count >= 2

def extract_question_and_choices(full_text: str) -> tuple[str, list[str]]:
    """Separates the core question and the list of choices in a multiple-choice question."""
    lines = full_text.strip().split("\n")
    q_lines = []
    options = []
    for line in lines:
        if re.match(r"^\s*[1-9][0-9]?[\.\s]", line):
            options.append(line.strip())
        else:
            q_lines.append(line.strip())
    question = " ".join(q_lines)
    return question, options

def make_prompt(text: str) -> str:
    """Creates a prompt for generating an answer using only the model's internal knowledge."""
    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        prompt = f"""### Instruction:
Provide only the number of the correct answer for the following question. Do not add any other explanations.

### Question:
{question}

### Choices:
{chr(10).join(options)}

### Answer:
"""
    else:
        prompt = f"""### Instruction:
Describe the answer to the following question in a complete Korean sentence, focusing on core keywords. Use your background knowledge even if not explicitly mentioned in any reference document. Do not use phrases like "According to the document...".

### Question:
{text}

### Answer:
"""
    return prompt

def make_rag_prompt(text: str, context: str) -> str:
    """Creates a prompt that includes the retrieved RAG context."""
    if not context.strip():
        return make_prompt(text)

    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        prompt = f"""### Instruction:
Based on the 'Reference Documents' provided, output only the number of the correct answer for the following question. Do not add any other explanations.

### Reference Documents:
{context}

### Question:
{question}

### Choices:
{chr(10).join(options)}

### Answer:
"""
    else:
        prompt = f"""### Instruction:
Based on the content of the 'Reference Documents', describe the answer to the following question in a complete Korean sentence. Do not use expressions like "According to the Reference Documents". Use your background knowledge even if not explicitly mentioned.

### Reference Documents:
{context}

### Question:
{text}

### Answer:
"""
    return prompt

# --- 4. Model and Tokenizer Loading ---

print("‚è≥ Loading the model and tokenizer...")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# ‚òÖ‚òÖ‚òÖ Directly load the SOLAR model ‚òÖ‚òÖ‚òÖ
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quantization_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# --- LoRA loading and merging section is removed ---
# print(f"‚è≥ Loading LoRA adapter from '{LORA_ADAPTER_PATH}' and applying to the model...")
# try:
#     model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
# except Exception as e:
#     print(f"‚ùå Error: Failed to load LoRA adapter. Please check the path: {LORA_ADAPTER_PATH}")
#     print(e)
#     exit()
# print("‚è≥ Merging LoRA weights into the base model...")
# model = model.merge_and_unload()

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)
print("‚úÖ Model loading and setup complete.")


# --- 5. Post-processing and Helper Functions ---

def post_process_answer(generated_text: str, original_question: str) -> str:
    """Enhanced function to extract and clean the final answer from the generated text."""
    answer = generated_text.strip()
    
    if not answer:
        return "1"  # Default to '1' if the answer is empty

    if "###" in answer:
        answer = answer.split("###")[0].strip()
        
    if is_multiple_choice(original_question):
        # Step 1: Extract number from clear patterns like "The answer is 5"
        match = re.search(r'(?:Ï†ïÎãµÏùÄ|ÎãµÏùÄ|ÏÑ†ÌÉùÏùÄ)\s*\D*(\d+)', answer)
        if match:
            return match.group(1)

        # Step 2: Extract number from patterns like "5Î≤à", "5."
        match = re.search(r'\b(\d+)\s*(?:Î≤à|Î≤àÏûÖÎãàÎã§|\.)', answer)
        if match:
            return match.group(1)

        # Step 3: Extract number at the beginning of the string
        match = re.search(r"^\s*(\d+)", answer)
        if match:
            return match.group(1)

        # Step 4: If no match yet, find the first number in the entire text
        match = re.search(r'(\d+)', answer)
        if match:
            return match.group(1)
            
        # Step 5: If still no number is found, default to '1'
        return "1"
    
    return answer if answer else "Failed to generate an answer."

def is_code_detected(text: str) -> bool:
    """Checks for code snippets in the generated text based on simple keywords."""
    code_keywords = ['def ', 'import ', 'class ', 'r\'', 'sys.stdout', 'ans_qna']
    text_lower = text.lower()
    return any(keyword in text_lower for keyword in code_keywords)


# --- 6. Main Execution with RAG ---
if __name__ == "__main__":
    retriever = build_or_load_rag_backend()

    try:
        test_df = pd.read_csv(TEST_CSV_PATH)
        print(f"‚úÖ Successfully loaded test data from '{TEST_CSV_PATH}'.")
    except FileNotFoundError:
        print(f"‚ùå Error: Could not find '{TEST_CSV_PATH}'. Please check the file path.")
        exit()

    preds = []
    MAX_RETRIES = 3 

    for index, q in tqdm(enumerate(test_df['Question']), total=len(test_df), desc="üöÄ RAG inference in progress"):
        
        try:
            retrieved_docs = retriever.get_relevant_documents(q)
            context_text = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
        except Exception as e:
            print(f"‚ö†Ô∏è An error occurred during document retrieval for TEST_{index}: {e}")
            context_text = ""

        prompt = make_rag_prompt(q, context_text)
        
        is_valid_answer = False
        retries = 0
        generated_text = ""

        while not is_valid_answer and retries < MAX_RETRIES:
            if retries > 0:
                print(f"\nüîÑ Retrying answer generation for TEST_{index}... ({retries}/{MAX_RETRIES})")

            output = pipe(
                prompt, 
                max_new_tokens=512,
                temperature=0.1 + (retries * 0.15),
                top_p=0.9,
                do_sample=True,
                return_full_text=False,
                eos_token_id=tokenizer.eos_token_id
            )
            
            generated_text = output[0]['generated_text']

            if is_code_detected(generated_text):
                retries += 1
                if retries == MAX_RETRIES:
                    print(f"‚ùå Exceeded max retries for TEST_{index}. Using the last generated answer.")
                    is_valid_answer = True
            else:
                is_valid_answer = True

        pred_answer = post_process_answer(generated_text, original_question=q)
        preds.append(pred_answer)

    print("\nüìÑ Inference complete. Generating submission file...")
    try:
        sample_submission = pd.read_csv('/workspace/open/sample_submission.csv')
        sample_submission['Answer'] = preds
        sample_submission.to_csv(SUBMISSION_CSV_PATH, index=False, encoding='utf-8-sig')
        print(f"‚úÖ Submission file created successfully: '{SUBMISSION_CSV_PATH}'")
    except FileNotFoundError:
        print(f"‚ùå Error: '/workspace/open/sample_submission.csv' not found. Please check the file path.")