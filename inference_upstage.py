import torch
import pandas as pd
import re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from peft import PeftModel
import os

# --- 1. ì„¤ì • (Configuration) ---

# â­ï¸ ìˆ˜ì • 1: ê¸°ë³¸ ëª¨ë¸ IDë¥¼ SOLARë¡œ ë³€ê²½
BASE_MODEL_ID = "upstage/SOLAR-10.7B-Instruct-v1.0"

# â­ï¸ ì‚¬ìš©ì ì„¤ì •: í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ê°€ ì €ì¥ëœ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.
# SOLAR ëª¨ë¸ë¡œ ìƒˆë¡œ í•™ìŠµí•œ LoRA ì–´ëŒ‘í„° ê²½ë¡œë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
LORA_ADAPTER_PATH = "/workspace/2025-AI-Challeng-finance/midm-lora-adapter-combined-laws/checkpoint-22" 

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ì œì¶œ íŒŒì¼ ê²½ë¡œ
TEST_CSV_PATH = '/workspace/open/test.csv'
SUBMISSION_CSV_PATH = './submission_solar_inference.csv' 

# --- 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---

def is_multiple_choice(question_text: str) -> bool:
    """ì§ˆë¬¸ì´ ê°ê´€ì‹ì¸ì§€ íŒë³„í•©ë‹ˆë‹¤."""
    lines = question_text.strip().split("\n")
    # ìˆ«ìë¡œ ì‹œì‘í•˜ê³  ê³µë°±ì´ë‚˜ ì ìœ¼ë¡œ êµ¬ë¶„ë˜ëŠ” ì„ íƒì§€ê°€ 2ê°œ ì´ìƒì¼ ê²½ìš° ê°ê´€ì‹ìœ¼ë¡œ íŒë‹¨
    option_count = sum(bool(re.match(r"^\s*[1-9][0-9]?[\.\s]", line)) for line in lines)
    return option_count >= 2

def extract_question_and_choices(full_text: str) -> tuple[str, list[str]]:
    """ê°ê´€ì‹ ì§ˆë¬¸ì—ì„œ ìˆœìˆ˜ ì§ˆë¬¸ê³¼ ì„ íƒì§€ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤."""
    lines = full_text.strip().split("\n")
    q_lines = []
    options = []
    for line in lines:
        if re.match(r"^\s*[1-9][0-9]?[\.\s]", line):
            options.append(line.strip())
        else:
            q_lines.append(line.strip())
    question = " ".join(q_lines)
    return question, options

# â­ï¸ ìˆ˜ì • 2: SOLAR ëª¨ë¸ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ ë³€ê²½
def make_prompt(text: str) -> str:
    """
    ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ SOLAR ëª¨ë¸ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # SOLARì˜ ê³µì‹ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: ### User:\n{instruction}\n\n### Assistant:\n
    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        # ê°ê´€ì‹ ì§ˆë¬¸ì„ instructionìœ¼ë¡œ êµ¬ì„±
        instruction = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë‹µë³€ì˜ 'ë²ˆí˜¸'ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n### ì§ˆë¬¸:\n{question}\n\n### ì„ íƒì§€:\n{chr(10).join(options)}"
    else:
        # ì£¼ê´€ì‹ ì§ˆë¬¸ì„ instructionìœ¼ë¡œ êµ¬ì„±
        instruction = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ ë‚´ìš©ì„ ë‹´ì•„ ì™„ë²½í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”.\n\n### ì§ˆë¬¸:\n{text}"
    
    # ìµœì¢… í”„ë¡¬í”„íŠ¸ ë°˜í™˜
    return f"### User:\n{instruction}\n\n### Assistant:\n"

# --- 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---

print("â³ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quantization_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"â³ '{LORA_ADAPTER_PATH}'ì—ì„œ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë”©í•˜ì—¬ ëª¨ë¸ì— ì ìš©í•©ë‹ˆë‹¤...")
try:
    model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: LoRA ì–´ëŒ‘í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {LORA_ADAPTER_PATH}")
    print(f"ìƒì„¸ ì˜¤ë¥˜: {e}")
    # SOLAR ëª¨ë¸ì€ LoRA ì—†ì´ë„ ì„±ëŠ¥ì´ ì¢‹ìœ¼ë¯€ë¡œ, ì–´ëŒ‘í„° ë¡œë”© ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ëª¨ë¸ë¡œ ê³„ì† ì§„í–‰í•˜ë„ë¡ ì„¤ì •
    print("âš ï¸ ê²½ê³ : LoRA ì–´ëŒ‘í„° ë¡œë”©ì— ì‹¤íŒ¨í•˜ì—¬ ê¸°ë³¸ ëª¨ë¸ë¡œ ì¶”ë¡ ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
    model = base_model 

if isinstance(model, PeftModel):
    print("â³ LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©í•©ë‹ˆë‹¤...")
    model = model.merge_and_unload()

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)
print("âœ… ëª¨ë¸ ë¡œë”© ë° ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- 4. ì¶”ë¡  ë° í›„ì²˜ë¦¬ ---

# â­ï¸ ìˆ˜ì • 3: í›„ì²˜ë¦¬ ë¡œì§ ê°•í™”
def post_process_answer(generated_text: str, original_question: str) -> str:
    """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ìµœì¢… ë‹µë³€ì„ ì¶”ì¶œí•˜ê³  ì •ë¦¬í•˜ëŠ” ê°•í™”ëœ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    answer = generated_text.strip()
    
    if not answer:
        return "1" if is_multiple_choice(original_question) else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ë‹µë³€ì— í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ì œê±° (ì˜ˆ: ### Assistant:)
    if "###" in answer:
        answer = answer.split("###")[0].strip()
        
    if is_multiple_choice(original_question):
        # 1ë‹¨ê³„: "ì •ë‹µì€ 5", "ë‹µì€ 5ë²ˆ" ë“± ëª…í™•í•œ íŒ¨í„´ì—ì„œ ìˆ«ì ì¶”ì¶œ
        match = re.search(r'(?:ì •ë‹µì€|ë‹µì€|ì„ íƒì€|ë‹µë³€ì€)\s*\D*(\d+)', answer)
        if match:
            return match.group(1)

        # 2ë‹¨ê³„: "5ë²ˆ", "5." ì™€ ê°™ì€ íŒ¨í„´ì—ì„œ ìˆ«ì ì¶”ì¶œ
        match = re.search(r'\b(\d+)\s*(?:ë²ˆ|ë²ˆì…ë‹ˆë‹¤|\.)', answer)
        if match:
            return match.group(1)

        # 3ë‹¨ê³„: ë¬¸ì¥ ë§¨ ì•ì— ìˆëŠ” ìˆ«ì ì¶”ì¶œ
        match = re.search(r"^\s*(\d+)", answer)
        if match:
            return match.group(1)

        # 4ë‹¨ê³„: ìœ„ ëª¨ë“  ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•Šì„ ê²½ìš°, í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ ì²˜ìŒ ë°œê²¬ë˜ëŠ” ìˆ«ì ì¶”ì¶œ
        match = re.search(r'(\d+)', answer)
        if match:
            return match.group(1)
            
        # 5ë‹¨ê³„: ê·¸ë˜ë„ ìˆ«ìë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ê¸°ë³¸ê°’ '1' ë°˜í™˜
        return "1"
    
    return answer if answer else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."


def is_code_detected(text: str) -> bool:
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ì½”ë“œê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    code_keywords = ['def ', 'import ', 'class ', 'r\'', 'sys.stdout', 'ans_qna']
    text_lower = text.lower()
    return any(keyword in text_lower for keyword in code_keywords)


# --- 5. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    try:
        test_df = pd.read_csv(TEST_CSV_PATH)
        print(f"âœ… '{TEST_CSV_PATH}'ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{TEST_CSV_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()

    preds = []
    MAX_RETRIES = 3 

    for index, q in tqdm(enumerate(test_df['Question']), total=len(test_df), desc="ğŸš€ ì¶”ë¡  ì§„í–‰ ì¤‘"):
        prompt = make_prompt(q)
        
        is_valid_answer = False
        retries = 0
        generated_text = ""

        while not is_valid_answer and retries < MAX_RETRIES:
            if retries > 0:
                print(f"\nğŸ”„ TEST_{index} ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì¬ì‹œë„ ì¤‘... ({retries}/{MAX_RETRIES})")

            output = pipe(
                prompt, 
                max_new_tokens=512,
                temperature=0.1 + (retries * 0.15),
                top_p=0.9,
                do_sample=True,
                return_full_text=False,
                eos_token_id=tokenizer.eos_token_id
            )
            
            generated_text = output[0]['generated_text']

            if is_code_detected(generated_text):
                retries += 1
                if retries == MAX_RETRIES:
                    print(f"âŒ TEST_{index} ì§ˆë¬¸ì— ëŒ€í•´ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë§ˆì§€ë§‰ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    is_valid_answer = True
            else:
                is_valid_answer = True

        pred_answer = post_process_answer(generated_text, original_question=q)
        preds.append(pred_answer)

    print("\nğŸ“„ ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        sample_submission = pd.read_csv('/workspace/open/sample_submission.csv')
        sample_submission['Answer'] = preds
        sample_submission.to_csv(SUBMISSION_CSV_PATH, index=False, encoding='utf-8-sig')
        print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: '{SUBMISSION_CSV_PATH}'")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '/workspace/open/sample_submission.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

