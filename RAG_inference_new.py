import os
import torch
import pandas as pd
import re
from tqdm import tqdm
import json

# --- 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ Import ---

# LangChain (RAG)
from langchain_community.document_loaders import JSONLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# Hugging Face (LLM)
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
# from peft import PeftModel # â­ï¸ PeftModelì€ ë” ì´ìƒ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì‚­ì œí•©ë‹ˆë‹¤.

# --- 2. ì„¤ì • (Configuration) ---

# ê¸°ë³¸ ëª¨ë¸ ID
BASE_MODEL_ID = "K-intelligence/Midm-2.0-Base-Instruct"

# â­ï¸ ì‚¬ìš©ì ì„¤ì •: í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ê²½ë¡œ (ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
# LORA_ADAPTER_PATH = "/workspace/checkpoint-708" 

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ì œì¶œ íŒŒì¼ ê²½ë¡œ
TEST_CSV_PATH = '/workspace/open/test.csv'
SUBMISSION_CSV_PATH = './submission_rag_langchain_v1.csv' 

# RAG ì§€ì‹ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©í•  JSONL íŒŒì¼ ê²½ë¡œ
RAG_JSONL_PATH = "/workspace/2025-AI-Challeng-finance/cybersecurity_data_regex_cleaned.jsonl"

EMBEDDING_MODEL_NAME = "BM-K/KoSimCSE-roberta-multitask"
FAISS_DB_PATH = "./faiss_index_langchain_jsonl" 


# --- 3. RAG ë°±ì—”ë“œ êµ¬ì¶• í•¨ìˆ˜ (LangChain ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •) ---
def build_or_load_rag_backend_langchain():
    """LangChainì„ ì‚¬ìš©í•˜ì—¬ FAISS ë²¡í„° DBë¥¼ êµ¬ì¶•í•˜ê±°ë‚˜ ê¸°ì¡´ DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    
    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    if os.path.exists(FAISS_DB_PATH):
        print(f"â³ ê¸°ì¡´ ë²¡í„° DBë¥¼ '{FAISS_DB_PATH}'ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...")
        vector_db = FAISS.load_local(
            FAISS_DB_PATH, 
            embedding_model, 
            allow_dangerous_deserialization=True
        )
        print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ.")
    else:
        print(f"â³ '{RAG_JSONL_PATH}' íŒŒì¼ë¡œ ìƒˆë¡œìš´ ë²¡í„° DBë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤...")
        
        if not os.path.exists(RAG_JSONL_PATH):
            print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: RAG ë°ì´í„° íŒŒì¼ '{RAG_JSONL_PATH}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        # JSONL íŒŒì¼ì„ LangChain Document ê°ì²´ë¡œ ë¡œë“œ
        loader = JSONLoader(
            file_path=RAG_JSONL_PATH,
            jq_schema='"ì§ˆë¬¸: " + .question + "\\në‹µë³€ : " + .answer', 
            json_lines=True,
            text_content=False
        )
        all_documents = loader.load()

        if not all_documents:
            print("âŒ ì˜¤ë¥˜: ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. JSONL íŒŒì¼ì˜ ë‚´ìš©ì´ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return None

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        split_documents = text_splitter.split_documents(all_documents)
        print(f"âœ… ë¬¸ì„œë¥¼ ì´ {len(split_documents)}ê°œì˜ ì‘ì€ ì¡°ê°(chunk)ìœ¼ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

        print(f"ì´ {len(split_documents)}ê°œì˜ ì¡°ê°(chunk)ì„ ì„ë² ë”©í•©ë‹ˆë‹¤...")
        vector_db = FAISS.from_documents(split_documents, embedding_model)
        
        vector_db.save_local(FAISS_DB_PATH)
        print(f"âœ… ìƒˆë¡œìš´ ë²¡í„° DB êµ¬ì¶• ë° ì €ì¥ ì™„ë£Œ: '{FAISS_DB_PATH}'")

    # LangChainì˜ Retriever ê°ì²´ ë°˜í™˜
    return vector_db.as_retriever(search_kwargs={'k': 3})


# --- 4. ìœ í‹¸ë¦¬í‹° ë° í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---

def is_multiple_choice(question_text: str) -> bool:
    lines = question_text.strip().split("\n")
    option_count = sum(bool(re.match(r"^\s*[1-9][0-9]?[\.\s]", line)) for line in lines)
    return option_count >= 2

def extract_question_and_choices(full_text: str) -> tuple[str, list[str]]:
    lines = full_text.strip().split("\n")
    q_lines, options = [], []
    for line in lines:
        if re.match(r"^\s*[1-9][0-9]?[\.\s]", line):
            options.append(line.strip())
        else:
            q_lines.append(line.strip())
    question = " ".join(q_lines)
    return question, options


def make_rag_prompt(text: str, context: str) -> str:
    """RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        prompt = f"""### ì§€ì‹œ:
"Please reason step by step, and you should must write the correct option number (1, 2, 3, 4 or 5).\n ì •ë‹µ ë²ˆí˜¸ë¥¼ ë°˜ë“œì‹œ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì€ í•„ìš” ì—†ìŠµë‹ˆë‹¤."
ì •ë‹µì„ ë„ì¶œí•  ë•Œ ì°¸ê³ ìë£Œì— ê´€ë ¨ ë‚´ìš©ì´ë‚˜ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ ë‹µì•ˆ ì„ íƒì— **ë°˜ë“œì‹œ í™œìš©**í•˜ì„¸ìš”

### ì°¸ê³  ë¬¸ì„œ:
{context}

### ì§ˆë¬¸:
{question}

### ì„ íƒì§€:
{chr(10).join(options)}

### ë‹µë³€:
"""
    else:
        prompt = f"""### ì§€ì‹œ:
ì£¼ì–´ì§„ **'ì°¸ê³  ë¬¸ì„œ'ì˜ ë‚´ìš©ë§Œì„ ê·¼ê±°**ë¡œ í•˜ì—¬ 'ì§ˆë¬¸'ì— ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬, ì „ë¬¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ 2~3ê°œì˜ ì™„ë²½í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.
**'ì°¸ê³  ë¬¸ì„œì— ë”°ë¥´ë©´'ê³¼ ê°™ì€ í‘œí˜„ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.** ë‹¹ì‹ ì˜ ë°°ê²½ ì§€ì‹ì´ë‚˜ ì™¸ë¶€ ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
Generate your thought process step by step, but don't print it out.

### ì°¸ê³  ë¬¸ì„œ:
{context}

### ì§ˆë¬¸:
{text}

### ë‹µë³€:
"""
    return prompt



# --- 5. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (â­ï¸ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì œê±°) ---
print("â³ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# â­ï¸ 'base_model' ëŒ€ì‹  'model' ë³€ìˆ˜ëª…ìœ¼ë¡œ ë°”ë¡œ ë¡œë“œí•˜ì—¬ íŒŒì´í”„ë¼ì¸ì— ì‚¬ìš©í•©ë‹ˆë‹¤.
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID, 
    quantization_config=quantization_config, 
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# â­ï¸ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•˜ê³  ë³‘í•©í•˜ëŠ” ë¶€ë¶„ì„ ì™„ì „íˆ ì œê±°í–ˆìŠµë‹ˆë‹¤.
# print(f"â³ '{LORA_ADAPTER_PATH}'ì—ì„œ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë”©í•˜ì—¬ ëª¨ë¸ì— ì ìš©í•©ë‹ˆë‹¤...")
# lora_model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
# print("â³ LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©í•©ë‹ˆë‹¤...")
# model = lora_model.merge_and_unload()

# â­ï¸ 'model' ë³€ìˆ˜ì— ì €ì¥ëœ ê¸°ë³¸ ëª¨ë¸ì„ íŒŒì´í”„ë¼ì¸ì— ë°”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device_map="auto")
print("âœ… ëª¨ë¸ ë¡œë”© ë° ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (ì‚¬ì „ í•™ìŠµëœ ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©)")


# --- 6. ë‹µë³€ í›„ì²˜ë¦¬ ë° ìœ í‹¸ë¦¬í‹° (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
def post_process_answer(generated_text: str, original_question: str) -> str:
    answer = generated_text.strip()
    if not answer: return "1"
    if "###" in answer: answer = answer.split("###")[0].strip()
        
    if is_multiple_choice(original_question):
        match = re.search(r'(?:ì •ë‹µì€|ë‹µì€|ì„ íƒì€)\s*\D*(\d+)', answer); 
        if match: return match.group(1)
        match = re.search(r'\b(\d+)\s*(?:ë²ˆ|ë²ˆì…ë‹ˆë‹¤|\.)', answer); 
        if match: return match.group(1)
        match = re.search(r"^\s*(\d+)", answer); 
        if match: return match.group(1)
        match = re.search(r'(\d+)', answer); 
        if match: return match.group(1)
        return "1"
    return answer if answer else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

def is_code_detected(text: str) -> bool:
    code_keywords = ['def ', 'import ', 'class ', 'r\'', 'sys.stdout', 'ans_qna']
    return any(keyword in text.lower() for keyword in code_keywords)


# --- 7. ë©”ì¸ ì‹¤í–‰ (RAG ì ìš©) ---
if __name__ == "__main__":
    retriever = build_or_load_rag_backend_langchain()
    
    if not retriever:
        print("âŒ RAG ë°±ì—”ë“œ ìƒì„±ì— ì‹¤íŒ¨í•˜ì—¬ ì¶”ë¡ ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        exit()

    try:
        test_df = pd.read_csv(TEST_CSV_PATH)
        print(f"âœ… '{TEST_CSV_PATH}'ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{TEST_CSV_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()

    preds = []
    MAX_RETRIES = 3 

    for index, q in tqdm(enumerate(test_df['Question']), total=len(test_df), desc="ğŸš€ ì¶”ë¡  ì§„í–‰ ì¤‘"):
        
        # LangChain Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = retriever.invoke(q)
        context_text = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
        
        prompt = make_rag_prompt(q, context_text)
        
        is_valid_answer = False
        retries = 0
        generated_text = ""

        while not is_valid_answer and retries < MAX_RETRIES:
            if retries > 0:
                print(f"\nğŸ”„ TEST_{index} ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì¬ì‹œë„ ì¤‘... ({retries}/{MAX_RETRIES})")

            output = pipe(
                prompt, 
                max_new_tokens=512,
                temperature=0.1 + (retries * 0.15),
                top_p=0.9,
                do_sample=True,
                return_full_text=False,
                eos_token_id=tokenizer.eos_token_id
            )
            
            generated_text = output[0]['generated_text']

            if is_code_detected(generated_text):
                retries += 1
                if retries == MAX_RETRIES:
                    print(f"âŒ TEST_{index} ì§ˆë¬¸ì— ëŒ€í•´ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë§ˆì§€ë§‰ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    is_valid_answer = True
            else:
                is_valid_answer = True

        pred_answer = post_process_answer(generated_text, original_question=q)
        preds.append(pred_answer)

    print("\n ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        sample_submission = pd.read_csv('/workspace/open/sample_submission.csv')
        sample_submission['Answer'] = preds
        sample_submission.to_csv(SUBMISSION_CSV_PATH, index=False, encoding='utf-8-sig')
        print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: '{SUBMISSION_CSV_PATH}'")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '/workspace/open/sample_submission.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
