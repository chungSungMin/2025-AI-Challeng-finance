{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d463c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re \n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c131f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path : str) : \n",
    "    try : \n",
    "        with fitz.open(pdf_path) as doc : \n",
    "            full_text = \"\".join(page.get_text() for page in doc)\n",
    "        return full_text\n",
    "    except Exception as e : \n",
    "        print(e)\n",
    "        return \"\"\n",
    "    \n",
    "pdf_file_path = \"/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf\"\n",
    "# extracted_text = extract_text_from_pdf(pdf_file_path)\n",
    "# print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_korean_law_by_article(text, source) : \n",
    "    # 예: \"제15조(개인정보의 수집ㆍ이용)\", \"제28조의2(가명정보의 처리 등)\"\n",
    "    pattern = r\"(제\\d+조(?:의\\d+)?\\s*\\(.+?\\))\" # /를 )로 수정하여 괄호 짝을 맞춤\n",
    "\n",
    "    split_text = re.split(pattern, text)\n",
    "\n",
    "    documents = []\n",
    "    for i in range(1, len(split_text), 2) : \n",
    "        article_title = split_text[i]\n",
    "        article_content = split_text[i+1].strip()\n",
    "\n",
    "        full_content = f\"{article_title}\\n{article_content}\"\n",
    "\n",
    "        doc = Document(\n",
    "            page_content=full_content,\n",
    "            metadata={\n",
    "                \"source\":source,\n",
    "                \"article\":article_title.split('(')[0].strip()\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ad5435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 총 5개의 '조(Article)'로 문서를 분할했습니다.\n",
      "\n",
      "--- [첫 번째 분할 결과 샘플] ---\n",
      "page_content='제1조(목적)\n",
      "이 영은 경찰공무원 및 경찰청ㆍ해양경찰청 소속 직원 등이 그 직무를 수행하기 위하여 필요한 경우 「개인\n",
      "정보 보호법」 제23조, 제24조 및 제24조의2에 따른 개인정보를 처리할 수 있는 근거를 규정함을 목적으로 한다.' metadata={'source': '/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf', 'article': '제1조'}\n",
      "\n",
      "--- [중간 분할 결과 샘플 (2번째)] ---\n",
      "page_content='제2조(경찰공무원 등의 민감정보 등의 처리)\n",
      "경찰공무원 및 경찰청ㆍ해양경찰청 소속 직원은 다음 각 호의 업무를 수행\n",
      "하기 위하여 불가피한 경우 「개인정보 보호법」 제23조에 따른 민감정보, 같은 법 제24조에 따른 고유식별정보 및 같\n",
      "은 법 제24조의2에 따른 주민등록번호가 포함된 자료를 처리할 수 있다.\n",
      "1. 「경찰관 직무집행법」 제2조에 따른 경찰관의 직무\n",
      "2. 「형사소송법」에 따른 범인ㆍ범죄사실 및 그 증거에 대한 수사\n",
      "3. 「디엔에이신원확인정보의 이용 및 보호에 관한 법률」에 따른 디엔에이감식시료의 채취, 디엔에이감식시료채취\n",
      "영장의 신청, 디엔에이신원확인정보의 수록ㆍ관리, 검색ㆍ회보, 삭제, 디엔에이감식시료의 폐기 등 디엔에이신원\n",
      "확인정보의 수집ㆍ이용 및 보호에 관한 업무\n",
      "4. 주민등록 관계 법령에 따른 주민등록증발급신청서의 관리, 검색ㆍ대조 등에 관한 업무\n",
      "5. 경찰공무원 및 경찰청ㆍ해양경찰청 소속 직원의 인사기록 작성ㆍ유지ㆍ변경ㆍ보관, 임용에 필요한 자격 및 요건\n",
      "의 확인, 그 밖에 시험과 임용 등 인사사무의 처리를 위한 업무\n",
      "6. 경찰청 또는 해양경찰청 내부 정보시스템 등 정보통신망 운영ㆍ관리에 관한 업무\n",
      "7. 제1호부터 제6호까지의 업무를 수행하기 위하여 부수적으로 필요한 업무' metadata={'source': '/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf', 'article': '제2조'}\n"
     ]
    }
   ],
   "source": [
    "extracted_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "if extracted_text:\n",
    "    # 2. 텍스트를 '조' 단위로 분할하여 Document 객체 리스트 생성 (이번 단계)\n",
    "    law_documents = chunk_korean_law_by_article(extracted_text, pdf_file_path)\n",
    "    \n",
    "    print(f\"\\n✅ 총 {len(law_documents)}개의 '조(Article)'로 문서를 분할했습니다.\")\n",
    "    \n",
    "    if law_documents:\n",
    "        print(\"\\n--- [첫 번째 분할 결과 샘플] ---\")\n",
    "        print(law_documents[0])\n",
    "        \n",
    "        print(\"\\n--- [중간 분할 결과 샘플 (2번째)] ---\")\n",
    "        print(law_documents[1]) # 제15조는 보통 15번째에 위치\n",
    "else:\n",
    "    print(\"\\n❌ 텍스트가 추출되지 않아 분할을 진행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl과 pdf를 각각 추출하고 이를 기반으로 하나의 거대 RAG를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f299c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PDF 파일 처리 시작 ---\n",
      "'/workspace/개인금융채권의 관리 및 개인금융채무자의 보호에 관한 법률(법률)(제20369호)(20241017).pdf' 파일 처리 중...\n",
      "'/workspace/개인금융채권의 관리 및 개인금융채무자의 보호에 관한 법률(법률)(제20369호)(20241017).pdf' 처리 중 오류: no such file: '/workspace/개인금융채권의 관리 및 개인금융채무자의 보호에 관한 법률(법률)(제20369호)(20241017).pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/개인정보 보호법(법률)(제19234호)(20250313) (1).pdf' 파일 처리 중...\n",
      "'/workspace/개인정보 보호법(법률)(제19234호)(20250313) (1).pdf' 처리 중 오류: no such file: '/workspace/개인정보 보호법(법률)(제19234호)(20250313) (1).pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/거버넌스.pdf' 파일 처리 중...\n",
      "'/workspace/거버넌스.pdf' 처리 중 오류: no such file: '/workspace/거버넌스.pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf' 파일 처리 중...\n",
      "'/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf' 처리 중 오류: no such file: '/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/금융보안연구원.pdf' 파일 처리 중...\n",
      "'/workspace/금융보안연구원.pdf' 처리 중 오류: no such file: '/workspace/금융보안연구원.pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/금융소비자 보호에 관한 법률(법률)(제20305호)(20240814).pdf' 파일 처리 중...\n",
      "'/workspace/금융소비자 보호에 관한 법률(법률)(제20305호)(20240814).pdf' 처리 중 오류: no such file: '/workspace/금융소비자 보호에 관한 법률(법률)(제20305호)(20240814).pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/금융실명거래 및 비밀보장에 관한 법률 시행규칙(총리령)(제01406호)(20170726).pdf' 파일 처리 중...\n",
      "'/workspace/금융실명거래 및 비밀보장에 관한 법률 시행규칙(총리령)(제01406호)(20170726).pdf' 처리 중 오류: no such file: '/workspace/금융실명거래 및 비밀보장에 관한 법률 시행규칙(총리령)(제01406호)(20170726).pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/랜섬웨어.pdf' 파일 처리 중...\n",
      "'/workspace/랜섬웨어.pdf' 처리 중 오류: no such file: '/workspace/랜섬웨어.pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/마이데이터.pdf' 파일 처리 중...\n",
      "'/workspace/마이데이터.pdf' 처리 중 오류: no such file: '/workspace/마이데이터.pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/메타버스.pdf' 파일 처리 중...\n",
      "'/workspace/메타버스.pdf' 처리 중 오류: no such file: '/workspace/메타버스.pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/법원 개인정보 보호에 관한 규칙(대법원규칙)(제03109호)(20240315).pdf' 파일 처리 중...\n",
      "'/workspace/법원 개인정보 보호에 관한 규칙(대법원규칙)(제03109호)(20240315).pdf' 처리 중 오류: no such file: '/workspace/법원 개인정보 보호에 관한 규칙(대법원규칙)(제03109호)(20240315).pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/아웃소싱.pdf' 파일 처리 중...\n",
      "'/workspace/아웃소싱.pdf' 처리 중 오류: no such file: '/workspace/아웃소싱.pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/정보_보안.pdf' 파일 처리 중...\n",
      "'/workspace/정보_보안.pdf' 처리 중 오류: no such file: '/workspace/정보_보안.pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "'/workspace/클라우드컴퓨팅 발전 및 이용자 보호에 관한 법률(법률)(제20732호)(20250131).pdf' 파일 처리 중...\n",
      "'/workspace/클라우드컴퓨팅 발전 및 이용자 보호에 관한 법률(법률)(제20732호)(20250131).pdf' 처리 중 오류: no such file: '/workspace/클라우드컴퓨팅 발전 및 이용자 보호에 관한 법률(법률)(제20732호)(20250131).pdf'\n",
      "-> 텍스트가 없어 건너뜁니다.\n",
      "\n",
      "--- JSONL 파일 처리 시작 ---\n",
      "'/workspace/2025-AI-Challeng-finance/cybersecurity_data_regex_cleaned_longText.jsonl' 파일 처리 중...\n",
      "-> 53311개의 'Q&A' 청크를 추가했습니다.\n",
      "'/workspace/객관식_한국어.jsonl' 파일 처리 중...\n",
      "-> 12761개의 'Q&A' 청크를 추가했습니다.\n",
      "\n",
      "--- ✅ 최종 통합 완료 ---\n",
      "모든 파일로부터 총 66072개의 Document를 생성했습니다.\n",
      "\n",
      "--- [통합된 데이터 샘플 확인] ---\n",
      "\n",
      "📝 JSONL에서 온 데이터 샘플:\n",
      "page_content='질문: TLS를 사용하여 암호화된 C2 채널을 분석하고 악성 신호를 감지하는 트래픽 분석 기술을 논의합니다.\n",
      "답변 : 트랜스포트 레이어 보안 (TLS) 를 활용한 암호화 명령 및 제어 (C2) 채널은 네트워크 방어자 (TNS) 패턴, 인증서 제품군 사슬 길이가 및 암호 선호도를 활용하는 데 큰 도전을 야기합니다. 전통적인 패킷 검사 방법은 유용 한 부하 내용을 직접 분석할 수 없기 때문에. 그러나, 정교한 트래픽 분석 기술은 여전히 행동 및 통계 패턴을 통해 잠재적으로 악성적인 세션들을 식별 할 수 있습니다.\\n\\n** 트래픽 분석 기술은:\\n\\n\\nStatistical flow analysis (Statistical flow analysis) 는 서버 이름 표시 (SNI) 패턴, 인증서 제품군 사슬 길이가, 암호 선호도를 이용하는 데 중요한 도전을 야기합니다. 악성적인 C2 채널은 종종 특이한 SNI 엔트로피, 비 표준 포트, 또는 합법적인 응용 패턴에서 벗어나는 일관적인 패턴을 표시합니다. NLS의 적성성성성 검증기 체계의 검증기기술은 이러한 사용자 행동의 연속적인 지표를 확인하는 것을 강조합니다. TLS의 사용 및 외부 데이터베이스 분석의 특징을 확인하는 TLS의 특성을 확인하는 것을 허용합니다.' metadata={'source': '/workspace/2025-AI-Challeng-finance/cybersecurity_data_regex_cleaned_longText.jsonl', 'seq_num': 1, 'data_type': 'qna_jsonl'}\n",
      "\n",
      "--- 💾 원본 문서 저장 시작 ---\n",
      "총 66072개의 Document 객체를 'original_docs.pkl' 파일로 저장합니다...\n",
      "--- ✅ 원본 문서 저장 완료 ---\n",
      "'original_docs.pkl' 파일이 성공적으로 생성되었습니다.\n",
      "이제 이 파일을 추론 코드에서 불러와 BM25 Retriever를 초기화할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "import pickle # ★★★ 1. pickle 라이브러리 임포트 ★★★\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- 1. 처리할 파일 목록 정의 ---\n",
    "pdf_file_paths = [\n",
    "    \"/workspace/개인금융채권의 관리 및 개인금융채무자의 보호에 관한 법률(법률)(제20369호)(20241017).pdf\",\n",
    "    \"/workspace/개인정보 보호법(법률)(제19234호)(20250313) (1).pdf\",\n",
    "    \"/workspace/거버넌스.pdf\",\n",
    "    \"/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf\",\n",
    "    \"/workspace/금융보안연구원.pdf\",\n",
    "    \"/workspace/금융소비자 보호에 관한 법률(법률)(제20305호)(20240814).pdf\",\n",
    "    \"/workspace/금융실명거래 및 비밀보장에 관한 법률 시행규칙(총리령)(제01406호)(20170726).pdf\",\n",
    "    \"/workspace/랜섬웨어.pdf\",\n",
    "    \"/workspace/마이데이터.pdf\",\n",
    "    \"/workspace/메타버스.pdf\",\n",
    "    \"/workspace/법원 개인정보 보호에 관한 규칙(대법원규칙)(제03109호)(20240315).pdf\",\n",
    "    \"/workspace/아웃소싱.pdf\",\n",
    "    \"/workspace/정보_보안.pdf\",\n",
    "    \"/workspace/클라우드컴퓨팅 발전 및 이용자 보호에 관한 법률(법률)(제20732호)(20250131).pdf\",\n",
    "]\n",
    "jsonl_file_paths = [\n",
    "    \"/workspace/2025-AI-Challeng-finance/cybersecurity_data_regex_cleaned_longText.jsonl\",\n",
    "    \"/workspace/객관식_한국어.jsonl\"\n",
    "]\n",
    "\n",
    "# --- 2. 각 파일 유형별 처리 함수 정의 ---\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"PDF 파일에서 텍스트를 추출합니다.\"\"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            return \"\".join(page.get_text() for page in doc)\n",
    "    except Exception as e:\n",
    "        print(f\"'{pdf_path}' 처리 중 오류: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_korean_law_by_article(text: str, source: str) -> list[Document]:\n",
    "    \"\"\"법률 텍스트를 '조' 단위로 분할하여 Document 객체 리스트를 생성합니다.\"\"\"\n",
    "    pattern = r\"(제\\d+조(?:의\\d+)?\\s*\\(.+?\\))\"\n",
    "    split_text = re.split(pattern, text)\n",
    "    documents = []\n",
    "    for i in range(1, len(split_text), 2):\n",
    "        article_title = split_text[i]\n",
    "        article_content = split_text[i+1].strip()\n",
    "        if not article_content: continue\n",
    "        doc = Document(\n",
    "            page_content=f\"{article_title}\\n{article_content}\",\n",
    "            metadata={\"source\": source, \"article\": article_title.split('(')[0].strip(), \"data_type\": \"legal_pdf\"}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "def chunk_general_pdf(text: str, source: str) -> list[Document]:\n",
    "    \"\"\"일반 텍스트를 의미 기반의 청크로 분할하여 Document 객체 리스트를 생성합니다.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"source\": source, \"chunk_num\": i + 1, \"data_type\": \"general_pdf\"}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# --- 3. 메인 실행: 모든 파일 처리 및 통합 ---\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "print(\"--- PDF 파일 처리 시작 ---\")\n",
    "for pdf_path in pdf_file_paths:\n",
    "    print(f\"'{pdf_path}' 파일 처리 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    if not extracted_text:\n",
    "        print(f\"-> 텍스트가 없어 건너뜁니다.\")\n",
    "        continue\n",
    "    if any(keyword in pdf_path for keyword in [\"법\", \"령\", \"규칙\"]):\n",
    "        documents = chunk_korean_law_by_article(extracted_text, pdf_path)\n",
    "        all_documents.extend(documents)\n",
    "        print(f\"-> [법률] {len(documents)}개의 '조(Article)'를 추가했습니다.\")\n",
    "    else:\n",
    "        documents = chunk_general_pdf(extracted_text, pdf_path)\n",
    "        all_documents.extend(documents)\n",
    "        print(f\"-> [일반] {len(documents)}개의 '청크'를 추가했습니다.\")\n",
    "\n",
    "print(\"\\n--- JSONL 파일 처리 시작 ---\")\n",
    "text_splitter_jsonl = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "for jsonl_path in jsonl_file_paths:\n",
    "    print(f\"'{jsonl_path}' 파일 처리 중...\")\n",
    "    try:\n",
    "        loader = JSONLoader(\n",
    "            file_path=jsonl_path,\n",
    "            jq_schema='\"질문: \" + .question + \"\\\\n답변 : \" + .answer',\n",
    "            json_lines=True\n",
    "        )\n",
    "        documents_from_jsonl = loader.load()\n",
    "        split_docs = text_splitter_jsonl.split_documents(documents_from_jsonl)\n",
    "        for doc in split_docs:\n",
    "            doc.metadata[\"data_type\"] = \"qna_jsonl\"\n",
    "        all_documents.extend(split_docs)\n",
    "        print(f\"-> {len(split_docs)}개의 'Q&A' 청크를 추가했습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"'{jsonl_path}' 처리 중 오류: {e}\")\n",
    "\n",
    "# --- 최종 결과 확인 ---\n",
    "print(\"\\n--- ✅ 최종 통합 완료 ---\")\n",
    "print(f\"모든 파일로부터 총 {len(all_documents)}개의 Document를 생성했습니다.\")\n",
    "\n",
    "if all_documents:\n",
    "    print(\"\\n--- [통합된 데이터 샘플 확인] ---\")\n",
    "    legal_pdf_sample = next((doc for doc in all_documents if doc.metadata.get(\"data_type\") == \"legal_pdf\"), None)\n",
    "    if legal_pdf_sample:\n",
    "        print(\"📄 법률 PDF에서 온 데이터 샘플:\")\n",
    "        print(legal_pdf_sample)\n",
    "    \n",
    "    general_pdf_sample = next((doc for doc in all_documents if doc.metadata.get(\"data_type\") == \"general_pdf\"), None)\n",
    "    if general_pdf_sample:\n",
    "        print(\"\\n📑 일반 PDF에서 온 데이터 샘플:\")\n",
    "        print(general_pdf_sample)\n",
    "    \n",
    "    jsonl_sample = next((doc for doc in all_documents if doc.metadata.get(\"data_type\") == \"qna_jsonl\"), None)\n",
    "    if jsonl_sample:\n",
    "        print(\"\\n📝 JSONL에서 온 데이터 샘플:\")\n",
    "        print(jsonl_sample)\n",
    "\n",
    "# ##################################################################\n",
    "# ############# ✨ all_documents 리스트를 .pkl 파일로 저장 ✨ #############\n",
    "# ##################################################################\n",
    "if all_documents:\n",
    "    output_path = \"original_docs.pkl\"\n",
    "    print(f\"\\n--- 💾 원본 문서 저장 시작 ---\")\n",
    "    print(f\"총 {len(all_documents)}개의 Document 객체를 '{output_path}' 파일로 저장합니다...\")\n",
    "    \n",
    "    try:\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            pickle.dump(all_documents, f)\n",
    "        print(f\"--- ✅ 원본 문서 저장 완료 ---\")\n",
    "        print(f\"'{output_path}' 파일이 성공적으로 생성되었습니다.\")\n",
    "        print(\"이제 이 파일을 추론 코드에서 불러와 BM25 Retriever를 초기화할 수 있습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"--- ❌ 원본 문서 저장 중 오류 발생 ---\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d6e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## huggingface 객관식 데이터를 question,answer형태로변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d92f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate english rag\n",
    "\n",
    "# import json \n",
    "\n",
    "# file_path = \"/workspace/merged_questions.json\"\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     dataset = json.load(f)\n",
    "\n",
    "# print(len(dataset[\"questions\"]))\n",
    "\n",
    "\n",
    "# document_len = len(dataset[\"questions\"])\n",
    "# real_data = dataset[\"questions\"]\n",
    "\n",
    "# all_docs = []\n",
    "\n",
    "# for i in range(document_len):\n",
    "\n",
    "#     question = real_data[i][\"question\"]\n",
    "#     # .get() is used to safely access the answer, avoiding errors if the key is missing\n",
    "#     answer = real_data[i][\"answers\"].get(real_data[i][\"solution\"])\n",
    "\n",
    "#     docs = {\n",
    "#         \"question\" : question,\n",
    "#         \"answer\" : answer\n",
    "#     }\n",
    "\n",
    "#     all_docs.append(docs)\n",
    "\n",
    "# print(all_docs[10])\n",
    "\n",
    "# # --- ✨ 여기에 JSONL 저장 코드를 추가합니다 ✨ ---\n",
    "\n",
    "# # 저장할 파일 이름 정의\n",
    "# output_filename = \"qa_data.jsonl\"\n",
    "\n",
    "# # all_docs 리스트를 jsonl 파일로 저장\n",
    "# with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "#     for doc in all_docs:\n",
    "#         # 각 딕셔너리를 JSON 문자열로 변환하여 파일에 한 줄씩 씁니다.\n",
    "#         # ensure_ascii=False는 비-ASCII 문자(예: 한글)가 깨지지 않도록 합니다.\n",
    "#         f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# print(f\"\\n✅ 데이터가 '{output_filename}' 파일에 성공적으로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6402c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [RAG 구성 시작] ---\n",
      "'BAAI/bge-m3' 임베딩 모델을 로드합니다...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868eded02e714395a2a4fac04e6556e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2e7eb4286844559f005fc9ce97f918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f793d9e01284c94b1c77bd3b9590359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883967c03f344c5e856cac1b8984cf74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a32e0e199984dd3aa7175d7eeebfb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd3ee78c9b2461ba71beae22a98d6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d94e090281241589616909abf029838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900b58ce1cb24b0c9c622a96b1c146f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d61846653034e84ac0218adf80048d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1929eb340b9649648ca561ebfa162082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 임베딩 모델 로드 완료.\n",
      "\n",
      "총 66315개의 Document를 벡터로 변환하여 FAISS DB를 구축합니다...\n",
      "✅ FAISS 벡터 DB 구축 완료.\n",
      "✅ Retriever 생성이 완료되었습니다.\n",
      "\n",
      "--- [RAG 검색 테스트] ---\n",
      "❓ 테스트 질문: \"랜섬웨어 공격을 방지하는 방법은 무엇인가요?\"\n",
      "\n",
      "🔍 검색된 관련 문서 (3개):\n",
      "\n",
      "--- [문서 1] ---\n",
      "질문: 특정 윈도우 데이터 구조를 수정하고 프로세스, 스레드 및 파일 시스템 활동을 숨기기 위한 방법들을 포함하여 직접 커널 객체 조작 (DKOM) 를 통해 랜섬웨어가 EDR 솔루션을 우회하는 데 사용하는 고급 피난 기술을 설명하십시오.\n",
      "답변 : 가장 중요한 것은 당신...\n",
      "(출처: /workspace/2025-AI-Challeng-finance/cybersecurity_data_regex_cleaned_longText.jsonl)\n",
      "\n",
      "--- [문서 2] ---\n",
      "질문: 어떻게 랜섬웨어가 백업 소프트웨어 바이너리를 공격하는 것을 막는 걸까요?\n",
      "답변 : 랜섬웨어 랜섬웨어가 백업 소프트웨어 바이너리를 타겟으로 하는 것을 막는 것은 보안 시스템 설계, 엄격한 액세스 제어, 지속적인 모니터링 및 코드 무결성의 정기적인 검증을 포함하는 ...\n",
      "(출처: /workspace/2025-AI-Challeng-finance/cybersecurity_data_regex_cleaned_longText.jsonl)\n",
      "\n",
      "--- [문서 3] ---\n",
      "질문: 랜섬웨어 확산으로부터 보호하기 위해 접근 거부를 어떻게 구성할 것인가?\n",
      "답변 : 랜섬웨어 배포로부터 보호하기 위해 랜섬웨어 접근 거부 구조를 구축하는 것은 초기 액세스 벡터와 측면 이동 기능을 제한하는 랜섬웨어 보호 깊이 전략을 구현하는 것을 요구합니다. 이 접...\n",
      "(출처: /workspace/2025-AI-Challeng-finance/cybersecurity_data_regex_cleaned_longText.jsonl)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "\n",
    "# --- 4. RAG 구성: 임베딩 및 벡터 스토어 생성 ---\n",
    "\n",
    "# 사용할 임베딩 모델 ID\n",
    "embedding_model_id = \"BAAI/bge-m3\"\n",
    "\n",
    "print(f\"\\n--- [RAG 구성 시작] ---\")\n",
    "print(f\"'{embedding_model_id}' 임베딩 모델을 로드합니다...\")\n",
    "\n",
    "# HuggingFaceEmbeddings 객체 생성\n",
    "# model_kwargs: GPU 사용 설정 (사용 가능할 경우)\n",
    "# encode_kwargs: 임베딩 정규화 설정 (성능 향상에 도움)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "print(\"✅ 임베딩 모델 로드 완료.\")\n",
    "\n",
    "print(f\"\\n총 {len(all_documents)}개의 Document를 벡터로 변환하여 FAISS DB를 구축합니다...\")\n",
    "\n",
    "# FAISS.from_documents를 사용하여 벡터 DB 생성\n",
    "# 이 과정에서 tqdm을 직접 사용하기는 어렵지만, 내부적으로 모든 문서를 처리합니다.\n",
    "vectorstore = FAISS.from_documents(all_documents, embeddings)\n",
    "\n",
    "print(\"✅ FAISS 벡터 DB 구축 완료.\")\n",
    "db_save_path = \"./faiss_db_kor_eng\"\n",
    "vectorstore.save_local(db_save_path)\n",
    "# 검색기(Retriever) 생성\n",
    "# search_kwargs={'k': 3}: 검색 시 가장 유사한 3개의 문서를 가져오도록 설정\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "print(\"✅ Retriever 생성이 완료되었습니다.\")\n",
    "\n",
    "\n",
    "# --- 5. RAG 검색 테스트 (선택 사항) ---\n",
    "\n",
    "if retriever:\n",
    "    print(\"\\n--- [RAG 검색 테스트] ---\")\n",
    "    test_query = \"랜섬웨어 공격을 방지하는 방법은 무엇인가요?\"\n",
    "    \n",
    "    # retriever.invoke()를 사용하여 테스트 쿼리와 관련된 문서 검색\n",
    "    retrieved_docs = retriever.invoke(test_query)\n",
    "    \n",
    "    print(f\"❓ 테스트 질문: \\\"{test_query}\\\"\")\n",
    "    print(f\"\\n🔍 검색된 관련 문서 ({len(retrieved_docs)}개):\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"\\n--- [문서 {i+1}] ---\")\n",
    "        # page_content의 앞부분 150자만 출력\n",
    "        print(doc.page_content[:150] + \"...\")\n",
    "        print(f\"(출처: {doc.metadata.get('source', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58db092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a174c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
