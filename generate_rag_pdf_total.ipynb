{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d463c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re \n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c131f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path : str) : \n",
    "    try : \n",
    "        with fitz.open(pdf_path) as doc : \n",
    "            full_text = \"\".join(page.get_text() for page in doc)\n",
    "        return full_text\n",
    "    except Exception as e : \n",
    "        print(e)\n",
    "        return \"\"\n",
    "    \n",
    "pdf_file_path = \"/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf\"\n",
    "# extracted_text = extract_text_from_pdf(pdf_file_path)\n",
    "# print(extracted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_korean_law_by_article(text, source) : \n",
    "    # 예: \"제15조(개인정보의 수집ㆍ이용)\", \"제28조의2(가명정보의 처리 등)\"\n",
    "    pattern = r\"(제\\d+조(?:의\\d+)?\\s*\\(.+?\\))\" # /를 )로 수정하여 괄호 짝을 맞춤\n",
    "\n",
    "    split_text = re.split(pattern, text)\n",
    "\n",
    "    documents = []\n",
    "    for i in range(1, len(split_text), 2) : \n",
    "        article_title = split_text[i]\n",
    "        article_content = split_text[i+1].strip()\n",
    "\n",
    "        full_content = f\"{article_title}\\n{article_content}\"\n",
    "\n",
    "        doc = Document(\n",
    "            page_content=full_content,\n",
    "            metadata={\n",
    "                \"source\":source,\n",
    "                \"article\":article_title.split('(')[0].strip()\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ad5435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 총 5개의 '조(Article)'로 문서를 분할했습니다.\n",
      "\n",
      "--- [첫 번째 분할 결과 샘플] ---\n",
      "page_content='제1조(목적)\n",
      "이 영은 경찰공무원 및 경찰청ㆍ해양경찰청 소속 직원 등이 그 직무를 수행하기 위하여 필요한 경우 「개인\n",
      "정보 보호법」 제23조, 제24조 및 제24조의2에 따른 개인정보를 처리할 수 있는 근거를 규정함을 목적으로 한다.' metadata={'source': '/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf', 'article': '제1조'}\n",
      "\n",
      "--- [중간 분할 결과 샘플 (2번째)] ---\n",
      "page_content='제2조(경찰공무원 등의 민감정보 등의 처리)\n",
      "경찰공무원 및 경찰청ㆍ해양경찰청 소속 직원은 다음 각 호의 업무를 수행\n",
      "하기 위하여 불가피한 경우 「개인정보 보호법」 제23조에 따른 민감정보, 같은 법 제24조에 따른 고유식별정보 및 같\n",
      "은 법 제24조의2에 따른 주민등록번호가 포함된 자료를 처리할 수 있다.\n",
      "1. 「경찰관 직무집행법」 제2조에 따른 경찰관의 직무\n",
      "2. 「형사소송법」에 따른 범인ㆍ범죄사실 및 그 증거에 대한 수사\n",
      "3. 「디엔에이신원확인정보의 이용 및 보호에 관한 법률」에 따른 디엔에이감식시료의 채취, 디엔에이감식시료채취\n",
      "영장의 신청, 디엔에이신원확인정보의 수록ㆍ관리, 검색ㆍ회보, 삭제, 디엔에이감식시료의 폐기 등 디엔에이신원\n",
      "확인정보의 수집ㆍ이용 및 보호에 관한 업무\n",
      "4. 주민등록 관계 법령에 따른 주민등록증발급신청서의 관리, 검색ㆍ대조 등에 관한 업무\n",
      "5. 경찰공무원 및 경찰청ㆍ해양경찰청 소속 직원의 인사기록 작성ㆍ유지ㆍ변경ㆍ보관, 임용에 필요한 자격 및 요건\n",
      "의 확인, 그 밖에 시험과 임용 등 인사사무의 처리를 위한 업무\n",
      "6. 경찰청 또는 해양경찰청 내부 정보시스템 등 정보통신망 운영ㆍ관리에 관한 업무\n",
      "7. 제1호부터 제6호까지의 업무를 수행하기 위하여 부수적으로 필요한 업무' metadata={'source': '/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf', 'article': '제2조'}\n"
     ]
    }
   ],
   "source": [
    "extracted_text = extract_text_from_pdf(pdf_file_path)\n",
    "\n",
    "if extracted_text:\n",
    "    # 2. 텍스트를 '조' 단위로 분할하여 Document 객체 리스트 생성 (이번 단계)\n",
    "    law_documents = chunk_korean_law_by_article(extracted_text, pdf_file_path)\n",
    "    \n",
    "    print(f\"\\n✅ 총 {len(law_documents)}개의 '조(Article)'로 문서를 분할했습니다.\")\n",
    "    \n",
    "    if law_documents:\n",
    "        print(\"\\n--- [첫 번째 분할 결과 샘플] ---\")\n",
    "        print(law_documents[0])\n",
    "        \n",
    "        print(\"\\n--- [중간 분할 결과 샘플 (2번째)] ---\")\n",
    "        print(law_documents[1]) # 제15조는 보통 15번째에 위치\n",
    "else:\n",
    "    print(\"\\n❌ 텍스트가 추출되지 않아 분할을 진행할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonl과 pdf를 각각 추출하고 이를 기반으로 하나의 거대 RAG를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6f299c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PDF 파일 처리 시작 ---\n",
      "'/workspace/개인금융채권의 관리 및 개인금융채무자의 보호에 관한 법률(법률)(제20369호)(20241017).pdf' 파일 처리 중...\n",
      "-> [일반] 30개의 '청크'를 추가했습니다.\n",
      "'/workspace/개인정보 보호법(법률)(제19234호)(20250313) (1).pdf' 파일 처리 중...\n",
      "-> [일반] 96개의 '청크'를 추가했습니다.\n",
      "'/workspace/거버넌스.pdf' 파일 처리 중...\n",
      "-> [일반] 3개의 '청크'를 추가했습니다.\n",
      "'/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf' 파일 처리 중...\n",
      "-> [일반] 2개의 '청크'를 추가했습니다.\n",
      "'/workspace/금융보안연구원.pdf' 파일 처리 중...\n",
      "-> [일반] 2개의 '청크'를 추가했습니다.\n",
      "'/workspace/금융소비자 보호에 관한 법률(법률)(제20305호)(20240814).pdf' 파일 처리 중...\n",
      "-> [일반] 53개의 '청크'를 추가했습니다.\n",
      "'/workspace/금융실명거래 및 비밀보장에 관한 법률 시행규칙(총리령)(제01406호)(20170726).pdf' 파일 처리 중...\n",
      "-> [일반] 1개의 '청크'를 추가했습니다.\n",
      "'/workspace/랜섬웨어.pdf' 파일 처리 중...\n",
      "-> [일반] 4개의 '청크'를 추가했습니다.\n",
      "'/workspace/마이데이터.pdf' 파일 처리 중...\n",
      "-> [일반] 3개의 '청크'를 추가했습니다.\n",
      "'/workspace/메타버스.pdf' 파일 처리 중...\n",
      "-> [일반] 12개의 '청크'를 추가했습니다.\n",
      "'/workspace/법원 개인정보 보호에 관한 규칙(대법원규칙)(제03109호)(20240315).pdf' 파일 처리 중...\n",
      "-> [일반] 7개의 '청크'를 추가했습니다.\n",
      "'/workspace/아웃소싱.pdf' 파일 처리 중...\n",
      "-> [일반] 6개의 '청크'를 추가했습니다.\n",
      "'/workspace/정보_보안.pdf' 파일 처리 중...\n",
      "-> [일반] 6개의 '청크'를 추가했습니다.\n",
      "'/workspace/클라우드컴퓨팅 발전 및 이용자 보호에 관한 법률(법률)(제20732호)(20250131).pdf' 파일 처리 중...\n",
      "-> [일반] 18개의 '청크'를 추가했습니다.\n",
      "\n",
      "--- JSONL 파일 처리 시작 ---\n",
      "'/workspace/2025-AI-Challeng-finance/cybersecurity_data_regex_cleaned.jsonl' 파일 처리 중...\n",
      "-> 2826개의 'Q&A' 청크를 추가했습니다.\n",
      "'/workspace/2025-AI-Challeng-finance/qa_data.jsonl' 파일 처리 중...\n",
      "-> 12760개의 'Q&A' 청크를 추가했습니다.\n",
      "\n",
      "--- ✅ 최종 통합 완료 ---\n",
      "모든 파일로부터 총 15829개의 Document를 생성했습니다.\n",
      "\n",
      "--- [통합된 데이터 샘플 확인] ---\n",
      "\n",
      "📑 일반 PDF에서 온 데이터 샘플:\n",
      "page_content='법제처                                                            1                                                       국가법령정보센터\n",
      "개인금융채권의 관리 및 개인금융채무자의 보호에 관한 법률\n",
      " \n",
      "개인금융채권의 관리 및 개인금융채무자의 보호에 관한 법률 ( 약칭: 개인채무자보호법 )\n",
      "[시행 2024. 10. 17.] [법률 제20369호, 2024. 2. 27., 타법개정]\n",
      "금융위원회 (서민금융과) 02-2100-2612\n",
      "       제1장 총칙\n",
      " \n",
      "제1조(목적) 이 법은 채권금융회사 등과 개인금융채무자 사이의 개인금융채권ㆍ채무 내용의 변동에 따른 개인금융채권\n",
      "의 관리 및 추심ㆍ조정(調停)에 필요한 채권금융회사 등의 준수사항을 규정함으로써 개인금융채무자의 권익을 보호\n",
      "하고 개인금융채권ㆍ채무와 관련된 금융업의 건전한 발전에 이바지함을 목적으로 한다.\n",
      " \n",
      "제2조(정의) 이 법에서 사용하는 용어의 뜻은 다음과 같다.\n",
      "1. “개인금융채권”이란 채권금융회사등이 다음 각 목의 행위를 원인으로 보유하게 된 개인금융채무자에 대한 금전\n",
      "채권을 말한다.\n",
      "가. 금전의 대부\n",
      "나. 대위변제\n",
      "다. 가목 또는 나목의 행위로 발생한 채권의 양수\n",
      "라. 그 밖에 어음할인 등 대통령령으로 정하는 행위\n",
      "2. “채권금융회사등”이란 개인금융채권을 보유하고 있는 다음 각 목의 자를 말한다.\n",
      "가. 「은행법」 등 대통령령으로 정하는 법률에 따라 인가ㆍ허가ㆍ등록ㆍ승인 등을 받아 금전의 대부를 업(業)으로\n",
      "하는 자\n",
      "나. 「공공기관의 운영에 관한 법률」에 따른 공공기관 중 개인금융채권을 보유할 수 있는 기관으로서 대통령령으\n",
      "로 정하는 기관\n",
      "다. 그 밖에 이 법 또는 다른 법률에 따라 개인금융채권을 보유할 수 있는 자로서 대통령령으로 정하는 자\n",
      "3. “개인금융채무자”란 제1호 각 목의 행위를 원인으로 채권금융회사등에 채무를 변제할 의무가 있는 사람(보증인\n",
      "및 채무인수인을 포함한다)을 말한다.' metadata={'source': '/workspace/개인금융채권의 관리 및 개인금융채무자의 보호에 관한 법률(법률)(제20369호)(20241017).pdf', 'chunk_num': 1, 'data_type': 'general_pdf'}\n",
      "\n",
      "📝 JSONL에서 온 데이터 샘플:\n",
      "page_content='질문: TLS 암호화를 통한 C2 채널에서 악성 신호를 어떻게 감지할 수 있나요?\n",
      "답변 : 트랜스포트 레이어 보안 (TLS) 를 활용한 암호화 명령 및 제어 (C2) 채널은 네트워크 방어자 (TNS) 들, 인증서 제품군 사슬의 길이가, 암호 선호도가 등 네트워크 방어자들에게 상당한 도전을 제시합니다. 전통적인 패킷 검사 방법은 유용 한 부하 내용을 직접 분석할 수 없기 때문에. 그러나, 정교한 트래픽 분석 기술은 여전히 행동 및 통계 패턴을 통해 잠재적으로 악성적인 세션들을 식별 할 수 있습니다.\\n\\n** 트래픽 분석 기술:\\n\\n\\nStatistical flow analysis (Statistical flow analysis) 는 서버 이름 표시 (SNI) 패턴, 인증서 제품군 사슬의 길이가, 암호 선호도를 포함하여 네트워크 방어자들에게 중요한 도전을 제시합니다. 악성적인 C2 채널은 종종 특이한 SNI 엔트로피, 비표준 포트, 또는 합법적인 응용 패턴에서 벗어나는 일관적인 패턴을 표시합니다.\\n\\n\\n\\n\\n\\nNLS의 정통성 검증기술은 이러한 사용자 행동 분석 알고리즘을 중시하는 일련의 수행기능을 강조합니다. TLST2CTC는 CTC와 CTC를 사용하여 CTC를 식별하는 것과 같은 비공인성적인 연결기술의 사용 시퀀스를 확인하는 것을 허용합니다.' metadata={'source': '/workspace/2025-AI-Challeng-finance/cybersecurity_data_regex_cleaned.jsonl', 'seq_num': 1, 'data_type': 'qna_jsonl'}\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- 1. 처리할 파일 목록 정의 ---\n",
    "pdf_file_paths = [\n",
    "    \"/workspace/개인금융채권의 관리 및 개인금융채무자의 보호에 관한 법률(법률)(제20369호)(20241017).pdf\",\n",
    "    \"/workspace/개인정보 보호법(법률)(제19234호)(20250313) (1).pdf\",\n",
    "    \"/workspace/거버넌스.pdf\",\n",
    "    \"/workspace/경찰공무원 등의 개인정보 처리에 관한 규정(대통령령)(제35039호)(20241203).pdf\",\n",
    "    \"/workspace/금융보안연구원.pdf\",\n",
    "    \"/workspace/금융소비자 보호에 관한 법률(법률)(제20305호)(20240814).pdf\",\n",
    "    \"/workspace/금융실명거래 및 비밀보장에 관한 법률 시행규칙(총리령)(제01406호)(20170726).pdf\",\n",
    "    \"/workspace/랜섬웨어.pdf\",\n",
    "    \"/workspace/마이데이터.pdf\",\n",
    "    \"/workspace/메타버스.pdf\",\n",
    "    \"/workspace/법원 개인정보 보호에 관한 규칙(대법원규칙)(제03109호)(20240315).pdf\",\n",
    "    \"/workspace/아웃소싱.pdf\",\n",
    "    \"/workspace/정보_보안.pdf\",\n",
    "    \"/workspace/클라우드컴퓨팅 발전 및 이용자 보호에 관한 법률(법률)(제20732호)(20250131).pdf\",\n",
    "]\n",
    "jsonl_file_paths = [\n",
    "    \"/workspace/2025-AI-Challeng-finance/cybersecurity_data_regex_cleaned.jsonl\",\n",
    "    \"/workspace/2025-AI-Challeng-finance/qa_data.jsonl\"\n",
    "]\n",
    "\n",
    "# --- 2. 각 파일 유형별 처리 함수 정의 ---\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"PDF 파일에서 텍스트를 추출합니다.\"\"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            return \"\".join(page.get_text() for page in doc)\n",
    "    except Exception as e:\n",
    "        print(f\"'{pdf_path}' 처리 중 오류: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_korean_law_by_article(text: str, source: str) -> list[Document]:\n",
    "    \"\"\"법률 텍스트를 '조' 단위로 분할하여 Document 객체 리스트를 생성합니다.\"\"\"\n",
    "    # ... (기존 코드와 동일)\n",
    "    pattern = r\"(제\\d+조(?:의\\d+)?\\s*\\(.+?\\))\"\n",
    "    split_text = re.split(pattern, text)\n",
    "    documents = []\n",
    "    for i in range(1, len(split_text), 2):\n",
    "        article_title = split_text[i]\n",
    "        article_content = split_text[i+1].strip()\n",
    "        if not article_content: continue\n",
    "        doc = Document(\n",
    "            page_content=f\"{article_title}\\n{article_content}\",\n",
    "            metadata={\"source\": source, \"article\": article_title.split('(')[0].strip(), \"data_type\": \"legal_pdf\"}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# ##################################################################\n",
    "# ############# ✨ 일반 PDF 처리를 위한 새 모듈(함수) ✨ #############\n",
    "# ##################################################################\n",
    "def chunk_general_pdf(text: str, source: str) -> list[Document]:\n",
    "    \"\"\"일반 텍스트를 의미 기반의 청크로 분할하여 Document 객체 리스트를 생성합니다.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # LangChain의 RecursiveCharacterTextSplitter를 사용하여 텍스트를 분할\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,      # 각 청크의 최대 크기\n",
    "        chunk_overlap=100,    # 청크 간 중복되는 글자 수\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"] # 문단 > 문장 > 단어 순으로 분할 시도\n",
    "    )\n",
    "    \n",
    "    # 텍스트를 직접 분할하여 문자열 리스트를 얻음\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # 각 청크 문자열을 Document 객체로 변환\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"source\": source, \"chunk_num\": i + 1, \"data_type\": \"general_pdf\"}\n",
    "        )\n",
    "        documents.append(doc)\n",
    "        \n",
    "    return documents\n",
    "\n",
    "# --- 3. 메인 실행: 모든 파일 처리 및 통합 ---\n",
    "\n",
    "all_documents = []\n",
    "\n",
    "print(\"--- PDF 파일 처리 시작 ---\")\n",
    "for pdf_path in pdf_file_paths:\n",
    "    print(f\"'{pdf_path}' 파일 처리 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    if not extracted_text:\n",
    "        print(f\"-> 텍스트가 없어 건너뜁니다.\")\n",
    "        continue\n",
    "\n",
    "    # 파일 이름에 '법', '령', '규칙'이 포함되어 있는지 확인하여 분기 처리\n",
    "    if any(keyword in pdf_path for keyword in [\"법\", \"령\", \"규칙\"]):\n",
    "        documents = chunk_korean_law_by_article(extracted_text, pdf_path)\n",
    "        all_documents.extend(documents)\n",
    "        print(f\"-> [법률] {len(documents)}개의 '조(Article)'를 추가했습니다.\")\n",
    "    else:\n",
    "        documents = chunk_general_pdf(extracted_text, pdf_path)\n",
    "        all_documents.extend(documents)\n",
    "        print(f\"-> [일반] {len(documents)}개의 '청크'를 추가했습니다.\")\n",
    "\n",
    "# --- JSONL 파일 처리 ---\n",
    "print(\"\\n--- JSONL 파일 처리 시작 ---\")\n",
    "# ... (기존 코드와 동일) ...\n",
    "text_splitter_jsonl = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "for jsonl_path in jsonl_file_paths:\n",
    "    print(f\"'{jsonl_path}' 파일 처리 중...\")\n",
    "    try:\n",
    "        loader = JSONLoader(\n",
    "            file_path=jsonl_path,\n",
    "            jq_schema='\"질문: \" + .question + \"\\\\n답변 : \" + .answer', \n",
    "            json_lines=True\n",
    "        )\n",
    "        documents_from_jsonl = loader.load()\n",
    "        split_docs = text_splitter_jsonl.split_documents(documents_from_jsonl)\n",
    "        for doc in split_docs:\n",
    "            doc.metadata[\"data_type\"] = \"qna_jsonl\"\n",
    "        all_documents.extend(split_docs)\n",
    "        print(f\"-> {len(split_docs)}개의 'Q&A' 청크를 추가했습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"'{jsonl_path}' 처리 중 오류: {e}\")\n",
    "\n",
    "# --- 최종 결과 확인 ---\n",
    "print(\"\\n--- ✅ 최종 통합 완료 ---\")\n",
    "print(f\"모든 파일로부터 총 {len(all_documents)}개의 Document를 생성했습니다.\")\n",
    "\n",
    "if all_documents:\n",
    "    print(\"\\n--- [통합된 데이터 샘플 확인] ---\")\n",
    "    legal_pdf_sample = next((doc for doc in all_documents if doc.metadata.get(\"data_type\") == \"legal_pdf\"), None)\n",
    "    if legal_pdf_sample:\n",
    "        print(\"📄 법률 PDF에서 온 데이터 샘플:\")\n",
    "        print(legal_pdf_sample)\n",
    "    \n",
    "    general_pdf_sample = next((doc for doc in all_documents if doc.metadata.get(\"data_type\") == \"general_pdf\"), None)\n",
    "    if general_pdf_sample:\n",
    "        print(\"\\n📑 일반 PDF에서 온 데이터 샘플:\")\n",
    "        print(general_pdf_sample)\n",
    "    \n",
    "    jsonl_sample = next((doc for doc in all_documents if doc.metadata.get(\"data_type\") == \"qna_jsonl\"), None)\n",
    "    if jsonl_sample:\n",
    "        print(\"\\n📝 JSONL에서 온 데이터 샘플:\")\n",
    "        print(jsonl_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d6e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## huggingface 객관식 데이터를 question,answer형태로변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92f6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12760\n",
      "{'question': 'Which of the following authentication factors involves something you know?', 'answer': 'Username'}\n",
      "\n",
      "✅ 데이터가 'qa_data.jsonl' 파일에 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# # generate english rag\n",
    "\n",
    "# import json \n",
    "\n",
    "# file_path = \"/workspace/merged_questions.json\"\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     dataset = json.load(f)\n",
    "\n",
    "# print(len(dataset[\"questions\"]))\n",
    "\n",
    "\n",
    "# document_len = len(dataset[\"questions\"])\n",
    "# real_data = dataset[\"questions\"]\n",
    "\n",
    "# all_docs = []\n",
    "\n",
    "# for i in range(document_len):\n",
    "\n",
    "#     question = real_data[i][\"question\"]\n",
    "#     # .get() is used to safely access the answer, avoiding errors if the key is missing\n",
    "#     answer = real_data[i][\"answers\"].get(real_data[i][\"solution\"])\n",
    "\n",
    "#     docs = {\n",
    "#         \"question\" : question,\n",
    "#         \"answer\" : answer\n",
    "#     }\n",
    "\n",
    "#     all_docs.append(docs)\n",
    "\n",
    "# print(all_docs[10])\n",
    "\n",
    "# # --- ✨ 여기에 JSONL 저장 코드를 추가합니다 ✨ ---\n",
    "\n",
    "# # 저장할 파일 이름 정의\n",
    "# output_filename = \"qa_data.jsonl\"\n",
    "\n",
    "# # all_docs 리스트를 jsonl 파일로 저장\n",
    "# with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "#     for doc in all_docs:\n",
    "#         # 각 딕셔너리를 JSON 문자열로 변환하여 파일에 한 줄씩 씁니다.\n",
    "#         # ensure_ascii=False는 비-ASCII 문자(예: 한글)가 깨지지 않도록 합니다.\n",
    "#         f.write(json.dumps(doc, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# print(f\"\\n✅ 데이터가 '{output_filename}' 파일에 성공적으로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6402c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [RAG 구성 시작] ---\n",
      "'BAAI/bge-m3' 임베딩 모델을 로드합니다...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 임베딩 모델 로드 완료.\n",
      "\n",
      "총 15829개의 Document를 벡터로 변환하여 FAISS DB를 구축합니다...\n",
      "✅ FAISS 벡터 DB 구축 완료.\n",
      "✅ Retriever 생성이 완료되었습니다.\n",
      "\n",
      "--- [RAG 검색 테스트] ---\n",
      "❓ 테스트 질문: \"랜섬웨어 공격을 방지하는 방법은 무엇인가요?\"\n",
      "\n",
      "🔍 검색된 관련 문서 (3개):\n",
      "\n",
      "--- [문서 1] ---\n",
      "2. 네트워크 접근제어 : 비인가자의 통신망 접속을 적절하게 조절하는 기술. 특정 보안기능을 적용한 후\n",
      "정보 보안의 방법\n",
      "에 접속할 수 있게 하므로 네트워크 장애를 발생시키는 빈도를 줄임.[5]\n",
      "3. 악성코드차단: 악성코드 종류는 다양하다. 컴퓨터 바이러스, 웜, 스파...\n",
      "(출처: /workspace/정보_보안.pdf)\n",
      "\n",
      "--- [문서 2] ---\n",
      "WannaCry 랜섬웨어에 감염된 모습\n",
      "랜섬웨어\n",
      "랜섬웨어(영어: ransomware)는 몸값을 지불할 때까지 피해자\n",
      "의 개인 데이터를 암호화하는 악성 소프트웨어의 한 유형이\n",
      "다. 컴퓨터 시스템을 감염시켜 접근을 제한하고 복호화 키를\n",
      "조건으로 일종의 몸값을 요구한다. 컴...\n",
      "(출처: /workspace/랜섬웨어.pdf)\n",
      "\n",
      "--- [문서 3] ---\n",
      "질문: Which of the following is a common practice to prevent/troubleshoot ransomware attacks?\n",
      "답변 : Having up-to-date backup copies...\n",
      "(출처: /workspace/2025-AI-Challeng-finance/qa_data.jsonl)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "\n",
    "# --- 4. RAG 구성: 임베딩 및 벡터 스토어 생성 ---\n",
    "\n",
    "# 사용할 임베딩 모델 ID\n",
    "embedding_model_id = \"BAAI/bge-m3\"\n",
    "\n",
    "print(f\"\\n--- [RAG 구성 시작] ---\")\n",
    "print(f\"'{embedding_model_id}' 임베딩 모델을 로드합니다...\")\n",
    "\n",
    "# HuggingFaceEmbeddings 객체 생성\n",
    "# model_kwargs: GPU 사용 설정 (사용 가능할 경우)\n",
    "# encode_kwargs: 임베딩 정규화 설정 (성능 향상에 도움)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_id,\n",
    "    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "print(\"✅ 임베딩 모델 로드 완료.\")\n",
    "\n",
    "print(f\"\\n총 {len(all_documents)}개의 Document를 벡터로 변환하여 FAISS DB를 구축합니다...\")\n",
    "\n",
    "# FAISS.from_documents를 사용하여 벡터 DB 생성\n",
    "# 이 과정에서 tqdm을 직접 사용하기는 어렵지만, 내부적으로 모든 문서를 처리합니다.\n",
    "vectorstore = FAISS.from_documents(all_documents, embeddings)\n",
    "\n",
    "print(\"✅ FAISS 벡터 DB 구축 완료.\")\n",
    "db_save_path = \"./faiss_db_kor_eng\"\n",
    "vectorstore.save_local(db_save_path)\n",
    "# 검색기(Retriever) 생성\n",
    "# search_kwargs={'k': 3}: 검색 시 가장 유사한 3개의 문서를 가져오도록 설정\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "print(\"✅ Retriever 생성이 완료되었습니다.\")\n",
    "\n",
    "\n",
    "# --- 5. RAG 검색 테스트 (선택 사항) ---\n",
    "\n",
    "if retriever:\n",
    "    print(\"\\n--- [RAG 검색 테스트] ---\")\n",
    "    test_query = \"랜섬웨어 공격을 방지하는 방법은 무엇인가요?\"\n",
    "    \n",
    "    # retriever.invoke()를 사용하여 테스트 쿼리와 관련된 문서 검색\n",
    "    retrieved_docs = retriever.invoke(test_query)\n",
    "    \n",
    "    print(f\"❓ 테스트 질문: \\\"{test_query}\\\"\")\n",
    "    print(f\"\\n🔍 검색된 관련 문서 ({len(retrieved_docs)}개):\")\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        print(f\"\\n--- [문서 {i+1}] ---\")\n",
    "        # page_content의 앞부분 150자만 출력\n",
    "        print(doc.page_content[:150] + \"...\")\n",
    "        print(f\"(출처: {doc.metadata.get('source', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58db092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
