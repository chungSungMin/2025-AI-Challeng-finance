import torch
import pandas as pd
import re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from peft import PeftModel
import os

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# --- 1. ì„¤ì • (Configuration) ---
# â­ï¸ ì‚¬ìš©ì ì„¤ì •: ê²½ë¡œë“¤ì„ ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ í™•ì¸í•´ì£¼ì„¸ìš”.
BASE_MODEL_ID = "K-intelligence/Midm-2.0-Base-Instruct"
LORA_ADAPTER_PATH = "/workspace/checkpoint-22" 
TEST_CSV_PATH = '/workspace/open/test.csv'
SUBMISSION_CSV_PATH = './submission_optimized.csv' 

# â­ï¸ RAG DB ì„¤ì •
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"
FAISS_DB_PATH = "/workspace/2025-AI-Challeng-finance/split_RAG_inference/faiss_index_laws"


# --- 2. RAG DB ë¡œë“œ ì „ìš© í•¨ìˆ˜ ---
def load_rag_retriever():
    """
    ë¯¸ë¦¬ ìƒì„±ëœ FAISS ë²¡í„° DBë¥¼ ë¡œë“œí•˜ì—¬ Retrieverë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(FAISS_DB_PATH):
        print(f"âŒ ì˜¤ë¥˜: ë²¡í„° DB ê²½ë¡œ '{FAISS_DB_PATH}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    embedding_model = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL_NAME,
        model_kwargs={'device': 'cuda'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    print(f"â³ ê¸°ì¡´ ë²¡í„° DBë¥¼ '{FAISS_DB_PATH}'ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...")
    vector_db = FAISS.load_local(
        FAISS_DB_PATH, 
        embedding_model, 
        allow_dangerous_deserialization=True
    )
    print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ.")
    
    return vector_db.as_retriever(search_kwargs={'k': 3})


# --- 3. í”„ë¡¬í”„íŠ¸ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---
def is_multiple_choice(question_text: str) -> bool:
    lines = question_text.strip().split("\n")
    option_count = sum(bool(re.match(r"^\s*[1-9][0-9]?[\.\s]", line)) for line in lines)
    return option_count >= 2

def extract_question_and_choices(full_text: str) -> tuple[str, list[str]]:
    lines = full_text.strip().split("\n")
    q_lines, options = [], []
    for line in lines:
        if re.match(r"^\s*[1-9][0-9]?[\.\s]", line):
            options.append(line.strip())
        else:
            q_lines.append(line.strip())
    return " ".join(q_lines), options

def make_prompt(text: str) -> str:
    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        prompt = f"### ì§€ì‹œ:\në‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë‹µë³€ì˜ 'ë²ˆí˜¸'ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n### ì§ˆë¬¸:\n{question}\n\n### ì„ íƒì§€:\n{chr(10).join(options)}\n\n### ë‹µë³€:\n"
    else:
        prompt = f"### ì§€ì‹œ:\në‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì™„ë²½í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”.\n\n### ì§ˆë¬¸:\n{text}\n\n### ë‹µë³€:\n"
    return prompt

def make_rag_prompt(text: str, context: str) -> str:
    if not context.strip():
        return make_prompt(text)
    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        prompt = f"### ì§€ì‹œ:\nì£¼ì–´ì§„ 'ì°¸ê³  ë¬¸ì„œ'ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë‹µë³€ì˜ 'ë²ˆí˜¸'ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n### ì°¸ê³  ë¬¸ì„œ:\n{context}\n\n### ì§ˆë¬¸:\n{question}\n\n### ì„ íƒì§€:\n{chr(10).join(options)}\n\n### ë‹µë³€:\n"
    else:
        prompt = f"### ì§€ì‹œ:\nì£¼ì–´ì§„ 'ì°¸ê³  ë¬¸ì„œ'ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì™„ë²½í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”.\n\n### ì°¸ê³  ë¬¸ì„œ:\n{context}\n\n### ì§ˆë¬¸:\n{text}\n\n### ë‹µë³€:\n"
    return prompt

def post_process_answer(generated_text: str, original_question: str) -> str:
    answer = generated_text.strip().split("###")[0].strip()
    if is_multiple_choice(original_question):
        match = re.search(r"^\s*(\d+)", answer)
        return match.group(1) if match else "1"
    return answer if answer else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."


# --- 4. LLM ë¡œë“œ (Flash Attention 2 ì ìš©) ---
print("â³ ì¶”ë¡ ìš© LLMì„ ë¡œë”©í•©ë‹ˆë‹¤...")
quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16)

# â­ï¸ Flash Attention 2 ì ìš©
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID, 
    quantization_config=quantization_config, 
    device_map="auto",
    attn_implementation="flash_attention_2"  # ì´ ë¶€ë¶„ ì¶”ê°€
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"â³ '{LORA_ADAPTER_PATH}'ì—ì„œ LoRA ì–´ëŒ‘í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤...")
model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
model = model.merge_and_unload()

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device_map="auto")
print("âœ… ì¶”ë¡ ìš© LLM ë¡œë”© ì™„ë£Œ.")


# --- 5. ë©”ì¸ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬ ì ìš©) ---
if __name__ == "__main__":
    retriever = load_rag_retriever()

    try:
        test_df = pd.read_csv(TEST_CSV_PATH)
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{TEST_CSV_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    # â­ï¸ ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰
    preds = []
    batch_size = 8  # í•œ ë²ˆì— ì²˜ë¦¬í•  ì§ˆë¬¸ ìˆ˜ (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ 4, 8, 16 ë“±ìœ¼ë¡œ ì¡°ì ˆ)
    all_questions = test_df['Question'].tolist()

    for i in tqdm(range(0, len(all_questions), batch_size), desc="ğŸš€ ë°°ì¹˜ ì¶”ë¡  ì§„í–‰ ì¤‘"):
        batch_questions = all_questions[i:i + batch_size]
        
        # 1. RAG ê²€ìƒ‰ (ë°°ì¹˜)
        batch_contexts = []
        for q in batch_questions:
            try:
                retrieved_docs = retriever.get_relevant_documents(q)
                context_text = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
                batch_contexts.append(context_text)
            except Exception as e:
                print(f"âš ï¸ RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                batch_contexts.append("")

        # 2. í”„ë¡¬í”„íŠ¸ ìƒì„± (ë°°ì¹˜)
        batch_prompts = [make_rag_prompt(q, ctx) for q, ctx in zip(batch_questions, batch_contexts)]

        # 3. LLM ì¶”ë¡  (ë°°ì¹˜)
        outputs = pipe(
            batch_prompts, 
            max_new_tokens=512, 
            do_sample=True, 
            temperature=0.1, 
            return_full_text=False, 
            eos_token_id=tokenizer.eos_token_id,
            batch_size=batch_size # íŒŒì´í”„ë¼ì¸ì— ë°°ì¹˜ í¬ê¸° ëª…ì‹œ
        )
        
        # 4. ê²°ê³¼ í›„ì²˜ë¦¬
        for idx, output in enumerate(outputs):
            # 'outputs'ëŠ” ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸ í›„ ì²˜ë¦¬
            if isinstance(output, list) and len(output) > 0:
                generated_text = output[0]['generated_text']
            elif isinstance(output, dict):
                generated_text = output['generated_text']
            else:
                generated_text = "" # ì˜ˆì™¸ ì²˜ë¦¬

            pred_answer = post_process_answer(generated_text, original_question=batch_questions[idx])
            preds.append(pred_answer)

    # ì œì¶œ íŒŒì¼ ìƒì„±
    try:
        sample_submission = pd.read_csv('/workspace/open/sample_submission.csv')
        sample_submission['Answer'] = preds
        sample_submission.to_csv(SUBMISSION_CSV_PATH, index=False, encoding='utf-8-sig')
        print(f"\nâœ… ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: '{SUBMISSION_CSV_PATH}'")
    except FileNotFoundError:
        print("âŒ ì˜¤ë¥˜: 'sample_submission.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
