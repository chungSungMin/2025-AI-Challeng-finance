# import os
# import torch
# import pandas as pd
# import re
# from tqdm import tqdm
# import json

# # --- 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ Import ---
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS
# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
# from peft import PeftModel

# # --- 2. ê²½ë¡œ ë° ëª¨ë¸ ì„¤ì • ---
# # ê¸°ë³¸ ëª¨ë¸ ID
# BASE_MODEL_ID = "K-intelligence/Midm-2.0-Base-Instruct"
# # í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ê²½ë¡œ
# LORA_ADAPTER_PATH = "/workspace/checkpoint-708" 
# # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ì œì¶œ íŒŒì¼ ê²½ë¡œ
# TEST_CSV_PATH = '/workspace/open/test.csv'
# SAMPLE_SUBMISSION_PATH = '/workspace/open/sample_submission.csv'
# SUBMISSION_CSV_PATH = './submission_with_loaded_rag.csv' 

# # â˜…â˜…â˜… ìˆ˜ì •ëœ ë¶€ë¶„ 1: ë¶ˆëŸ¬ì˜¬ DB ê²½ë¡œì™€ ì„ë² ë”© ëª¨ë¸ ì§€ì • â˜…â˜…â˜…
# # ë¯¸ë¦¬ êµ¬ì¶•í•œ FAISS DBê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
# FAISS_DB_PATH = "/workspace/2025-AI-Challeng-finance/faiss_db_kor_eng" 
# # DB êµ¬ì¶• ì‹œ ì‚¬ìš©í–ˆë˜ ì„ë² ë”© ëª¨ë¸ (DBì™€ ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•¨)
# EMBEDDING_MODEL_NAME = "BAAI/bge-m3" 


# # --- 3. RAG ë°±ì—”ë“œ ë¡œë“œ í•¨ìˆ˜ (ìˆ˜ì •ëœ ë¶€ë¶„) ---
# def load_rag_retriever(db_path: str, embedding_model_name: str):
#     """ì§€ì •ëœ ê²½ë¡œì—ì„œ ë¯¸ë¦¬ êµ¬ì¶•ëœ FAISS ë²¡í„° DBë¥¼ ë¡œë“œí•˜ì—¬ Retrieverë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
#     if not os.path.exists(db_path):
#         print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: RAG ë°ì´í„°ë² ì´ìŠ¤ë¥¼ '{db_path}'ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#         print("DB ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜, DBë¥¼ ë¨¼ì € êµ¬ì¶•í•´ì£¼ì„¸ìš”.")
#         return None

#     print(f"â³ ê¸°ì¡´ ë²¡í„° DBë¥¼ '{db_path}'ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...")
#     try:
#         embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)
#         vector_db = FAISS.load_local(
#             db_path, 
#             embedding_model, 
#             allow_dangerous_deserialization=True
#         )
#         print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ.")
#         # LangChainì˜ Retriever ê°ì²´ ë°˜í™˜
#         return vector_db.as_retriever(search_kwargs={'k': 3})
#     except Exception as e:
#         print(f"âŒ ì˜¤ë¥˜: ë²¡í„° DB ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#         return None

# # --- 4. ìœ í‹¸ë¦¬í‹° ë° í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼) ---
# def is_multiple_choice(question_text: str) -> bool:
#     lines = question_text.strip().split("\n")
#     option_count = sum(bool(re.match(r"^\s*[1-9][0-9]?[\.\s]", line)) for line in lines)
#     return option_count >= 2

# def extract_question_and_choices(full_text: str) -> tuple[str, list[str]]:
#     lines = full_text.strip().split("\n")
#     q_lines, options = [], []
#     for line in lines:
#         if re.match(r"^\s*[1-9][0-9]?[\.\s]", line):
#             options.append(line.strip())
#         else:
#             q_lines.append(line.strip())
#     question = " ".join(q_lines)
#     return question, options

# def make_rag_prompt(text: str, context: str) -> str:
#     """RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
#     if is_multiple_choice(text):
#         question, options = extract_question_and_choices(text)
#         prompt = f"""### ì§€ì‹œ:
# ì£¼ì–´ì§„ **'ì°¸ê³  ë¬¸ì„œ'ì˜ ë‚´ìš©ë§Œì„ ê·¼ê±°**ë¡œ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë‹µë³€ì˜ 'ë²ˆí˜¸'ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

# ### ì°¸ê³  ë¬¸ì„œ:
# {context}

# ### ì§ˆë¬¸:
# {question}

# ### ì„ íƒì§€:
# {chr(10).join(options)}

# ### ë‹µë³€:
# """
#     else:
#         prompt = f"""### ì§€ì‹œ:
# ì£¼ì–´ì§„ **'ì°¸ê³  ë¬¸ì„œ'ì˜ ë‚´ìš©ë§Œì„ ê·¼ê±°**ë¡œ í•˜ì—¬ 'ì§ˆë¬¸'ì— ë‹µë³€í•˜ì„¸ìš”.
# ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬, ì „ë¬¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ 2~3ê°œì˜ ì™„ë²½í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.
# **'ì°¸ê³  ë¬¸ì„œì— ë”°ë¥´ë©´'ê³¼ ê°™ì€ í‘œí˜„ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.** ë‹¹ì‹ ì˜ ë°°ê²½ ì§€ì‹ì´ë‚˜ ì™¸ë¶€ ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

# ### ì°¸ê³  ë¬¸ì„œ:
# {context}

# ### ì§ˆë¬¸:
# {text}

# ### ë‹µë³€:
# """
#     return prompt

# # --- 5. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ê¸°ì¡´ê³¼ ë™ì¼) ---
# print("â³ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")
# quantization_config = BitsAndBytesConfig(
#     load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
# )
# base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, quantization_config=quantization_config, device_map="auto")
# tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
# if tokenizer.pad_token is None:
#     tokenizer.pad_token = tokenizer.eos_token
# print(f"â³ '{LORA_ADAPTER_PATH}'ì—ì„œ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë”©í•˜ì—¬ ëª¨ë¸ì— ì ìš©í•©ë‹ˆë‹¤...")
# model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
# print("â³ LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©í•©ë‹ˆë‹¤...")
# model = model.merge_and_unload()
# pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
# print("âœ… ëª¨ë¸ ë¡œë”© ë° ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# # --- 6. ë‹µë³€ í›„ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼) ---
# def post_process_answer(generated_text: str, original_question: str) -> str:
#     answer = generated_text.strip()
#     if not answer: return "1"
#     if "###" in answer: answer = answer.split("###")[-1].strip()
#     if is_multiple_choice(original_question):
#         match = re.search(r'(\d+)', answer)
#         return match.group(1) if match else "1"
#     return answer if answer else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

# # --- 7. ë©”ì¸ ì‹¤í–‰ (ìˆ˜ì •ëœ ë¶€ë¶„) ---
# if __name__ == "__main__":
#     # â˜…â˜…â˜… ìˆ˜ì •ëœ ë¶€ë¶„ 2: DB ë¡œë“œ í•¨ìˆ˜ í˜¸ì¶œ â˜…â˜…â˜…
#     retriever = load_rag_retriever(FAISS_DB_PATH, EMBEDDING_MODEL_NAME)
    
#     if not retriever:
#         print("âŒ RAG ë°±ì—”ë“œ ë¡œë”©ì— ì‹¤íŒ¨í•˜ì—¬ ì¶”ë¡ ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
#         exit()

#     try:
#         test_df = pd.read_csv(TEST_CSV_PATH)
#         print(f"âœ… '{TEST_CSV_PATH}'ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
#     except FileNotFoundError:
#         print(f"âŒ ì˜¤ë¥˜: '{TEST_CSV_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
#         exit()

#     preds = []
#     for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc="ğŸš€ ì¶”ë¡  ì§„í–‰ ì¤‘"):
#         question = row['Question']
        
#         # LangChain Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
#         retrieved_docs = retriever.invoke(question)
#         context_text = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
        
#         prompt = make_rag_prompt(question, context_text)
        
#         output = pipe(
#             prompt, 
#             max_new_tokens=512,
#             temperature=0.1,
#             top_p=0.9,
#             do_sample=True,
#             return_full_text=False,
#             eos_token_id=tokenizer.eos_token_id
#         )
#         generated_text = output[0]['generated_text']

#         pred_answer = post_process_answer(generated_text, original_question=question)
#         preds.append(pred_answer)

#     print("\nğŸ“„ ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
#     try:
#         sample_submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)
#         sample_submission['Answer'] = preds
#         sample_submission.to_csv(SUBMISSION_CSV_PATH, index=False, encoding='utf-8-sig')
#         print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: '{SUBMISSION_CSV_PATH}'")
#     except FileNotFoundError:
#         print(f"âŒ ì˜¤ë¥˜: '{SAMPLE_SUBMISSION_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")





import os
import torch
import pandas as pd
import re
from tqdm import tqdm
import json

# --- 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ Import ---
from langchain.retrievers import MultiQueryRetriever
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from peft import PeftModel

# --- 2. ê²½ë¡œ ë° ëª¨ë¸ ì„¤ì • ---
# ê¸°ë³¸ ëª¨ë¸ ID
BASE_MODEL_ID = "K-intelligence/Midm-2.0-Base-Instruct"
# í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ê²½ë¡œ
LORA_ADAPTER_PATH = "/workspace/checkpoint-708" 
# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ì œì¶œ íŒŒì¼ ê²½ë¡œ
TEST_CSV_PATH = '/workspace/open/test.csv'
SAMPLE_SUBMISSION_PATH = '/workspace/open/sample_submission.csv'
SUBMISSION_CSV_PATH = './submission_multiquery_rag.csv' 

# ë¯¸ë¦¬ êµ¬ì¶•í•œ FAISS DBê°€ ì €ì¥ëœ í´ë” ê²½ë¡œ
FAISS_DB_PATH = "/workspace/2025-AI-Challeng-finance/faiss_db_kor_eng" 
# DB êµ¬ì¶• ì‹œ ì‚¬ìš©í–ˆë˜ ì„ë² ë”© ëª¨ë¸ (DBì™€ ë°˜ë“œì‹œ ì¼ì¹˜í•´ì•¼ í•¨)
EMBEDDING_MODEL_NAME = "BAAI/bge-m3" 


# --- 3. MultiQueryRetriever ë¡œë“œ í•¨ìˆ˜ ---
def load_multi_query_retriever(db_path: str, embedding_model_name: str, llm_pipeline):
    """
    ë¯¸ë¦¬ êµ¬ì¶•ëœ FAISS DBì™€ LLMì„ ì‚¬ìš©í•˜ì—¬ MultiQueryRetrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(db_path):
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: RAG ë°ì´í„°ë² ì´ìŠ¤ë¥¼ '{db_path}'ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    print(f"â³ ê¸°ì¡´ ë²¡í„° DBë¥¼ '{db_path}'ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...")
    try:
        embedding_model = HuggingFaceEmbeddings(model_name=embedding_model_name)
        vector_db = FAISS.load_local(
            db_path, 
            embedding_model, 
            allow_dangerous_deserialization=True
        )
        print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ.")

        # LangChainì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ LLM íŒŒì´í”„ë¼ì¸ ê°ì²´í™”
        llm_for_retriever = HuggingFacePipeline(pipeline=llm_pipeline)
        
        # MultiQueryRetriever ìƒì„±
        multi_query_retriever = MultiQueryRetriever.from_llm(
            retriever=vector_db.as_retriever(search_kwargs={'k': 5}), # í›„ë³´êµ°ì„ ëŠ˜ë¦¬ê¸° ìœ„í•´ kê°’ ìƒí–¥
            llm=llm_for_retriever
        )
        print("âœ… MultiQueryRetriever ìƒì„± ì™„ë£Œ.")
        return multi_query_retriever
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: ë²¡í„° DB ë¡œë”© ë˜ëŠ” Retriever ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# --- 4. ìœ í‹¸ë¦¬í‹° ë° í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ ---
def is_multiple_choice(question_text: str) -> bool:
    lines = question_text.strip().split("\n")
    option_count = sum(bool(re.match(r"^\s*[1-9][0-9]?[\.\s]", line)) for line in lines)
    return option_count >= 2

def extract_question_and_choices(full_text: str) -> tuple[str, list[str]]:
    lines = full_text.strip().split("\n")
    q_lines, options = [], []
    for line in lines:
        if re.match(r"^\s*[1-9][0-9]?[\.\s]", line):
            options.append(line.strip())
        else:
            q_lines.append(line.strip())
    question = " ".join(q_lines)
    return question, options

def make_rag_prompt(text: str, context: str) -> str:
    """RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        prompt = f"""### ì§€ì‹œ:
ì£¼ì–´ì§„ **'ì°¸ê³  ë¬¸ì„œ'ì˜ ë‚´ìš©ë§Œì„ ê·¼ê±°**ë¡œ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë‹µë³€ì˜ 'ë²ˆí˜¸'ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

### ì°¸ê³  ë¬¸ì„œ:
{context}

### ì§ˆë¬¸:
{question}

### ì„ íƒì§€:
{chr(10).join(options)}

### ë‹µë³€:
"""
    else:
        prompt = f"""### ì§€ì‹œ:
ì£¼ì–´ì§„ **'ì°¸ê³  ë¬¸ì„œ'ì˜ ë‚´ìš©ë§Œì„ ê·¼ê±°**ë¡œ í•˜ì—¬ 'ì§ˆë¬¸'ì— ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ í•µì‹¬ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬, ì „ë¬¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ 2~3ê°œì˜ ì™„ë²½í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.
**'ì°¸ê³  ë¬¸ì„œì— ë”°ë¥´ë©´'ê³¼ ê°™ì€ í‘œí˜„ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**

### ì°¸ê³  ë¬¸ì„œ:
{context}

### ì§ˆë¬¸:
{text}

### ë‹µë³€:
"""
    return prompt

# --- 5. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---
print("â³ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_dtype=torch.bfloat16
)
base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, quantization_config=quantization_config, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
print(f"â³ '{LORA_ADAPTER_PATH}'ì—ì„œ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë”©í•˜ì—¬ ëª¨ë¸ì— ì ìš©í•©ë‹ˆë‹¤...")
model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
print("â³ LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©í•©ë‹ˆë‹¤...")
model = model.merge_and_unload()
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
print("âœ… ëª¨ë¸ ë¡œë”© ë° ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- 6. ë‹µë³€ í›„ì²˜ë¦¬ ---
def post_process_answer(generated_text: str, original_question: str) -> str:
    answer = generated_text.strip()
    if not answer: return "1"
    if "###" in answer: answer = answer.split("###")[-1].strip()
    if is_multiple_choice(original_question):
        match = re.search(r'(\d+)', answer)
        return match.group(1) if match else "1"
    return answer if answer else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

# --- 7. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    # ëª¨ë¸ê³¼ íŒŒì´í”„ë¼ì¸ì´ ë¨¼ì € ë¡œë“œëœ í›„ Retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    retriever = load_multi_query_retriever(FAISS_DB_PATH, EMBEDDING_MODEL_NAME, pipe)
    
    if not retriever:
        print("âŒ RAG ë°±ì—”ë“œ ë¡œë”©ì— ì‹¤íŒ¨í•˜ì—¬ ì¶”ë¡ ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        exit()

    try:
        test_df = pd.read_csv(TEST_CSV_PATH)
        print(f"âœ… '{TEST_CSV_PATH}'ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{TEST_CSV_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()

    preds = []
    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc="ï¿½ ì¶”ë¡  ì§„í–‰ ì¤‘"):
        question = row['Question']
        
        # MultiQueryRetrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = retriever.invoke(question)
        context_text = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
        
        prompt = make_rag_prompt(question, context_text)
        
        output = pipe(
            prompt, 
            max_new_tokens=512,
            temperature=0.1,
            top_p=0.9,
            do_sample=True,
            return_full_text=False,
            eos_token_id=tokenizer.eos_token_id
        )
        generated_text = output[0]['generated_text']

        pred_answer = post_process_answer(generated_text, original_question=question)
        preds.append(pred_answer)

    print("\nğŸ“„ ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        sample_submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)
        sample_submission['Answer'] = preds
        sample_submission.to_csv(SUBMISSION_CSV_PATH, index=False, encoding='utf-8-sig')
        print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: '{SUBMISSION_CSV_PATH}'")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{SAMPLE_SUBMISSION_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
