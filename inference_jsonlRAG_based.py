import torch
import pandas as pd
import re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from peft import PeftModel
import os
from datasets import load_dataset
from langchain_community.document_loaders import PyPDFLoader

# RAGë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# --- 1. ì„¤ì • (Configuration) ---

# ê¸°ë³¸ ëª¨ë¸ ID (í•™ìŠµì— ì‚¬ìš©í•œ ëª¨ë¸)
BASE_MODEL_ID = "K-intelligence/Midm-2.0-Base-Instruct"

# â­ï¸ ì‚¬ìš©ì ì„¤ì •: í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ê°€ ì €ì¥ëœ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.
LORA_ADAPTER_PATH = "/workspace/2025-AI-Challeng-finance/midm-lora-adapter-trainer/checkpoint-110" 

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ì œì¶œ íŒŒì¼ ê²½ë¡œ/workspace/paper_generate_mid_ì •ë³´ë³´í˜¸ì‚°ì—….jsonl
TEST_CSV_PATH = '/workspace/open/test.csv'
SUBMISSION_CSV_PATH = './submission_rag_inference.csv' 

# RAG ì§€ì‹ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©í•  JSONL íŒŒì¼ ëª©ë¡
RAG_DATA_FILES = [
"/workspace/á„€á…¢á„‹á…µá†«á„€á…³á†·á„‹á…²á†¼á„á…¢á„€á…¯á†«á„‹á…´ á„€á…ªá†«á„…á…µ á„†á…µá†¾ á„€á…¢á„‹á…µá†«á„€á…³á†·á„‹á…²á†¼á„á…¢á„†á…®á„Œá…¡á„‹á…´ á„‡á…©á„’á…©á„‹á…¦ á„€á…ªá†«á„’á…¡á†« á„‡á…¥á†¸á„…á…²á†¯(á„‡á…¥á†¸á„…á…²á†¯)(á„Œá…¦20369á„’á…©)(20241017).pdf",
"/workspace/á„€á…¢á„‹á…µá†«á„Œá…¥á†¼á„‡á…© á„‡á…©á„’á…©á„‡á…¥á†¸(á„‡á…¥á†¸á„…á…²á†¯)(á„Œá…¦19234á„’á…©)(20250313) (1).pdf",
"/workspace/á„€á…¥á„‡á…¥á„‚á…¥á†«á„‰á…³.pdf",
"/workspace/á„€á…§á†¼á„á…¡á†¯á„€á…©á†¼á„†á…®á„‹á…¯á†« á„ƒá…³á†¼á„‹á…´ á„€á…¢á„‹á…µá†«á„Œá…¥á†¼á„‡á…© á„á…¥á„…á…µá„‹á…¦ á„€á…ªá†«á„’á…¡á†« á„€á…²á„Œá…¥á†¼(á„ƒá…¢á„á…©á†¼á„…á…§á†¼á„…á…§á†¼)(á„Œá…¦35039á„’á…©)(20241203).pdf",
"/workspace/á„€á…³á†·á„‹á…²á†¼á„‡á…©á„‹á…¡á†«á„‹á…§á†«á„€á…®á„‹á…¯á†«.pdf",
"/workspace/á„€á…³á†·á„‹á…²á†¼á„‰á…©á„‡á…µá„Œá…¡ á„‡á…©á„’á…©á„‹á…¦ á„€á…ªá†«á„’á…¡á†« á„‡á…¥á†¸á„…á…²á†¯(á„‡á…¥á†¸á„…á…²á†¯)(á„Œá…¦20305á„’á…©)(20240814).pdf",
"/workspace/á„€á…³á†·á„‹á…²á†¼á„‰á…µá†¯á„†á…§á†¼á„€á…¥á„…á…¢ á„†á…µá†¾ á„‡á…µá„†á…µá†¯á„‡á…©á„Œá…¡á†¼á„‹á…¦ á„€á…ªá†«á„’á…¡á†« á„‡á…¥á†¸á„…á…²á†¯ á„‰á…µá„’á…¢á†¼á„€á…²á„á…µá†¨(á„á…©á†¼á„…á…µá„…á…§á†¼)(á„Œá…¦01406á„’á…©)(20170726).pdf",
"/workspace/á„…á…¢á†«á„‰á…¥á†·á„‹á…°á„‹á…¥.pdf",
"/workspace/á„†á…¡á„‹á…µá„ƒá…¦á„‹á…µá„á…¥.pdf",
"/workspace/á„†á…¦á„á…¡á„‡á…¥á„‰á…³.pdf",
"/workspace/á„‡á…¥á†¸á„‹á…¯á†« á„€á…¢á„‹á…µá†«á„Œá…¥á†¼á„‡á…© á„‡á…©á„’á…©á„‹á…¦ á„€á…ªá†«á„’á…¡á†« á„€á…²á„á…µá†¨(á„ƒá…¢á„‡á…¥á†¸á„‹á…¯á†«á„€á…²á„á…µá†¨)(á„Œá…¦03109á„’á…©)(20240315).pdf",
"/workspace/á„‹á…¡á„‹á…®á†ºá„‰á…©á„‰á…µá†¼.pdf",
"/workspace/á„Œá…¥á†¼á„‡á…©_á„‡á…©á„‹á…¡á†«.pdf",
"/workspace/á„á…³á†¯á„…á…¡á„‹á…®á„ƒá…³á„á…¥á†·á„‘á…²á„á…µá†¼ á„‡á…¡á†¯á„Œá…¥á†« á„†á…µá†¾ á„‹á…µá„‹á…­á†¼á„Œá…¡ á„‡á…©á„’á…©á„‹á…¦ á„€á…ªá†«á„’á…¡á†« á„‡á…¥á†¸á„…á…²á†¯(á„‡á…¥á†¸á„…á…²á†¯)(á„Œá…¦20732á„’á…©)(20250131).pdf",
]
EMBEDDING_MODEL_NAME = "BAAI/bge-m3"
# ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° DB ê²½ë¡œ ì„¤ì •
# EMBEDDING_MODEL_NAME = "jhgan/ko-sbert-nli"
FAISS_DB_PATH = "./faiss_index_laws"


# --- 2. RAG ë°±ì—”ë“œ êµ¬ì¶• í•¨ìˆ˜ ---
# def build_or_load_rag_backend():
#     """3ê°œì˜ JSONL íŒŒì¼ë¡œë¶€í„° FAISS ë²¡í„° DBë¥¼ êµ¬ì¶•í•˜ê±°ë‚˜ ê¸°ì¡´ DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    
#     # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
#     embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
#     # 2. ì´ë¯¸ ìƒì„±ëœ DBê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
#     if os.path.exists(FAISS_DB_PATH):
#         print(f"â³ ê¸°ì¡´ ë²¡í„° DBë¥¼ '{FAISS_DB_PATH}'ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...")
#         vector_db = FAISS.load_local(
#             FAISS_DB_PATH, 
#             embedding_model, 
#             allow_dangerous_deserialization=True
#         )
#         print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ.")
#     else:
#         print(f"â³ '{RAG_DATA_FILES}' íŒŒì¼ë“¤ë¡œ ìƒˆë¡œìš´ ë²¡í„° DBë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤...")
#         # ë°ì´í„°ì…‹ ë¡œë“œ
#         dataset = load_dataset('json', data_files=RAG_DATA_FILES, split='train')
        
#         # Langchain Document ê°ì²´ë¡œ ë³€í™˜
#         # source_chunkë¥¼ ê²€ìƒ‰ ëŒ€ìƒ í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
#         documents = [
#             Document(page_content=item['source_chunk'], metadata={'question': item['question'], 'answer': item['answer']}) 
#             for item in tqdm(dataset, desc="ğŸ“„ Document ê°ì²´ ìƒì„± ì¤‘")
#         ]
        
#         # FAISS ë²¡í„° DB ìƒì„±
#         vector_db = FAISS.from_documents(documents, embedding_model)
        
#         # ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ ë¡œì»¬ì— ì €ì¥
#         vector_db.save_local(FAISS_DB_PATH)
#         print(f"âœ… ìƒˆë¡œìš´ ë²¡í„° DB êµ¬ì¶• ë° ì €ì¥ ì™„ë£Œ: '{FAISS_DB_PATH}'")

#     # ê²€ìƒ‰ê¸°(Retriever) ë°˜í™˜ (ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰)
#     return vector_db.as_retriever(search_kwargs={'k': 3})


# í•„ìš”í•œ PDF ë¡œë”ë¥¼ import í•©ë‹ˆë‹¤.
# --- 2. RAG ë°±ì—”ë“œ êµ¬ì¶• í•¨ìˆ˜ (ìˆ˜ì •ëœ ë²„ì „) ---
def build_or_load_rag_backend():
    """PDF ë° JSONL íŒŒì¼ë“¤ë¡œë¶€í„° FAISS ë²¡í„° DBë¥¼ êµ¬ì¶•í•˜ê±°ë‚˜ ê¸°ì¡´ DBë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    
    # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    embedding_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    
    # 2. ì´ë¯¸ ìƒì„±ëœ DBê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if os.path.exists(FAISS_DB_PATH):
        print(f"â³ ê¸°ì¡´ ë²¡í„° DBë¥¼ '{FAISS_DB_PATH}'ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤...")
        vector_db = FAISS.load_local(
            FAISS_DB_PATH, 
            embedding_model, 
            allow_dangerous_deserialization=True
        )
        print("âœ… ë²¡í„° DB ë¡œë“œ ì™„ë£Œ.")
    else:
        print(f"â³ '{RAG_DATA_FILES}' íŒŒì¼ë“¤ë¡œ ìƒˆë¡œìš´ ë²¡í„° DBë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤...")
        
        # --- â˜…â˜…â˜… í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ì‹œì‘ â˜…â˜…â˜… ---
        all_documents = []
        # ê° PDF íŒŒì¼ì„ ìˆœíšŒí•˜ë©° í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        for file_path in tqdm(RAG_DATA_FILES, desc="ğŸ“š PDF íŒŒì¼ ë¡œë”© ì¤‘"):
            try:
                # PyPDFLoaderë¥¼ ì‚¬ìš©í•´ PDF íŒŒì¼ ë¡œë“œ
                loader = PyPDFLoader(file_path)
                # PDFì˜ ê° í˜ì´ì§€ê°€ ë³„ë„ì˜ Document ê°ì²´ë¡œ ë¶„ë¦¬ë˜ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ë¨
                documents_from_pdf = loader.load() 
                all_documents.extend(documents_from_pdf)
            except Exception as e:
                print(f"âš ï¸ ê²½ê³ : '{file_path}' íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # --- â˜…â˜…â˜… í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ë â˜…â˜…â˜… ---

        if not all_documents:
            print("âŒ ì˜¤ë¥˜: ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. RAG_DATA_FILES ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            exit()

        print(f"ì´ {len(all_documents)}ê°œì˜ í˜ì´ì§€(Document)ë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤...")
        # FAISS ë²¡í„° DB ìƒì„± (ê¸°ì¡´ documents ë³€ìˆ˜ëª…ì„ all_documentsë¡œ ë³€ê²½)
        vector_db = FAISS.from_documents(all_documents, embedding_model)
        
        # ë‹¤ìŒ ì‹¤í–‰ì„ ìœ„í•´ ë¡œì»¬ì— ì €ì¥
        vector_db.save_local(FAISS_DB_PATH)
        print(f"âœ… ìƒˆë¡œìš´ ë²¡í„° DB êµ¬ì¶• ë° ì €ì¥ ì™„ë£Œ: '{FAISS_DB_PATH}'")

    # ê²€ìƒ‰ê¸°(Retriever) ë°˜í™˜ (ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰)
    return vector_db.as_retriever(search_kwargs={'k': 3})



# --- 3. ìœ í‹¸ë¦¬í‹° ë° í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ ---

def is_multiple_choice(question_text: str) -> bool:
    """ì§ˆë¬¸ì´ ê°ê´€ì‹ì¸ì§€ ì£¼ê´€ì‹ì¸ì§€ íŒë³„í•©ë‹ˆë‹¤."""
    lines = question_text.strip().split("\n")
    # ìˆ«ìë¡œ ì‹œì‘í•˜ê³  ì (.) ë˜ëŠ” ê³µë°±ìœ¼ë¡œ ëë‚˜ëŠ” ì„ íƒì§€ê°€ 2ê°œ ì´ìƒì´ë©´ ê°ê´€ì‹ìœ¼ë¡œ íŒë‹¨
    option_count = sum(bool(re.match(r"^\s*[1-9][0-9]?[\.\s]", line)) for line in lines)
    return option_count >= 2

def extract_question_and_choices(full_text: str) -> tuple[str, list[str]]:
    """ê°ê´€ì‹ ì§ˆë¬¸ì—ì„œ ìˆœìˆ˜ ì§ˆë¬¸ê³¼ ì„ íƒì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤."""
    lines = full_text.strip().split("\n")
    q_lines = []
    options = []
    for line in lines:
        if re.match(r"^\s*[1-9][0-9]?[\.\s]", line):
            options.append(line.strip())
        else:
            q_lines.append(line.strip())
    question = " ".join(q_lines)
    return question, options


# <<< ìˆ˜ì • 1: RAG ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  Fallback í”„ë¡¬í”„íŠ¸ (ì²« ë²ˆì§¸ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìˆ˜ì •) >>>
def make_prompt(text: str) -> str:
    """
    RAG ì—†ì´ ëª¨ë¸ì˜ ë‚´ë¶€ ì§€ì‹ë§Œìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤.
    (ì²« ë²ˆì§¸ ì„±ê³µì ì¸ ì½”ë“œì˜ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°ì™€ ë™ì¼)
    """
    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        # ê°ê´€ì‹ í”„ë¡¬í”„íŠ¸
        prompt = f"""### ì§€ì‹œ:
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë‹µë³€ì˜ 'ë²ˆí˜¸'ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

### ì§ˆë¬¸:
{question}

### ì„ íƒì§€:
{chr(10).join(options)}

### ë‹µë³€:
"""
    else:
        # ì£¼ê´€ì‹ í”„ë¡¬í”„íŠ¸
        prompt = f"""### ì§€ì‹œ:
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì™„ë²½í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”.
'ì°¸ê³  ë¬¸ì„œ'ì— ì§ì ‘ì ì¸ ì–¸ê¸‰ì´ ì—†ì–´ë„ ìµœëŒ€í•œ ë°°ê²½ ì§€ì‹ì„ í™œìš©í•´ì„œ ë‹µí•´ì£¼ì„¸ìš”.
"ë¬¸ì„œì— ë”°ë¥´ë©´~ " ì´ë¼ëŠ” ë‚´ìš©ì„ ì“°ì§€ ë§ì•„ì£¼ì„¸ìš”.


### ì§ˆë¬¸:
{text}

### ë‹µë³€:
"""
    return prompt


# <<< ìˆ˜ì • 2: RAG ì „ìš© í”„ë¡¬í”„íŠ¸ (ì²« ë²ˆì§¸ ì½”ë“œì˜ ì„±ê³µì ì¸ êµ¬ì¡° ê¸°ë°˜ìœ¼ë¡œ ìˆ˜ì •) >>>
def make_rag_prompt(text: str, context: str) -> str:
    """RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨í•˜ì—¬ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    # ì°¸ê³  ë¬¸ì„œê°€ ë¹„ì–´ìˆìœ¼ë©´ RAG ì—†ì´ ì¼ë°˜ í”„ë¡¬í”„íŠ¸ë¡œ ëŒ€ì²´
    if not context.strip():
        return make_prompt(text)

    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        # ê°ê´€ì‹ RAG í”„ë¡¬í”„íŠ¸
        prompt = f"""### ì§€ì‹œ:
ì£¼ì–´ì§„ 'ì°¸ê³  ë¬¸ì„œ'ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë‹µë³€ì˜ 'ë²ˆí˜¸'ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

### ì°¸ê³  ë¬¸ì„œ:
{context}

### ì§ˆë¬¸:
{question}

### ì„ íƒì§€:
{chr(10).join(options)}

### ë‹µë³€:
"""
    else:
        # ì£¼ê´€ì‹ RAG í”„ë¡¬í”„íŠ¸
        prompt = f"""### ì§€ì‹œ:
ì£¼ì–´ì§„ 'ì°¸ê³  ë¬¸ì„œ'ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì™„ë²½í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”. 'ì°¸ê³  ë¬¸ì„œì— ë”°ë¥´ë©´'ê³¼ ê°™ì€ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
'ì°¸ê³  ë¬¸ì„œ'ì— ì§ì ‘ì ì¸ ì–¸ê¸‰ì´ ì—†ì–´ë„ ìµœëŒ€í•œ ë°°ê²½ ì§€ì‹ì„ í™œìš©í•´ì„œ ë‹µí•´ì£¼ì„¸ìš”.
"ë¬¸ì„œì— ë”°ë¥´ë©´~ " ì´ë¼ëŠ” ë‚´ìš©ì„ ì“°ì§€ ë§ì•„ì£¼ì„¸ìš”.

### ì°¸ê³  ë¬¸ì„œ:
{context}

### ì§ˆë¬¸:
{text}

### ë‹µë³€:
"""
    return prompt

# --- 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---

print("â³ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quantization_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"â³ '{LORA_ADAPTER_PATH}'ì—ì„œ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë”©í•˜ì—¬ ëª¨ë¸ì— ì ìš©í•©ë‹ˆë‹¤...")
try:
    model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: LoRA ì–´ëŒ‘í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {LORA_ADAPTER_PATH}")
    print(e)
    exit()

print("â³ LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©í•©ë‹ˆë‹¤...")
model = model.merge_and_unload()

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)
print("âœ… ëª¨ë¸ ë¡œë”© ë° ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


def post_process_answer(generated_text: str, original_question: str) -> str:
    """[ìˆ˜ì •] ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ìµœì¢… ë‹µë³€ì„ ì¶”ì¶œí•˜ê³  ì •ë¦¬í•˜ëŠ” ê°•í™”ëœ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    answer = generated_text.strip()
    
    if not answer:
        return "1"  # ë‹µë³€ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ '1' ë°˜í™˜

    # ë‹µë³€ì— í”„ë¡¬í”„íŠ¸ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ì œê±°
    if "###" in answer:
        answer = answer.split("###")[0].strip()
        
    if is_multiple_choice(original_question):
        # 1ë‹¨ê³„: "ì •ë‹µì€ 5", "ë‹µì€ 5ë²ˆ"ê³¼ ê°™ì´ ëª…í™•í•œ íŒ¨í„´ì—ì„œ ìˆ«ì ì¶”ì¶œ
        match = re.search(r'(?:ì •ë‹µì€|ë‹µì€|ì„ íƒì€)\s*\D*(\d+)', answer)
        if match:
            return match.group(1)

        # 2ë‹¨ê³„: "5ë²ˆ", "5." ì™€ ê°™ì€ íŒ¨í„´ì—ì„œ ìˆ«ì ì¶”ì¶œ
        match = re.search(r'\b(\d+)\s*(?:ë²ˆ|ë²ˆì…ë‹ˆë‹¤|\.)', answer)
        if match:
            return match.group(1)

        # 3ë‹¨ê³„: ë¬¸ì¥ ë§¨ ì•ì— ìˆëŠ” ìˆ«ì ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§)
        match = re.search(r"^\s*(\d+)", answer)
        if match:
            return match.group(1)

        # 4ë‹¨ê³„: ìœ„ ëª¨ë“  ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•Šì„ ê²½ìš°, í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ ì²˜ìŒ ë°œê²¬ë˜ëŠ” ìˆ«ì ì¶”ì¶œ
        match = re.search(r'(\d+)', answer)
        if match:
            return match.group(1)
            
        # 5ë‹¨ê³„: ê·¸ë˜ë„ ìˆ«ìë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ê¸°ë³¸ê°’ '1' ë°˜í™˜
        return "1"
    
    # ì£¼ê´€ì‹ ë‹µë³€ ì²˜ë¦¬
    return answer if answer else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

# is_code_detected í•¨ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
def is_code_detected(text: str) -> bool:
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ì½”ë“œê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    code_keywords = ['def ', 'import ', 'class ', 'r\'', 'sys.stdout', 'ans_qna']
    text_lower = text.lower()
    return any(keyword in text_lower for keyword in code_keywords)


def is_code_detected(text: str) -> bool:
    """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ì½”ë“œê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    code_keywords = ['def ', 'import ', 'class ', 'r\'', 'sys.stdout', 'ans_qna']
    text_lower = text.lower()
    return any(keyword in text_lower for keyword in code_keywords)


# --- 6. ë©”ì¸ ì‹¤í–‰ (RAG ì ìš©) ---
if __name__ == "__main__":
    # RAG ë°±ì—”ë“œ(Retriever) ì¤€ë¹„
    retriever = build_or_load_rag_backend()

    try:
        test_df = pd.read_csv(TEST_CSV_PATH)
        print(f"âœ… '{TEST_CSV_PATH}'ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{TEST_CSV_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()

    preds = []
    MAX_RETRIES = 3 

    for index, q in tqdm(enumerate(test_df['Question']), total=len(test_df), desc="ğŸš€ RAG ì¶”ë¡  ì§„í–‰ ì¤‘"):
        
        # RAG ê²€ìƒ‰ ë‹¨ê³„ ì¶”ê°€
        try:
            # ì§ˆë¬¸(q)ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
            retrieved_docs = retriever.get_relevant_documents(q)
            # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹©ë‹ˆë‹¤.
            context_text = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
        except Exception as e:
            print(f"âš ï¸ TEST_{index} ì§ˆë¬¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            context_text = "" # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ contextëŠ” ë¹„ì›Œë‘ 

        # RAG í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = make_rag_prompt(q, context_text)
        
        is_valid_answer = False
        retries = 0
        generated_text = ""

        while not is_valid_answer and retries < MAX_RETRIES:
            if retries > 0:
                print(f"\nğŸ”„ TEST_{index} ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì¬ì‹œë„ ì¤‘... ({retries}/{MAX_RETRIES})")

            output = pipe(
                prompt, 
                max_new_tokens=512,
                temperature=0.1 + (retries * 0.15),
                top_p=0.9,
                do_sample=True,
                return_full_text=False,
                eos_token_id=tokenizer.eos_token_id
            )
            
            generated_text = output[0]['generated_text']

            if is_code_detected(generated_text):
                retries += 1
                if retries == MAX_RETRIES:
                    print(f"âŒ TEST_{index} ì§ˆë¬¸ì— ëŒ€í•´ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë§ˆì§€ë§‰ìœ¼ë¡œ ìƒì„±ëœ ë‹µë³€ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    is_valid_answer = True
            else:
                is_valid_answer = True

        pred_answer = post_process_answer(generated_text, original_question=q)
        preds.append(pred_answer)

    print("\nğŸ“„ ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        sample_submission = pd.read_csv('/workspace/open/sample_submission.csv')
        sample_submission['Answer'] = preds
        sample_submission.to_csv(SUBMISSION_CSV_PATH, index=False, encoding='utf-8-sig')
        print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: '{SUBMISSION_CSV_PATH}'")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '/workspace/open/sample_submission.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
