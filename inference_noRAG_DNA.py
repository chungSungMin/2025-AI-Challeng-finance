import torch
import pandas as pd
import re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from peft import PeftModel
import os

# --- 1. ì„¤ì • (Configuration) ---

# ê¸°ë³¸ ëª¨ë¸ ID
BASE_MODEL_ID = "dnotitia/DNA-2.0-14B"

# â­ï¸ ì‚¬ìš©ì ì„¤ì •: 'dnotitia/DNA-2.0-14B'ë¡œ í•™ìŠµëœ LoRA ì–´ëŒ‘í„° ê²½ë¡œ
LORA_ADAPTER_PATH = "/workspace/2025-AI-Challeng-finance/dna-lora-adapter-trainer/checkpoint-500"

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ì œì¶œ íŒŒì¼ ê²½ë¡œ
TEST_CSV_PATH = '/workspace/open/test.csv'
SUBMISSION_CSV_PATH = './submission_batch_dna_inference.csv' # íŒŒì¼ ì´ë¦„ ë³€ê²½


# --- 2. ìœ í‹¸ë¦¬í‹° ë° í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜ (RAG ê´€ë ¨ í•¨ìˆ˜ ì œê±°) ---

def is_multiple_choice(question_text: str) -> bool:
    """ì§ˆë¬¸ì´ ê°ê´€ì‹ì¸ì§€ íŒë³„í•©ë‹ˆë‹¤."""
    lines = question_text.strip().split("\n")
    option_count = sum(bool(re.match(r"^\s*[1-9][0-9]?[\.\s]", line)) for line in lines)
    return option_count >= 2

def extract_question_and_choices(full_text: str) -> tuple[str, list[str]]:
    """ê°ê´€ì‹ ì§ˆë¬¸ì—ì„œ ìˆœìˆ˜ ì§ˆë¬¸ê³¼ ì„ íƒì§€ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤."""
    lines = full_text.strip().split("\n")
    q_lines = []
    options = []
    for line in lines:
        if re.match(r"^\s*[1-9][0-9]?[\.\s]", line):
            options.append(line.strip())
        else:
            q_lines.append(line.strip())
    question = " ".join(q_lines)
    return question, options

def make_prompt(text: str) -> str:
    """ëª¨ë¸ì˜ ë‚´ë¶€ ì§€ì‹ë§Œìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ì…ë‹ˆë‹¤."""
    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        prompt = f"""### ì§€ì‹œ:
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì˜¬ë°”ë¥¸ ë‹µë³€ì˜ 'ë²ˆí˜¸'ë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

### ì§ˆë¬¸:
{question}

### ì„ íƒì§€:
{chr(10).join(options)}

### ë‹µë³€:
"""
    else:
        prompt = f"""### ì§€ì‹œ:
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì™„ë²½í•œ í•œêµ­ì–´ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”. ë°°ê²½ ì§€ì‹ì„ í™œìš©í•´ì„œ ë‹µí•´ì£¼ì„¸ìš”. "ë¬¸ì„œì— ë”°ë¥´ë©´~ " ì´ë¼ëŠ” ë‚´ìš©ì„ ì“°ì§€ ë§ì•„ì£¼ì„¸ìš”.
ìµœëŒ€í•œ **ì „ë¬¸ ìš©ì–´**ë¥¼ í™œìš©í•´ì„œ ì„œìˆ í•´ì£¼ì„¸ìš”. ê·¸ë¦¬ê³  ë§ˆí¬ë‹¤ìš´ì„ ì‚¬ìš©í•˜ì§€ë§ê³ , 2~3ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ì„ ë‹´ì•„ ì„œìˆ í•˜ì„¸ìš”.

### ì§ˆë¬¸:
{text}

### ë‹µë³€:
"""
    return prompt

# --- 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---

print("â³ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quantization_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print(f"â³ '{LORA_ADAPTER_PATH}'ì—ì„œ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë”©í•˜ì—¬ ëª¨ë¸ì— ì ìš©í•©ë‹ˆë‹¤...")
try:
    model = PeftModel.from_pretrained(base_model, LORA_ADAPTER_PATH)
except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: LoRA ì–´ëŒ‘í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {LORA_ADAPTER_PATH}")
    print(e)
    exit()

print("â³ LoRA ê°€ì¤‘ì¹˜ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©í•©ë‹ˆë‹¤...")
model = model.merge_and_unload()

tokenizer.padding_side = 'left'

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)
print("âœ… ëª¨ë¸ ë¡œë”© ë° ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


def post_process_answer(generated_text: str, original_question: str) -> str:
    """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ìµœì¢… ë‹µë³€ì„ ì¶”ì¶œí•˜ê³  ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    answer = generated_text.strip()
    
    if not answer:
        return "1"

    if "###" in answer:
        answer = answer.split("###")[0].strip()
        
    if is_multiple_choice(original_question):
        match = re.search(r'(?:ì •ë‹µì€|ë‹µì€|ì„ íƒì€)\s*\D*(\d+)', answer)
        if match: return match.group(1)

        match = re.search(r'\b(\d+)\s*(?:ë²ˆ|ë²ˆì…ë‹ˆë‹¤|\.)', answer)
        if match: return match.group(1)

        match = re.search(r"^\s*(\d+)", answer)
        if match: return match.group(1)

        match = re.search(r'(\d+)', answer)
        if match: return match.group(1)
            
        return "1"
    
    return answer if answer else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

# --- 4. ë©”ì¸ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬ ì ìš©) ---
# --- 4. ë©”ì¸ ì‹¤í–‰ (ìˆ˜ë™ ë°°ì¹˜ ì²˜ë¦¬ ì ìš©) ---
if __name__ == "__main__":
    print("[INFO] ëª¨ë¸ì˜ ë‚´ë¶€ ì§€ì‹ë§Œìœ¼ë¡œ ì¶”ë¡ ì„ ì§„í–‰í•©ë‹ˆë‹¤.")

    try:
        test_df = pd.read_csv(TEST_CSV_PATH)
        print(f"âœ… '{TEST_CSV_PATH}'ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '{TEST_CSV_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()

    prompts = [make_prompt(q) for q in test_df['Question']]
    
    # â˜…â˜…â˜… ìˆ˜ë™ ë°°ì¹˜ë¥¼ ìœ„í•œ ì„¤ì • â˜…â˜…â˜…
    batch_size = 2
    all_outputs = [] # ëª¨ë“  ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    print(f"ğŸš€ ë°°ì¹˜ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ë°°ì¹˜ í¬ê¸°: {batch_size})...")
    
    # â˜…â˜…â˜… tqdmìœ¼ë¡œ ì „ì²´ ë£¨í”„ë¥¼ ê°ì‹¸ê³ , ìˆ˜ë™ìœ¼ë¡œ ë°°ì¹˜ë¥¼ ìƒì„±í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì— ì „ë‹¬ â˜…â˜…â˜…
    for i in tqdm(range(0, len(prompts), batch_size), desc="ğŸš€ ì¶”ë¡  ì§„í–‰ ì¤‘"):
        # í˜„ì¬ ì²˜ë¦¬í•  ë°°ì¹˜ ìŠ¬ë¼ì´ì‹±
        batch_prompts = prompts[i:i + batch_size]
        
        # íŒŒì´í”„ë¼ì¸ì€ í˜„ì¬ ë°°ì¹˜ë§Œ ì²˜ë¦¬
        # batch_size ì¸ìëŠ” íŒŒì´í”„ë¼ì¸ í˜¸ì¶œì—ì„œ ì œê±° (ìˆ˜ë™ìœ¼ë¡œ ì œì–´í•˜ë¯€ë¡œ)
        outputs_batch = pipe(
            batch_prompts,
            max_new_tokens=512,
            temperature=0.1,
            top_p=0.9,
            do_sample=True,
            return_full_text=False,
            eos_token_id=tokenizer.eos_token_id
        )
        # ì²˜ë¦¬ëœ ë°°ì¹˜ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        all_outputs.extend(outputs_batch)

    print("\nğŸ“„ ì¶”ë¡  ì™„ë£Œ. ë‹µë³€ì„ í›„ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    
    # ë³€ìˆ˜ ì´ë¦„ì„ outputs -> all_outputs ë¡œ ë³€ê²½
    preds = []
    for i, output in enumerate(tqdm(all_outputs, desc="ë‹µë³€ í›„ì²˜ë¦¬ ì¤‘")):
        generated_text = output[0]['generated_text']
        original_question = test_df['Question'][i]
        pred_answer = post_process_answer(generated_text, original_question)
        preds.append(pred_answer)

    print("\nğŸ“„ ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        sample_submission = pd.read_csv('/workspace/open/sample_submission.csv')
        sample_submission['Answer'] = preds
        sample_submission.to_csv(SUBMISSION_CSV_PATH, index=False, encoding='utf-8-sig')
        print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: '{SUBMISSION_CSV_PATH}'")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '/workspace/open/sample_submission.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")