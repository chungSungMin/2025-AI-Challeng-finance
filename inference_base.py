import torch
import pandas as pd
import re
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
import os

# --- 1. ì„¤ì • (Configuration) ---

# ê¸°ë³¸ ëª¨ë¸ ID (ì‚¬ìš©í•  ëª¨ë¸)
BASE_MODEL_ID = "K-intelligence/Midm-2.0-Base-Instruct"

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë° ì œì¶œ íŒŒì¼ ê²½ë¡œ
TEST_CSV_PATH = '/workspace/open/test.csv'
SUBMISSION_CSV_PATH = './submission_cot_model.csv' 

# --- 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---

def is_multiple_choice(question_text: str) -> bool:
    lines = question_text.strip().split("\n")
    option_count = sum(bool(re.match(r"^\s*[1-9][0-9]?\s", line)) for line in lines)
    return option_count >= 2

def extract_question_and_choices(full_text: str) -> tuple[str, list[str]]:
    lines = full_text.strip().split("\n")
    q_lines = []
    options = []
    for line in lines:
        if re.match(r"^\s*[1-9][0-9]?\s", line):
            options.append(line.strip())
        else:
            q_lines.append(line.strip())
    question = " ".join(q_lines)
    return question, options

# (í•µì‹¬ ë³€ê²½) 1. make_prompt í•¨ìˆ˜ë¥¼ Chain-of-Thought ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
def make_prompt(text: str) -> str:
    """
    ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ Chain-of-Thought í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if is_multiple_choice(text):
        question, options = extract_question_and_choices(text)
        # ê°ê´€ì‹ CoT í”„ë¡¬í”„íŠ¸
        prompt = f"""### ì§€ì‹œ:
                    ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ê° ì„ íƒì§€ë¥¼ ë¶„ì„í•˜ê³ , ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •ì„ ì„œìˆ í•œ ë’¤, ìµœì¢…ì ìœ¼ë¡œ ë°˜ë“œì‹œ ê°€ì¥ ì˜¬ë°”ë¥¸ ë‹µë³€ì˜ ë²ˆí˜¸ë¥¼ 'ìµœì¢… ë‹µë³€: [ë²ˆí˜¸]' í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

                    ### ì˜ˆì‹œ:
                    ì§ˆë¬¸: ê¸ˆìœµê¸°ê´€ì˜ ê°œì¸ì •ë³´ ë³´í˜¸ ì˜ë¬´ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€?
                    ì„ íƒì§€:
                    1. ê³ ê°ì˜ ë™ì˜ ì—†ì´ ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ë‹¤.
                    2. ê³ ê°ì˜ ë™ì˜ ì—†ì´ ê°œì¸ì •ë³´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ë‹¤.
                    3. ê³ ê°ì˜ ë™ì˜ ì—†ì´ ê°œì¸ì •ë³´ë¥¼ ê³µê°œí•  ìˆ˜ ì—†ë‹¤.
                    4. ê³ ê°ì˜ ë™ì˜ ì—†ì´ ê°œì¸ì •ë³´ë¥¼ ì‚­ì œí•  ìˆ˜ ì—†ë‹¤.
                    ë‹µë³€:
                    * 1ë²ˆ ê³ ê°ì˜ ë™ì˜ ì—†ì´ ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ê²ƒì€ ê¸ˆìœµê¸°ê´€ì˜ ê¸°ë³¸ì ì¸ ì˜ë¬´ì´ë©°, ì´ëŠ” ê°œì¸ì •ë³´ ë³´í˜¸ë²•ì— ëª…ì‹œë˜ì–´ ìˆë‹¤.
                    * 2ë²ˆ ê³ ê°ì˜ ë™ì˜ ì—†ì´ ê°œì¸ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì€ ê°€ëŠ¥í•˜ë‚˜, í•´ë‹¹ ì²˜ë¦¬ì˜ ë²”ìœ„ì™€ ëª©ì ì´ ëª…í™•í•´ì•¼ í•œë‹¤.
                    * 3ë²ˆ ê³ ê°ì˜ ë™ì˜ ì—†ì´ ê°œì¸ì •ë³´ë¥¼ ê³µê°œí•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤
                    * 4ë²ˆ "ê³ ê°ì˜ ë™ì˜ ì—†ì´ ê°œì¸ì •ë³´ë¥¼ ì‚­ì œí•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤.
                    ë”°ë¼ì„œ ê°€ì¥ ì˜¬ë°”ë¥¸ ì„¤ëª…ì€ 1ë²ˆì´ë‹¤.
                    ìµœì¢… ë‹µë³€: 1



                    ### ì˜ˆì‹œ:
                    ì§ˆë¬¸: ì •ë³´í†µì‹ ë§ ì´ìš©ìë¡œë¶€í„° ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•  ë•Œ, ìˆ˜ì§‘ ëª©ì ì„ ëª…í™•íˆ í•´ì•¼ í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?
                    ì„ íƒì§€:
                    1. ì •ë³´í†µì‹ ë§ ì´ìš©ìì˜ ë™ì˜ë¥¼ ì–»ê¸° ìœ„í•œ ì ˆì°¨
                    2. ë²•ì  ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ì ˆì°¨ 
                    3. ì •ë³´í†µì‹ ë§ ì„œë¹„ìŠ¤ ì œê³µìê°€ ê°œì¸ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ë°©ë²•
                    4. ì •ë³´í†µì‹ ë§ ì´ìš©ìì—ê²Œ ê°œì¸ì •ë³´ ì²˜ë¦¬ ë°©ì¹¨ì„ ì•Œë¦¬ëŠ” ì˜ë¬´
                    ë‹µë³€:
                    * 1ë²ˆ ì •ë³´í†µì‹ ë§ ì´ìš©ìë¡œë¶€í„° ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í•  ë•Œ, ìˆ˜ì§‘ ëª©ì ì„ ëª…í™•íˆ í•´ì•¼ í•˜ë¯€ë¡œ ì´ìš©ìëŠ” ìì‹ ì˜ ì •ë³´ê°€ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    * 2ë²ˆ ë²•ì  ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•˜ê¸° ìœ„í•´ì„œëŠ” ìˆ˜ì§‘ ëª©ì ì„ ëª…í™•íˆ í•´ì•¼ í•˜ì§€ë§Œ, ì´ëŠ” ê°œì¸ì •ë³´ ìˆ˜ì§‘ì˜ ê¸°ë³¸ ì›ì¹™ì´ì§€, ìˆ˜ì§‘ ëª©ì ì„ ëª…í™•íˆ í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ” ì´ìœ ê°€ ì•„ë‹™ë‹ˆë‹¤.
                    * 3ë²ˆ ì •ë³´í†µì‹ ë§ ì„œë¹„ìŠ¤ ì œê³µìê°€ ê°œì¸ì •ë³´ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ë°©ë²•ì€ ì¤‘ìš”í•˜ì§€ë§Œ, ìˆ˜ì§‘ ëª©ì ì„ ëª…í™•íˆ í•˜ëŠ” ê²ƒì€ ë³„ê°œì˜ ë¬¸ì œì…ë‹ˆë‹¤.
                    * 4ë²ˆ ì •ë³´í†µì‹ ë§ ì´ìš©ìì—ê²Œ ê°œì¸ì •ë³´ ì²˜ë¦¬ ë°©ì¹¨ì„ ì•Œë¦¬ëŠ” ì˜ë¬´ëŠ” ìˆì§€ë§Œ, ì´ëŠ” ìˆ˜ì§‘ ëª©ì ì„ ëª…í™•íˆ í•˜ëŠ” ê²ƒê³¼ëŠ” ë‹¤ë¦…ë‹ˆë‹¤.
                    ë”°ë¼ì„œ ê°€ì¥ ì˜¬ë°”ë¥¸ ì„¤ëª…ì€ 1ë²ˆì´ë‹¤.
                    ìµœì¢… ë‹µë³€: 1


                    ### ì‹¤ì œ ì…ë ¥
                    ì§ˆë¬¸:
                    {question}

                    ì„ íƒì§€:
                    {chr(10).join(options)}

                    ë‹µë³€:
                """
    else:
        # ì£¼ê´€ì‹ í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€í•˜ë˜ ì§€ì‹œë¬¸ í†µì¼)
        prompt = f"""### ì§€ì‹œ:
                    ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì™„ë²½í•œ ë¬¸ì¥ìœ¼ë¡œ ì„œìˆ í•˜ì„¸ìš”.

                    ### ì˜ˆì‹œ1 : 
                    ì§ˆë¬¸ : ê°œì¸ì •ë³´ì˜ êµ­ì™¸ ì´ì „ì´ ì¤‘ì§€ë  ìˆ˜ ìˆëŠ” ì¡°ê±´ì€ ë¬´ì—‡ì¸ê°€?
                    ë‹µë³€ : ê°œì¸ì •ë³´ì˜ êµ­ì™¸ ì´ì „ì´ ì¤‘ì§€ë  ìˆ˜ ìˆëŠ” ì¡°ê±´ì€ ì œ28ì¡°ì˜8ì œ1í•­, ì œ4í•­ ë˜ëŠ” ì œ5í•­ì„ ìœ„ë°˜í•˜ê±°ë‚˜ ê°œì¸ì •ë³´ë¥¼ ì´ì „ë°›ëŠ” ìë‚˜ êµ­ê°€ê°€ ê°œì¸ì •ë³´ ë³´í˜¸ ìˆ˜ì¤€ì— ë¯¸ì¹˜ì§€ ëª»í•˜ì—¬ ì •ë³´ì£¼ì²´ì—ê²Œ í”¼í•´ê°€ ë°œìƒí•  ìš°ë ¤ê°€ ìˆëŠ” ê²½ìš°ì´ë‹¤.

                    ### ì˜ˆì‹œ2 : 
                    ì§ˆë¬¸ : ë¶„ìŸì¡°ì •ìœ„ì›íšŒê°€ ë¶„ìŸì¡°ì • ì‹ ì²­ì„ ë°›ì€ í›„ ì‹¬ì‚¬í•˜ì—¬ ì¡°ì •ì•ˆì„ ì‘ì„±í•´ì•¼ í•˜ëŠ” ê¸°ê°„ì€ ì–¼ë§ˆì¸ê°€?
                    ë‹µë³€ : ë¶„ìŸì¡°ì •ìœ„ì›íšŒëŠ” ë¶„ìŸì¡°ì • ì‹ ì²­ì„ ë°›ì€ ë‚ ë¶€í„° 60ì¼ ì´ë‚´ì— ì‹¬ì‚¬í•˜ì—¬ ì¡°ì •ì•ˆì„ ì‘ì„±í•´ì•¼ í•œë‹¤.
                    
                    ### ì‹¤ì œ ì…ë ¥ : 
                    ì§ˆë¬¸:
                    {text}

                    ### ë‹µë³€:
                """
    return prompt

# --- 4. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---

print("â³ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quantization_config,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)
print("âœ… ëª¨ë¸ ë¡œë”© ë° ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# (í•µì‹¬ ë³€ê²½) 2. post_process_answer í•¨ìˆ˜ë¥¼ CoT ê²°ê³¼ì— ë§ê²Œ ìˆ˜ì •
def post_process_answer(generated_text: str, original_question: str) -> str:
    """ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ìµœì¢… ë‹µë³€ì„ ì¶”ì¶œí•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤."""
    answer = generated_text.strip()
    
    if not answer:
        return "ë¯¸ì‘ë‹µ"

    if is_multiple_choice(original_question):
        # CoT ê²°ê³¼ì—ì„œ 'ìµœì¢… ë‹µë³€: [ìˆ«ì]' íŒ¨í„´ì„ ì°¾ì•„ ìˆ«ìë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        match = re.search(r"ìµœì¢… ë‹µë³€:\s*([1-9][0-9]?)", answer)
        if match:
            return match.group(1)
        else:
            # ë§Œì•½ 'ìµœì¢… ë‹µë³€' íŒ¨í„´ì„ ì°¾ì§€ ëª»í•˜ë©´, ì›ë˜ ë°©ì‹ëŒ€ë¡œ ë‹µë³€ì—ì„œ ìˆ«ìë¼ë„ ì°¾ì•„ë´…ë‹ˆë‹¤.
            fallback_match = re.search(r"\D*([1-9][0-9]?)", answer)
            return fallback_match.group(1) if fallback_match else "1" # ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ 1ë²ˆ ì¶”ì¸¡
    
    # ì£¼ê´€ì‹ ë‹µë³€ì€ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ê°„ë‹¨íˆ ì •ë¦¬
    if "###" in answer:
        answer = answer.split("###")[0].strip()
        
    return answer if answer else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

# --- 6. ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    try:
        test_df = pd.read_csv(TEST_CSV_PATH)
        print(f"âœ… '{TEST_CSV_PATH}'ì—ì„œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    except FileNotFoundError:
        # ì˜¬ë°”ë¥¸ ë³€ìˆ˜ëª…ìœ¼ë¡œ ìˆ˜ì •
        print(f"âŒ ì˜¤ë¥˜: '{TEST_CSV_PATH}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()


    prompts = [make_prompt(q) for q in tqdm(test_df['Question'], desc="í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘")]
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ
    outputs = pipe(
        prompts, 
        max_new_tokens=512,
        temperature=0.1,
        top_p=0.9,
        do_sample=True,
        return_full_text=False,
        eos_token_id=tokenizer.eos_token_id,
        batch_size=8 # GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì¡°ì ˆ
    )

    preds = []
    for i, output in enumerate(tqdm(outputs, desc="ğŸ“„ ê²°ê³¼ í›„ì²˜ë¦¬ ì¤‘")):
        generated_text = output[0]['generated_text']
        original_question = test_df['Question'].iloc[i]
        pred_answer = post_process_answer(generated_text, original_question)
        preds.append(pred_answer)

    print("ğŸ“„ ì¶”ë¡ ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        sample_submission = pd.read_csv('/workspace/open/sample_submission.csv')
        sample_submission['Answer'] = preds
        sample_submission.to_csv(SUBMISSION_CSV_PATH, index=False, encoding='utf-8-sig')
        print(f"âœ… ì œì¶œ íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: '{SUBMISSION_CSV_PATH}'")
    except FileNotFoundError:
        print(f"âŒ ì˜¤ë¥˜: '/workspace/open/sample_submission.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")